{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jimbo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jimbo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jimbo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/jimbo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jimbo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jimbo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import scipy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from sklearn.metrics import f1_score\n",
    "from IPython.display import HTML, display\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 Percentage:  0.7522737712448323\n",
      "Class 1 Percentage:  0.24772622875516767\n"
     ]
    }
   ],
   "source": [
    "data_0 = pd.read_csv('../datasets/fossology-master-corrected.csv')\n",
    "X_0 = data_0[\"copyright\"]\n",
    "y_0 = data_0[\"falsePositive\"]\n",
    "X_0 = X_0.drop_duplicates()\n",
    "y_0 = y_0[X_0.index]\n",
    "\n",
    "data_1 = pd.read_csv('../datasets/kubernetes-master-corrected.csv')\n",
    "X_1 = data_1[\"copyright\"]\n",
    "y_1 = data_1[\"falsePositive\"]\n",
    "X_1 = X_1.drop_duplicates()\n",
    "y_1 = y_1[X_1.index]\n",
    "\n",
    "data_2 = pd.read_csv('../datasets/tensorflow-master-corrected.csv')\n",
    "X_2 = data_2[\"copyright\"]\n",
    "y_2 = data_2[\"falsePositive\"]\n",
    "X_2 = X_2.drop_duplicates()\n",
    "y_2 = y_2[X_2.index]\n",
    "\n",
    "data_3 = pd.read_csv('../datasets/fossology-provided-1-corrected.csv')\n",
    "\n",
    "X_3 = data_3['copyright']\n",
    "y_3 = data_3['falsePositive']\n",
    "X_3 = X_3.drop_duplicates()\n",
    "y_3 = y_3[X_3.index]\n",
    "\n",
    "data_4 = pd.read_csv('../datasets/fossology-provided-2.csv')\n",
    "\n",
    "X_4 = data_4['copyright']\n",
    "y_4 = data_4['falsePositive']\n",
    "X_4 = X_4.drop_duplicates()\n",
    "y_4 = y_4[X_4.index]\n",
    "\n",
    "data_5 = pd.read_csv('../datasets/feature-extraction-paper.csv')\n",
    "\n",
    "X_5 = data_5['copyright']\n",
    "y_5 = data_5['falsePositive']\n",
    "X_5 = X_5.drop_duplicates()\n",
    "y_5 = y_5[X_5.index]\n",
    "\n",
    "X = pd.concat([X_0, X_1, X_2, X_3])\n",
    "y = pd.concat([y_0, y_1, y_2, y_3])\n",
    "\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)\n",
    "\n",
    "print('Class 0 Percentage: ', len(y[y == 0]) / len(y))\n",
    "print('Class 1 Percentage: ', len(y[y == 1]) / len(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_reports(reports, print_aggregates=True):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    dfs = []\n",
    "    for metric in ['precision', 'recall', 'f1-score']:\n",
    "        scores = []\n",
    "        for report in reports:\n",
    "            scores.append([report['0'][metric], report['1'][metric]])\n",
    "        scores = np.array(scores)\n",
    "        scores = scores[:, :2]\n",
    "        mean_scores = np.mean(scores, axis=0)\n",
    "        mean_scores = [f\"{score:.6f}\" for score in mean_scores]\n",
    "        df = pd.DataFrame(scores, columns=['0', '1'])\n",
    "        df.loc['Mean'] = mean_scores\n",
    "        df['Metric'] = metric\n",
    "        dfs.append(df)\n",
    "    if print_aggregates:\n",
    "        print(\"## Precision\")\n",
    "        print(dfs[0].to_markdown())\n",
    "        print(\"## Recall\")\n",
    "        print(dfs[1].to_markdown())\n",
    "        print(\"## F1-score\")\n",
    "        print(dfs[2].to_markdown())\n",
    "    else:\n",
    "        return dfs[0], dfs[1], dfs[2]\n",
    "\n",
    "def get_missclassified_rows(X, y_true, y_pred, only_this_class = [0, 1], return_index=False):\n",
    "    if type(y_true) != list:\n",
    "        y_true = y_true.tolist()\n",
    "    if type(y_pred) != list:\n",
    "        y_pred = y_pred.tolist()\n",
    "    if type(X) != list:\n",
    "        X = X.tolist()\n",
    "    missclassified_rows = []\n",
    "    for i in range(len(y_true)):\n",
    "        if y_true[i] != y_pred[i] and y_true[i] in only_this_class:\n",
    "            missclassified_rows.append(i)\n",
    "    if return_index:\n",
    "        return [(y_pred[i], i, X[i]) for i in missclassified_rows]\n",
    "    else:\n",
    "        return [(y_pred[i], X[i]) for i in missclassified_rows]\n",
    "\n",
    "def preprocess_function(sentences, lower=False, replace_copyright_symbols=False,\n",
    "                        replace_dates=False, remove_numbers=False, remove_punctuation=False,\n",
    "                        remove_special_characters=False, remove_whitespaces=False, remove_specials_weird=False,\n",
    "                        remove_stopwords=False, replace_emails=False,\n",
    "                        tokenize=False, lemmatize=False, glove=False, remove_endlines=False,\n",
    "                        replace_copyright_symbol_v2=False, replace_entities=False, spacy_model=None):\n",
    "    if type(sentences) is not list:\n",
    "        sentences = sentences.to_list()\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = str(sentences[i])\n",
    "    if replace_entities:\n",
    "        if spacy_model is None: # give error\n",
    "            raise ValueError('spacy_model must be provided if replace_entities is True')\n",
    "        else:\n",
    "            nlp = spacy_model\n",
    "        sentences = [nlp(sentence) for sentence in sentences]\n",
    "        new_sentences = []\n",
    "        for sentence in sentences:\n",
    "            new_sentence = sentence.text\n",
    "            for entity in sentence.ents:\n",
    "                if entity.label_ == 'ENT':\n",
    "                    new_sentence = re.sub(re.escape(entity.text), ' ENTITY ', new_sentence)\n",
    "            new_sentences.append(new_sentence)\n",
    "        sentences = new_sentences\n",
    "    if replace_dates:\n",
    "        if glove:\n",
    "            sentences = [re.sub(r'\\d{4}', ' <DATE> ', sentence) for sentence in sentences]\n",
    "        else:\n",
    "            sentences = [re.sub(r'\\d{4}', ' DATE ', sentence) for sentence in sentences]\n",
    "    if remove_numbers:\n",
    "        sentences = [re.sub(r'\\d+', ' ', sentence) for sentence in sentences]\n",
    "    if replace_copyright_symbols:\n",
    "        if glove:\n",
    "            symbol_text = ' <COPYRIGHT SYMBOL> '\n",
    "        else:\n",
    "            symbol_text = ' COPYRIGHTSYMBOL '\n",
    "        sentences = [re.sub(r'©', symbol_text, sentence) for sentence in sentences]\n",
    "        sentences = [re.sub(r'\\(c\\)', symbol_text, sentence) for sentence in sentences]\n",
    "        sentences = [re.sub(r'\\(C\\)', symbol_text, sentence) for sentence in sentences]\n",
    "    if replace_copyright_symbol_v2:\n",
    "        if glove:\n",
    "            symbol_text = ' <COPYRIGHT SYMBOL> '\n",
    "        else:\n",
    "            symbol_text = ' COPYRIGHT SYMBOL '\n",
    "        sentences = [re.sub(r'©', symbol_text, sentence) for sentence in sentences]\n",
    "        sentences = [re.sub(r'\\(c\\)', symbol_text, sentence) for sentence in sentences]\n",
    "        sentences = [re.sub(r'\\(C\\)', symbol_text, sentence) for sentence in sentences]\n",
    "    if replace_emails:\n",
    "        if glove:\n",
    "            email_text = ' <EMAIL> '\n",
    "        else:\n",
    "            email_text = ' EMAIL '\n",
    "        sentences = [re.sub(\"\"\"(?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\", email_text, sentence) for sentence in sentences]\n",
    "    if tokenize:\n",
    "        if not glove:\n",
    "            sentences = [''.join(tokenizer.tokenize(sentence)) for sentence in sentences]\n",
    "        else:\n",
    "            sentences = [tokenizer.tokenize(sentence) for sentence in sentences]\n",
    "    if lemmatize:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        if not glove:\n",
    "            sentences = [''.join([lemmatizer.lemmatize(word) for word in sentence]) for sentence in sentences]\n",
    "        else:\n",
    "            sentences = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in sentences]\n",
    "    if remove_punctuation:\n",
    "        sentences = [re.sub(r'[^\\w\\s]', ' ', sentence) for sentence in sentences]\n",
    "    if remove_special_characters:\n",
    "        sentences = [re.sub(r'[^a-zA-Z0-9]', ' ', sentence) for sentence in sentences]\n",
    "    if remove_specials_weird:\n",
    "        sentences = [re.sub(r'[^a-zA-Z0-9]', '', sentence) for sentence in sentences]\n",
    "    if lower:\n",
    "        sentences = [sentence.lower() for sentence in sentences]\n",
    "    if remove_stopwords:\n",
    "        sentences = [re.sub(r'\\b(?:{})\\b'.format('|'.join(stopwords.words('english'))), ' ', sentence)\n",
    "                     for sentence in sentences]\n",
    "    if remove_endlines:\n",
    "        sentences = [re.sub(r'\\n', ' ', sentence) for sentence in sentences]\n",
    "    if remove_whitespaces:\n",
    "        sentences = [re.sub(r' {2,}', ' ', sentence) for sentence in sentences]\n",
    "    return sentences\n",
    "\n",
    "def train(svm, vectorizer, threshold, preprocess_function,**kwargs):\n",
    "    X_train_tfidf = vectorizer.fit_transform(preprocess_function(X_train, **kwargs))\n",
    "    X_test_tfidf = vectorizer.transform(preprocess_function(X_test, **kwargs))\n",
    "    X_1_tfidf = vectorizer.transform(preprocess_function(X_1, **kwargs))\n",
    "    X_2_tfidf = vectorizer.transform(preprocess_function(X_2, **kwargs))\n",
    "    X_3_tfidf = vectorizer.transform(preprocess_function(X_3, **kwargs))\n",
    "    preprocessed_X = preprocess_function(X, **kwargs)\n",
    "    X_tfidf = vectorizer.transform(preprocessed_X)\n",
    "    svm.fit(X_train_tfidf, y_train)\n",
    "    if True: #svm.probability:\n",
    "        y_pred = svm.predict_proba(X_test_tfidf)\n",
    "        y_pred_1 = svm.predict_proba(X_1_tfidf)\n",
    "        y_pred_2 = svm.predict_proba(X_2_tfidf)\n",
    "        y_pred_3 = svm.predict_proba(X_3_tfidf)\n",
    "        y_pred_4 = svm.predict_proba(X_tfidf)\n",
    "        if threshold is None:\n",
    "            y_pred_classification = np.argmax(y_pred, axis=1)\n",
    "            y_pred_1_classification = np.argmax(y_pred_1, axis=1)\n",
    "            y_pred_2_classification = np.argmax(y_pred_2, axis=1)\n",
    "            y_pred_3_classification = np.argmax(y_pred_3, axis=1)\n",
    "            y_pred_4_classification = np.argmax(y_pred_4, axis=1)\n",
    "        else:\n",
    "            y_pred_classification = [np.argmax(y) if max(y) > threshold else 0 for y in y_pred]\n",
    "            y_pred_1_classification = [np.argmax(y) if max(y) > threshold else 0 for y in y_pred_1]\n",
    "            y_pred_2_classification = [np.argmax(y) if max(y) > threshold else 0 for y in y_pred_2]\n",
    "            y_pred_3_classification = [np.argmax(y) if max(y) > threshold else 0 for y in y_pred_3]\n",
    "            y_pred_4_classification = [np.argmax(y) if max(y) > threshold else 0 for y in y_pred_4]\n",
    "    else:\n",
    "        y_pred_classification = svm.predict(X_test_tfidf)\n",
    "        y_pred_1_classification = svm.predict(X_1_tfidf)\n",
    "        y_pred_2_classification = svm.predict(X_2_tfidf)\n",
    "        y_pred_3_classification = svm.predict(X_3_tfidf)\n",
    "        y_pred_4_classification = svm.predict(X_tfidf)\n",
    "    report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "    report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "    report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "    report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "    report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "    miss_classified_rows_0 = get_missclassified_rows(preprocessed_X, y, y_pred_4_classification, only_this_class=[0], return_index=True)\n",
    "    miss_classified_rows_1 = get_missclassified_rows(preprocessed_X, y, y_pred_4_classification, only_this_class=[1], return_index=True)\n",
    "    #aggregate_reports([report, report_1, report_2, report_3, report_4])\n",
    "    print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "    print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "    return svm, vectorizer, miss_classified_rows_0, miss_classified_rows_1, preprocessed_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model_outputs_to_disk(X, model_path, path=None):\n",
    "    model = spacy.load(model_path)\n",
    "    X_enitities = [model(sentence) for sentence in tqdm(X)]\n",
    "    train_docs, valid_docs = train_test_split(X_enitities, test_size=0.2, random_state=42, shuffle=True)\n",
    "    train_doc_bin = spacy.tokens.DocBin(docs=train_docs)\n",
    "    valid_doc_bin = spacy.tokens.DocBin(docs=valid_docs)\n",
    "    if path is None:\n",
    "        files = os.listdir('../NER_datasets/spacy_compatible/copyright-data/')\n",
    "        files = [file for file in files if file.startswith('entire-dataset')]\n",
    "        iteration_number = int(len(files) / 2 + 1)\n",
    "        train_path = '../NER_datasets/spacy_compatible/copyright-data/entire-dataset-semi-supervised-train-' \\\n",
    "                    + str(iteration_number) + '.spacy'\n",
    "        valid_path = '../NER_datasets/spacy_compatible/copyright-data/entire-dataset-semi-supervised-valid-' \\\n",
    "                    + str(iteration_number) + '.spacy'\n",
    "    else:\n",
    "        train_path = path + '-train.spacy'\n",
    "        valid_path = path + '-valid.spacy'\n",
    "    train_doc_bin.to_disk(train_path)\n",
    "    valid_doc_bin.to_disk(valid_path)\n",
    "\n",
    "def create_semi_supervised_config_file():\n",
    "    files = os.listdir('../NER_configs/')\n",
    "    files = [file for file in files if file.startswith('train-semi-supervised')]\n",
    "    iteration_number = len(files) + 1\n",
    "    \n",
    "    with open(os.path.join('../NER_configs/', files[-1]), 'r', encoding='utf-8') as file:\n",
    "        cfg_contents = file.read()\n",
    "    \n",
    "    new_train_path = f'../NER_datasets/spacy_compatible/copyright-data/entire-dataset-semi-supervised-train-{iteration_number}.spacy'\n",
    "    new_dev_path = f'../NER_datasets/spacy_compatible/copyright-data/entire-dataset-semi-supervised-valid-{iteration_number}.spacy'\n",
    "    \n",
    "    cfg_contents = re.sub(r'train\\s*=\\s*\".*\"', f'train = \"{new_train_path}\"', cfg_contents)\n",
    "    cfg_contents = re.sub(r'dev\\s*=\\s*\".*\"', f'dev = \"{new_dev_path}\"', cfg_contents)\n",
    "    \n",
    "    new_cfg_file_path = f'../NER_configs/train-semi-supervised-dataset-{iteration_number}.cfg'\n",
    "    with open(new_cfg_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(cfg_contents)z\n",
    "\n",
    "def train_semi_supervised_ner_model(X, model_path=None, path=None, epochs=10):\n",
    "    for i in range(epochs):\n",
    "        if model_path is None:\n",
    "            files = os.listdir('../NER_models/')\n",
    "            files = [file for file in files if file.startswith('train-semi-supervised-dataset')]\n",
    "            iteration_number = len(files)\n",
    "            model_path = f'../NER_models/train-semi-supervised-dataset-{iteration_number}/model-best'\n",
    "        write_model_outputs_to_disk(X, model_path, path)\n",
    "        create_semi_supervised_config_file()\n",
    "\n",
    "        files = os.listdir('../NER_models/')\n",
    "        files = [file for file in files if file.startswith('train-semi-supervised-dataset')]\n",
    "        iteration_number = len(files) + 1\n",
    "        print('Training model number: ', iteration_number)\n",
    "        train_command = f\"python -m spacy train '../NER_configs/train-semi-supervised-dataset-{iteration_number}.cfg' --output '../NER_models/train-semi-supervised-dataset-{iteration_number}'\"\n",
    "        os.system(train_command)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case I need to go through more iterations (currently not as F1-Score is 0.965)\n",
    "train_semi_supervised_ner_model(X, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missclassifications in class 0:  25.0 out of a total sample of:  16377.0  - about  0.15 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  43.0 out of a total sample of:  5393.0  - about  0.8 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(SVC(probability=True, C=25))\n",
    "test = train(clf, TfidfVectorizer(ngram_range=(1, 2), binary=True), None, preprocess_function,\n",
    "                                            replace_dates=True, remove_numbers=True, replace_copyright_symbols=True,\n",
    "                                            remove_whitespaces=True, lower=True, remove_special_characters=True, \n",
    "                                            replace_emails=True, replace_entities=True,\n",
    "                                            spacy_model=spacy.load('../NER_models/train-semi-supervised-dataset-1/model-best'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missclassifications in class 0:  2.0 out of a total sample of:  16377.0  - about  0.01 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  318.0 out of a total sample of:  5393.0  - about  5.9 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "clf = OneVsRestClassifier(SVC(probability=True, C=25))\n",
    "best_model = train(clf, TfidfVectorizer(ngram_range=(1, 2), binary=True), 0.99, preprocess_function,\n",
    "                                            replace_dates=True, remove_numbers=True, replace_copyright_symbols=True,\n",
    "                                            remove_whitespaces=True, lower=True, remove_special_characters=True, \n",
    "                                            replace_emails=True, replace_entities=True,\n",
    "                                            spacy_model=spacy.load('../NER_models/train-semi-supervised-dataset-1/model-best'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of missclassifications in class 0:  3.0 out of a total sample of:  16377.0  - about  0.02 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  301.0 out of a total sample of:  5393.0  - about  5.58 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(probability=True, C=25)\n",
    "test_2 = train(clf, TfidfVectorizer(ngram_range=(1, 2), binary=True), 0.99, preprocess_function,\n",
    "                                            replace_dates=True, remove_numbers=True, replace_copyright_symbols=True,\n",
    "                                            remove_whitespaces=True, lower=True, remove_special_characters=True, \n",
    "                                            replace_emails=True, replace_entities=True,\n",
    "                                            spacy_model=spacy.load('../NER_models/train-semi-supervised-dataset-1/model-best'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copyrightfpd.CopyrightFPD import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = CopyrightFPD()\n",
    "test_predictions = c.predict(X_test, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.992336 | 0.978022 | precision |\n",
      "| Mean | 0.992336 | 0.978022 | precision |\n",
      "## Recall\n",
      "|      |       0 |        1 | Metric   |\n",
      "|:-----|--------:|---------:|:---------|\n",
      "| 0    | 0.99264 | 0.977127 | recall   |\n",
      "| Mean | 0.99264 | 0.977127 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.992488 | 0.977574 | f1-score |\n",
      "| Mean | 0.992488 | 0.977574 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#test_predictions = [0 if prediction == 't' else 1 for prediction in test_predictions]\n",
    "print(aggregate_reports([classification_report(y_test, test_predictions, output_dict=True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9923359901900675,\n",
       "  'recall': 0.9926402943882244,\n",
       "  'f1-score': 0.9924881189636671,\n",
       "  'support': 3261.0},\n",
       " '1': {'precision': 0.978021978021978,\n",
       "  'recall': 0.9771271729185728,\n",
       "  'f1-score': 0.9775743707093821,\n",
       "  'support': 1093.0},\n",
       " 'accuracy': 0.9887459807073955,\n",
       " 'macro avg': {'precision': 0.9851789841060228,\n",
       "  'recall': 0.9848837336533987,\n",
       "  'f1-score': 0.9850312448365246,\n",
       "  'support': 4354.0},\n",
       " 'weighted avg': {'precision': 0.9887426931529242,\n",
       "  'recall': 0.9887459807073955,\n",
       "  'f1-score': 0.9887442680583078,\n",
       "  'support': 4354.0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(y_test, test_predictions, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9956709956709957,\n",
       "  'recall': 0.9871244635193133,\n",
       "  'f1-score': 0.9913793103448276,\n",
       "  'support': 466.0},\n",
       " '1': {'precision': 0.9478260869565217,\n",
       "  'recall': 0.9819819819819819,\n",
       "  'f1-score': 0.9646017699115044,\n",
       "  'support': 111.0},\n",
       " 'accuracy': 0.9861351819757366,\n",
       " 'macro avg': {'precision': 0.9717485413137588,\n",
       "  'recall': 0.9845532227506476,\n",
       "  'f1-score': 0.977990540128166,\n",
       "  'support': 577.0},\n",
       " 'weighted avg': {'precision': 0.9864668624520936,\n",
       "  'recall': 0.9861351819757366,\n",
       "  'f1-score': 0.9862279984070479,\n",
       "  'support': 577.0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_1 = c.predict(X_1, 0.5)\n",
    "test_predictions_1 = [0 if prediction == 't' else 1 for prediction in test_predictions_1]\n",
    "classification_report(y_1, test_predictions_1, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9738562091503268,\n",
       "  'recall': 0.9612903225806452,\n",
       "  'f1-score': 0.9675324675324675,\n",
       "  'support': 155.0},\n",
       " '1': {'precision': 0.9375,\n",
       "  'recall': 0.9574468085106383,\n",
       "  'f1-score': 0.9473684210526315,\n",
       "  'support': 94.0},\n",
       " 'accuracy': 0.9598393574297188,\n",
       " 'macro avg': {'precision': 0.9556781045751634,\n",
       "  'recall': 0.9593685655456418,\n",
       "  'f1-score': 0.9574504442925496,\n",
       "  'support': 249.0},\n",
       " 'weighted avg': {'precision': 0.9601313751738982,\n",
       "  'recall': 0.9598393574297188,\n",
       "  'f1-score': 0.9599203375360635,\n",
       "  'support': 249.0}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_2 = c.predict(X_2, 0.5)\n",
    "test_predictions_2 = [0 if prediction == 't' else 1 for prediction in test_predictions_2]\n",
    "classification_report(y_2, test_predictions_2, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9994939271255061,\n",
       "  'recall': 0.9989883662114315,\n",
       "  'f1-score': 0.9992410827219833,\n",
       "  'support': 1977.0},\n",
       " '1': {'precision': 0.9809523809523809,\n",
       "  'recall': 0.9903846153846154,\n",
       "  'f1-score': 0.985645933014354,\n",
       "  'support': 104.0},\n",
       " 'accuracy': 0.9985583853916387,\n",
       " 'macro avg': {'precision': 0.9902231540389435,\n",
       "  'recall': 0.9946864907980235,\n",
       "  'f1-score': 0.9924435078681686,\n",
       "  'support': 2081.0},\n",
       " 'weighted avg': {'precision': 0.9985672953129137,\n",
       "  'recall': 0.9985583853916387,\n",
       "  'f1-score': 0.9985616518860422,\n",
       "  'support': 2081.0}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions_5 = c.predict(X_5, 0.5)\n",
    "test_predictions_5 = [0 if prediction == 't' else 1 for prediction in test_predictions_5]\n",
    "classification_report(y_5, test_predictions_5, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['false_positive_detection_vectorizer.pkl']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pypi package with the model\n",
    "import joblib\n",
    "joblib.dump(best_model[0], 'false_positive_detection_model.pkl')\n",
    "joblib.dump(best_model[1], 'false_positive_detection_vectorizer.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --upgrade setuptools wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../datasets/feature-extraction-paper.csv')\n",
    "test = test.rename(columns={'copyright': 'content'})\n",
    "test = test.drop(columns=['falsePositive'])\n",
    "# to json\n",
    "test.to_json('test.json', orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Copyrights for code taken from ext2:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>copyright notice, this list of conditions and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copyright notice, this list of conditions and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parts derived from drivers/block/rd.c, and dri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Original copyright notice follows:</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content\n",
       "0               Copyrights for code taken from ext2:\n",
       "1  copyright notice, this list of conditions and ...\n",
       "2  copyright notice, this list of conditions and ...\n",
       "3  Parts derived from drivers/block/rd.c, and dri...\n",
       "4                 Original copyright notice follows:"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>copyright</th>\n",
       "      <th>falsePositive</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>copyright/agent_tests/Unit/test_copyright src/...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>copyright/VERSION-copyright src/spdx2/agent_te...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copyright_list src/cli/fo_folder src/cli/fo_no...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>copyright/VERSION-keyword</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>copyright/VERSION-ecc</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19462</th>\n",
       "      <td>copyright_path = '/bin/copyright' keyword_path...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19463</th>\n",
       "      <td>copyright violation found\")</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19464</th>\n",
       "      <td>© 2009 Hewlett-Packard Development Company, L.P.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19465</th>\n",
       "      <td>copyright_library.py usr/local/lib/python_foss...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19466</th>\n",
       "      <td>© 2017 Maximilian Huber</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19467 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               copyright  falsePositive  pred\n",
       "0      copyright/agent_tests/Unit/test_copyright src/...              1     1\n",
       "1      copyright/VERSION-copyright src/spdx2/agent_te...              1     1\n",
       "2      copyright_list src/cli/fo_folder src/cli/fo_no...              1     1\n",
       "3                              copyright/VERSION-keyword              1     1\n",
       "4                                  copyright/VERSION-ecc              1     1\n",
       "...                                                  ...            ...   ...\n",
       "19462  copyright_path = '/bin/copyright' keyword_path...              1     1\n",
       "19463                        copyright violation found\")              1     0\n",
       "19464   © 2009 Hewlett-Packard Development Company, L.P.              0     0\n",
       "19465  copyright_library.py usr/local/lib/python_foss...              1     1\n",
       "19466                            © 2017 Maximilian Huber              0     0\n",
       "\n",
       "[19467 rows x 3 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(columns=['copyright', 'falsePositive'])\n",
    "new_df['copyright'] = X\n",
    "new_df['falsePositive'] = y\n",
    "new_df.to_csv('false_positive_detection_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>copyright</th>\n",
       "      <th>falsePositive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>copyright/agent_tests/Unit/test_copyright src/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>copyright/VERSION-copyright src/spdx2/agent_te...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>copyright_list src/cli/fo_folder src/cli/fo_no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>copyright/VERSION-keyword</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>copyright/VERSION-ecc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           copyright  falsePositive\n",
       "0  copyright/agent_tests/Unit/test_copyright src/...              1\n",
       "1  copyright/VERSION-copyright src/spdx2/agent_te...              1\n",
       "2  copyright_list src/cli/fo_folder src/cli/fo_no...              1\n",
       "3                          copyright/VERSION-keyword              1\n",
       "4                              copyright/VERSION-ecc              1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('false_positive_detection_dataset.csv')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
