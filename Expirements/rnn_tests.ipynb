{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import numpy as np\n",
    "import string\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 Percentage:  0.7385852090032154\n",
      "Class 1 Percentage:  0.26141479099678455\n"
     ]
    }
   ],
   "source": [
    "data_0 = pd.read_csv('../datasets/fossology-master.csv')\n",
    "X_0 = data_0[\"copyright\"]\n",
    "y_0 = data_0[\"falsePositive\"]\n",
    "X_0 = X_0.drop_duplicates()\n",
    "y_0 = y_0[X_0.index]\n",
    "\n",
    "data_1 = pd.read_csv('../datasets/kubernetes-master.csv')\n",
    "X_1 = data_1[\"copyright\"]\n",
    "y_1 = data_1[\"falsePositive\"]\n",
    "X_1 = X_1.drop_duplicates()\n",
    "y_1 = y_1[X_1.index]\n",
    "\n",
    "data_2 = pd.read_csv('../datasets/tensorflow-master.csv')\n",
    "X_2 = data_2[\"copyright\"]\n",
    "y_2 = data_2[\"falsePositive\"]\n",
    "X_2 = X_2.drop_duplicates()\n",
    "y_2 = y_2[X_2.index]\n",
    "\n",
    "data_3 = pd.read_csv('../datasets/Fossology-Provided-Dataset-1.csv')\n",
    "\n",
    "X_3 = data_3['scanner_content']\n",
    "y_3 = data_3['falsePositive']\n",
    "X_3 = X_3.drop_duplicates()\n",
    "y_3 = y_3[X_3.index]\n",
    "\n",
    "X = pd.concat([X_0, X_1, X_2, X_3])\n",
    "y = pd.concat([y_0, y_1, y_2, y_3])\n",
    "\n",
    "print('Class 0 Percentage: ', len(y[y == 0]) / len(y))\n",
    "print('Class 1 Percentage: ', len(y[y == 1]) / len(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_0, y_0, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_reports(reports, print_aggregates=True):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    dfs = []\n",
    "    for metric in ['precision', 'recall', 'f1-score']:\n",
    "        scores = []\n",
    "        for report in reports:\n",
    "            scores.append([report['0'][metric], report['1'][metric]])\n",
    "        scores = np.array(scores)\n",
    "        scores = scores[:, :2]\n",
    "        mean_scores = np.mean(scores, axis=0)\n",
    "        mean_scores = [f\"{score:.6f}\" for score in mean_scores]\n",
    "        df = pd.DataFrame(scores, columns=['0', '1'])\n",
    "        df.loc['Mean'] = mean_scores\n",
    "        df['Metric'] = metric\n",
    "        dfs.append(df)\n",
    "    if print_aggregates:\n",
    "        print(\"## Precision\")\n",
    "        print(dfs[0].to_markdown())\n",
    "        print(\"## Recall\")\n",
    "        print(dfs[1].to_markdown())\n",
    "        print(\"## F1-score\")\n",
    "        print(dfs[2].to_markdown())\n",
    "    else:\n",
    "        return dfs[0], dfs[1], dfs[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "X_1_tfidf = vectorizer.transform(X_1)\n",
    "X_2_tfidf = vectorizer.transform(X_2)\n",
    "X_3_tfidf = vectorizer.transform(X_3)\n",
    "X_tfidf = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:06<00:13,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.139624834060669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:13<00:06,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 2.1396169662475586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:19<00:00,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 2.1396145820617676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_size = 100 # Set desired input size\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "batch_size = 32 # Set batch size\n",
    "\n",
    "embedding_size = 14641 # Set embedding size to match number of features in X_train_tfidf\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, embedding_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Linear(embedding_size, input_size)\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x)\n",
    "        return output, hidden\n",
    "\n",
    "rnn = RNNModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, embedding_size=embedding_size)\n",
    "\n",
    "device = 'cpu' #torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "rnn = rnn.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=0.01)\n",
    "\n",
    "# Convert sparse matrix to dense and then to PyTorch tensor\n",
    "X_train_tfidf_tensor = torch.from_numpy(X_train_tfidf.toarray())\n",
    "y_train_tensor = torch.from_numpy(y_train.to_numpy())\n",
    "\n",
    "# Move data to device\n",
    "X_train_tfidf_tensor = X_train_tfidf_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "\n",
    "# Convert input data to torch.float32\n",
    "X_train_tfidf_tensor = X_train_tfidf_tensor.float()\n",
    "\n",
    "# Convert target data to torch.long\n",
    "y_train_tensor = y_train_tensor.long()\n",
    "\n",
    "# Create data loader for batching\n",
    "train_data = TensorDataset(X_train_tfidf_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "epochs = 3\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = rnn(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = []\n",
    "datasets = [(X_test_tfidf, y_test), (X_1_tfidf, y_1), (X_2_tfidf, y_2), (X_3_tfidf, y_3), (X_tfidf, y)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        X_temp_tfidf_tensor = torch.from_numpy(dataset[0].toarray())\n",
    "        y_temp_tensor =  torch.from_numpy(dataset[1].to_numpy())\n",
    "        X_temp_tfidf_tensor = X_temp_tfidf_tensor.to(device)\n",
    "        X_temp_tfidf_tensor = X_temp_tfidf_tensor.float()\n",
    "        y_temp_tensor = y_temp_tensor.to(device)\n",
    "        y_temp_tensor = y_temp_tensor.long()\n",
    "        temp_data = TensorDataset(X_temp_tfidf_tensor, y_temp_tensor)\n",
    "        temp_loader = DataLoader(temp_data, shuffle=True, batch_size=batch_size)\n",
    "        y_pred = []\n",
    "        for inputs, labels in tqdm(temp_loader):\n",
    "            temp, _ = rnn(inputs)\n",
    "            y_pred.append(temp)\n",
    "        y_pred = torch.cat(y_pred, dim=0)\n",
    "        y_pred = torch.argmax(y_pred, dim= 1)\n",
    "        y_pred = y_pred.to('cpu')\n",
    "        reports.append(classification_report(dataset[1], y_pred))\n",
    "        #del X_temp_tfidf_tensor\n",
    "        #del y_temp_tensor\n",
    "        #del temp_data\n",
    "        #del temp_loader\n",
    "        #del y_pred\n",
    "        #torch.cuda.empty_cache()\n",
    "\n",
    "print(aggregate_reports(reports))\n",
    "print('Number of missclassifications in class 0: ', reports[4]['0']['support'] - round(reports[4]['0']['recall'] * reports[4]['0']['support']), 'out of a total sample of: ', reports[4]['0']['support'], ' - about ', round((1 - reports[4]['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', reports[4]['1']['support'] - round(reports[4]['1']['recall'] * reports[4]['1']['support']), 'out of a total sample of: ', reports[4]['1']['support'], ' - about ', round((1 - reports[4]['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      1.00      0.85      2870\n",
      "           1       0.00      0.00      0.00      1024\n",
      "\n",
      "    accuracy                           0.74      3894\n",
      "   macro avg       0.37      0.50      0.42      3894\n",
      "weighted avg       0.54      0.74      0.63      3894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(reports[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "model = FastText(vector_size=500, window=5, min_count=10, workers=6)\n",
    "\n",
    "model.build_vocab(X_train)\n",
    "\n",
    "model.train(X_train, total_examples=len(X_train), epochs=10)\n",
    "\n",
    "X_train_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_train]\n",
    "X_test_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_test]\n",
    "\n",
    "X_1_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_1]\n",
    "X_2_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_2]\n",
    "X_3_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_3]\n",
    "X_ft = [model.wv.get_sentence_vector(sentence) for sentence in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:00<00:17,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3.147402048110962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [00:01<00:15,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 2.827108383178711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:02<00:14,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 2.5307514667510986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [00:03<00:13,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 2.3667521476745605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [00:04<00:12,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 2.2474281787872314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [00:04<00:11,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 2.2089412212371826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [00:05<00:10,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 2.1882436275482178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [00:06<00:09,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 2.176600933074951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [00:07<00:08,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 2.165022134780884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [00:08<00:08,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 2.1600875854492188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [00:08<00:07,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 2.1576340198516846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [00:09<00:06,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 2.1513583660125732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [00:10<00:05,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 2.148862838745117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [00:11<00:04,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 2.14841890335083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [00:12<00:04,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 2.1457128524780273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [00:13<00:03,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 2.1488726139068604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [00:13<00:02,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 2.1432182788848877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [00:14<00:01,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 2.1429619789123535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [00:15<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 2.1421453952789307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:16<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 2.1418404579162598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_size = 500 # Set desired input size\n",
    "hidden_size = 50\n",
    "num_layers = 2\n",
    "batch_size = 32 # Set batch size\n",
    "\n",
    "embedding_size = 500 # Set embedding size to match number of features in X_train_ft\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, embedding_size):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Linear(embedding_size, input_size)\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, hidden = self.rnn(x)\n",
    "        return output, hidden\n",
    "\n",
    "rnn = RNNModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, embedding_size=embedding_size)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "rnn = rnn.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(rnn.parameters(), lr=1e-5)\n",
    "\n",
    "# Convert sparse matrix to dense and then to PyTorch tensor\n",
    "X_train_ft_tensor = torch.from_numpy(np.array(X_train_ft))\n",
    "y_train_tensor = torch.from_numpy(y_train.to_numpy())\n",
    "\n",
    "# Move data to device\n",
    "X_train_ft_tensor = X_train_ft_tensor.to(device)\n",
    "y_train_tensor = y_train_tensor.to(device)\n",
    "\n",
    "# Convert input data to torch.float32\n",
    "X_train_ft_tensor = X_train_ft_tensor.float()\n",
    "\n",
    "# Convert target data to torch.long\n",
    "y_train_tensor = y_train_tensor.long()\n",
    "\n",
    "# Create data loader for batching\n",
    "train_data = TensorDataset(X_train_ft_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "epochs = 20\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = rnn(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch {epoch+1}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/122 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 122/122 [00:00<00:00, 1746.23it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 1285.04it/s]\n",
      "100%|██████████| 8/8 [00:00<00:00, 1324.74it/s]\n",
      "100%|██████████| 47/47 [00:00<00:00, 1692.28it/s]\n",
      "100%|██████████| 681/681 [00:00<00:00, 1850.82it/s]\n",
      "/home/jimbo/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jimbo/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/jimbo/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30833/2180970787.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m#torch.cuda.empty_cache()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggregate_reports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreports\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of missclassifications in class 0: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreports\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'support'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreports\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreports\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'support'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'out of a total sample of: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreports\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'support'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' - about '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mreports\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'% of the class was missclassified'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Number of missclassifications in class 1: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreports\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'support'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreports\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreports\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'support'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'out of a total sample of: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreports\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'support'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' - about '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mreports\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'recall'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'% of the class was missclassified'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_30833/807091468.py\u001b[0m in \u001b[0;36maggregate_reports\u001b[0;34m(reports, print_aggregates)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mreport\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreports\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "reports = []\n",
    "datasets = [(X_test_ft, y_test), (X_1_ft, y_1), (X_2_ft, y_2), (X_3_ft, y_3), (X_ft, y)]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for dataset in datasets:\n",
    "        X_temp_tfidf_tensor = torch.from_numpy(np.array(dataset[0]))\n",
    "        y_temp_tensor =  torch.from_numpy(dataset[1].to_numpy())\n",
    "        X_temp_tfidf_tensor = X_temp_tfidf_tensor.to(device)\n",
    "        X_temp_tfidf_tensor = X_temp_tfidf_tensor.float()\n",
    "        y_temp_tensor = y_temp_tensor.to(device)\n",
    "        y_temp_tensor = y_temp_tensor.long()\n",
    "        temp_data = TensorDataset(X_temp_tfidf_tensor, y_temp_tensor)\n",
    "        temp_loader = DataLoader(temp_data, shuffle=True, batch_size=batch_size)\n",
    "        y_pred = []\n",
    "        for inputs, labels in tqdm(temp_loader):\n",
    "            temp, _ = rnn(inputs)\n",
    "            y_pred.append(temp)\n",
    "        y_pred = torch.cat(y_pred, dim=0)\n",
    "        y_pred = torch.argmax(y_pred, dim= 1)\n",
    "        y_pred = y_pred.to('cpu')\n",
    "        reports.append(classification_report(dataset[1], y_pred))\n",
    "        #del X_temp_tfidf_tensor\n",
    "        #del y_temp_tensor\n",
    "        #del temp_data\n",
    "        #del temp_loader\n",
    "        #del y_pred\n",
    "        #torch.cuda.empty_cache()\n",
    "\n",
    "print(aggregate_reports(reports))\n",
    "print('Number of missclassifications in class 0: ', reports[4]['0']['support'] - round(reports[4]['0']['recall'] * reports[4]['0']['support']), 'out of a total sample of: ', reports[4]['0']['support'], ' - about ', round((1 - reports[4]['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', reports[4]['1']['support'] - round(reports[4]['1']['recall'] * reports[4]['1']['support']), 'out of a total sample of: ', reports[4]['1']['support'], ' - about ', round((1 - reports[4]['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.97      0.84      2870\n",
      "           1       0.23      0.03      0.05      1024\n",
      "\n",
      "    accuracy                           0.72      3894\n",
      "   macro avg       0.48      0.50      0.44      3894\n",
      "weighted avg       0.60      0.72      0.63      3894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(reports[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
