{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jimbo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jimbo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(sentences, labels, mode=-1):\n",
    "    sentences = sentences.to_list()\n",
    "    labels = labels.to_list()\n",
    "    # Remove stopwords and extra spaces\n",
    "    if mode == 0:\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for word in sentence.split():\n",
    "                if word in stop_words:\n",
    "                    sentence = sentence.replace(word, '')\n",
    "                else:\n",
    "                    sentence = sentence.replace(word, word.lower())\n",
    "            sentence = sentence.replace('  ', ' ')\n",
    "            sentences[i] = sentence\n",
    "    # Replace (c), (C), © with COPYRIGHT_SYMBOL then use the word_tokenize function instaed of split + previous\n",
    "    elif mode > 0:\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentence = sentence.replace('(c)', 'COPYRIGHT_SYMBOL')\n",
    "            sentence = sentence.replace('(C)', 'COPYRIGHT_SYMBOL')\n",
    "            sentence = sentence.replace('©', 'COPYRIGHT_SYMBOL')\n",
    "            tokens = word_tokenize(sentence)\n",
    "            if mode > 1: # Change tokens to lower case\n",
    "                tokens = [token.lower() for token in tokens]\n",
    "            if mode > 2: # Apply lemmatization\n",
    "                tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "            sentences[i] = tokens\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 Percentage:  0.738802884836235\n",
      "Class 1 Percentage:  0.261197115163765\n"
     ]
    }
   ],
   "source": [
    "data_0 = pd.read_csv('../cleared_datasets/fossology-master.csv')\n",
    "X_0 = data_0[\"copyright\"]\n",
    "y_0 = data_0[\"falsePositive\"]\n",
    "X_0 = X_0.drop_duplicates()\n",
    "y_0 = y_0[X_0.index]\n",
    "\n",
    "data_1 = pd.read_csv('../cleared_datasets/kubernetes-master.csv')\n",
    "X_1 = data_1[\"copyright\"]\n",
    "y_1 = data_1[\"falsePositive\"]\n",
    "X_1 = X_1.drop_duplicates()\n",
    "y_1 = y_1[X_1.index]\n",
    "\n",
    "data_2 = pd.read_csv('../cleared_datasets/tensorflow-master.csv')\n",
    "X_2 = data_2[\"copyright\"]\n",
    "y_2 = data_2[\"falsePositive\"]\n",
    "X_2 = X_2.drop_duplicates()\n",
    "y_2 = y_2[X_2.index]\n",
    "\n",
    "data_3 = pd.read_csv('../Fossology-Provided-Dataset-1.csv')\n",
    "\n",
    "X_3 = data_3['scanner_content']\n",
    "y_3 = data_3['falsePositive']\n",
    "X_3 = X_3.drop_duplicates()\n",
    "y_3 = y_3[X_3.index]\n",
    "\n",
    "X = pd.concat([X_0, X_1, X_2, X_3])\n",
    "y = pd.concat([y_0, y_1, y_2, y_3])\n",
    "\n",
    "print('Class 0 Percentage: ', len(y[y == 0]) / len(y))\n",
    "print('Class 1 Percentage: ', len(y[y == 1]) / len(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_0, y_0, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_reports(reports):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    dfs = []\n",
    "    for metric in ['precision', 'recall', 'f1-score']:\n",
    "        scores = []\n",
    "        for report in reports:\n",
    "            scores.append([report['0'][metric], report['1'][metric]])\n",
    "        scores = np.array(scores)\n",
    "        scores = scores[:, :2]\n",
    "        mean_scores = np.mean(scores, axis=0)\n",
    "        mean_scores = [f\"{score:.6f}\" for score in mean_scores]\n",
    "        df = pd.DataFrame(scores, columns=['0', '1'])\n",
    "        df.loc['Mean'] = mean_scores\n",
    "        df['Metric'] = metric\n",
    "        dfs.append(df)\n",
    "    print(\"## Precision\")\n",
    "    print(dfs[0].to_markdown())\n",
    "    print(\"## Recall\")\n",
    "    print(dfs[1].to_markdown())\n",
    "    print(\"## F1-score\")\n",
    "    print(dfs[2].to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Worlds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "X_1_bow = vectorizer.transform(X_1)\n",
    "\n",
    "X_2_bow = vectorizer.transform(X_2)\n",
    "\n",
    "X_3_bow = vectorizer.transform(X_3)\n",
    "\n",
    "X_bow = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.99361  | 0.934076 | precision |\n",
      "| 1    | 0.991892 | 0.623188 | precision |\n",
      "| 2    | 1        | 0.793103 | precision |\n",
      "| 3    | 1        | 0.76435  | precision |\n",
      "| 4    | 0.998423 | 0.95625  | precision |\n",
      "| Mean | 0.996785 | 0.814194 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.975261 | 0.982422 | recall   |\n",
      "| 1    | 0.824719 | 0.977273 | recall   |\n",
      "| 2    | 0.776119 | 1        | recall   |\n",
      "| 3    | 0.936222 | 1        | recall   |\n",
      "| 4    | 0.983896 | 0.995603 | recall   |\n",
      "| Mean | 0.899244 | 0.99106  | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.98435  | 0.957639 | f1-score |\n",
      "| 1    | 0.900613 | 0.761062 | f1-score |\n",
      "| 2    | 0.87395  | 0.884615 | f1-score |\n",
      "| 3    | 0.967061 | 0.866438 | f1-score |\n",
      "| 4    | 0.991106 | 0.97553  | f1-score |\n",
      "| Mean | 0.943416 | 0.889057 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_bow, y_train)\n",
    "y_pred = rf.predict(X_test_bow)\n",
    "y_pred_1 = rf.predict(X_1_bow)\n",
    "y_pred_2 = rf.predict(X_2_bow)\n",
    "y_pred_3 = rf.predict(X_3_bow)\n",
    "y_pred_4 = rf.predict(X_bow)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "X_1_tfidf = vectorizer.transform(X_1)\n",
    "\n",
    "X_2_tfidf = vectorizer.transform(X_2)\n",
    "\n",
    "X_3_tfidf = vectorizer.transform(X_3)\n",
    "\n",
    "X_tfidf = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.988792 | 0.954764 | precision |\n",
      "| 1    | 0.982234 | 0.68306  | precision |\n",
      "| 2    | 1        | 0.804196 | precision |\n",
      "| 3    | 0.99916  | 0.881119 | precision |\n",
      "| 4    | 0.997367 | 0.970593 | precision |\n",
      "| Mean | 0.99351  | 0.858746 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.983624 | 0.96875  | recall   |\n",
      "| 1    | 0.869663 | 0.94697  | recall   |\n",
      "| 2    | 0.791045 | 1        | recall   |\n",
      "| 3    | 0.9722   | 0.996047 | recall   |\n",
      "| 4    | 0.989368 | 0.992613 | recall   |\n",
      "| Mean | 0.92118  | 0.980876 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.986201 | 0.961706 | f1-score |\n",
      "| 1    | 0.922527 | 0.793651 | f1-score |\n",
      "| 2    | 0.883333 | 0.891473 | f1-score |\n",
      "| 3    | 0.985495 | 0.935065 | f1-score |\n",
      "| 4    | 0.993351 | 0.98148  | f1-score |\n",
      "| Mean | 0.954182 | 0.912675 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_tfidf, y_train)\n",
    "y_pred = rf.predict(X_test_tfidf)\n",
    "y_pred_1 = rf.predict(X_1_tfidf)\n",
    "y_pred_2 = rf.predict(X_2_tfidf)\n",
    "y_pred_3 = rf.predict(X_3_tfidf)\n",
    "y_pred_4 = rf.predict(X_tfidf)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "import numpy as np\n",
    "def load_glove(file):\n",
    "    \"\"\"Load GloVe embeddings from a text file.\n",
    "    Args:\n",
    "        file (str): path to the glove file.\n",
    "    Returns:\n",
    "        dict: a dictionary mapping words to their vector representations.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove50 = load_glove('../glove.6B/glove.6B.50d.txt')\n",
    "glove100 = load_glove('../glove.6B/glove.6B.100d.txt')\n",
    "glove200 = load_glove('../glove.6B/glove.6B.200d.txt')\n",
    "glove300 = load_glove('../glove.6B/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_embeddings(sentences, embeddings):\n",
    "    \"\"\"\n",
    "        Convert a list of sentences into a matrix of embeddings. \n",
    "        \n",
    "        Args:\n",
    "            sentences (list): a list of strings, each representing a sentence.\n",
    "            embeddings (dict): a dictionary mapping words to their vector representations.\n",
    "\n",
    "        Returns: \n",
    "            np.array: a 2D array of shape (len(sentences), len(embeddings[word])), where each\n",
    "                      row is the average of the word vectors in the sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    matrix = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        vectors = [embeddings.get(word.lower(), np.zeros(len(embeddings['the']))) for word in words] \n",
    "        mean = np.mean(vectors, axis=0) \n",
    "        matrix.append(mean)\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove50 = sentences_to_embeddings(X_train, glove50) \n",
    "X_test_glove50 = sentences_to_embeddings(X_test, glove50)\n",
    "\n",
    "X_1_glove50 = sentences_to_embeddings(X_1, glove50)\n",
    "X_2_glove50 = sentences_to_embeddings(X_2, glove50)\n",
    "X_3_glove50 = sentences_to_embeddings(X_3, glove50)\n",
    "X_glove50 = sentences_to_embeddings(X, glove50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.967185 | 0.92993  | precision |\n",
      "| 1    | 0.979651 | 0.536481 | precision |\n",
      "| 2    | 0.875    | 0.737226 | precision |\n",
      "| 3    | 0.995851 | 0.915129 | precision |\n",
      "| 4    | 0.990792 | 0.954342 | precision |\n",
      "| Mean | 0.961696 | 0.814622 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.97561  | 0.907227 | recall   |\n",
      "| 1    | 0.757303 | 0.94697  | recall   |\n",
      "| 2    | 0.731343 | 0.878261 | recall   |\n",
      "| 3    | 0.981194 | 0.980237 | recall   |\n",
      "| 4    | 0.983523 | 0.974147 | recall   |\n",
      "| Mean | 0.885795 | 0.937368 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.971379 | 0.918438 | f1-score |\n",
      "| 1    | 0.854246 | 0.684932 | f1-score |\n",
      "| 2    | 0.796748 | 0.801587 | f1-score |\n",
      "| 3    | 0.988468 | 0.946565 | f1-score |\n",
      "| 4    | 0.987144 | 0.964143 | f1-score |\n",
      "| Mean | 0.919597 | 0.863133 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_glove50, y_train)\n",
    "y_pred = rf.predict(X_test_glove50)\n",
    "y_pred_1 = rf.predict(X_1_glove50)\n",
    "y_pred_2 = rf.predict(X_2_glove50)\n",
    "y_pred_3 = rf.predict(X_3_glove50)\n",
    "y_pred_4 = rf.predict(X_glove50)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove100 = sentences_to_embeddings(X_train, glove100) \n",
    "X_test_glove100 = sentences_to_embeddings(X_test, glove100)\n",
    "\n",
    "X_1_glove100 = sentences_to_embeddings(X_1, glove100)\n",
    "X_2_glove100 = sentences_to_embeddings(X_2, glove100)\n",
    "X_3_glove100 = sentences_to_embeddings(X_3, glove100)\n",
    "X_glove100 = sentences_to_embeddings(X, glove100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.973976 | 0.937747 | precision |\n",
      "| 1    | 0.968927 | 0.542601 | precision |\n",
      "| 2    | 1        | 0.761589 | precision |\n",
      "| 3    | 0.997504 | 0.912409 | precision |\n",
      "| 4    | 0.992602 | 0.95703  | precision |\n",
      "| Mean | 0.986602 | 0.822275 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.978049 | 0.926758 | recall   |\n",
      "| 1    | 0.770787 | 0.916667 | recall   |\n",
      "| 2    | 0.731343 | 1        | recall   |\n",
      "| 3    | 0.980376 | 0.988142 | recall   |\n",
      "| 4    | 0.984456 | 0.979247 | recall   |\n",
      "| Mean | 0.889002 | 0.962163 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.976008 | 0.93222  | f1-score |\n",
      "| 1    | 0.858573 | 0.68169  | f1-score |\n",
      "| 2    | 0.844828 | 0.864662 | f1-score |\n",
      "| 3    | 0.988866 | 0.948767 | f1-score |\n",
      "| 4    | 0.988512 | 0.968011 | f1-score |\n",
      "| Mean | 0.931357 | 0.87907  | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_glove100, y_train)\n",
    "y_pred = rf.predict(X_test_glove100)\n",
    "y_pred_1 = rf.predict(X_1_glove100)\n",
    "y_pred_2 = rf.predict(X_2_glove100)\n",
    "y_pred_3 = rf.predict(X_3_glove100)\n",
    "y_pred_4 = rf.predict(X_glove100)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove200 = sentences_to_embeddings(X_train, glove200) \n",
    "X_test_glove200 = sentences_to_embeddings(X_test, glove200)\n",
    "\n",
    "X_1_glove200 = sentences_to_embeddings(X_1, glove200)\n",
    "X_2_glove200 = sentences_to_embeddings(X_2, glove200)\n",
    "X_3_glove200 = sentences_to_embeddings(X_3, glove200)\n",
    "X_glove200 = sentences_to_embeddings(X, glove200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.973282 | 0.935771 | precision |\n",
      "| 1    | 0.971279 | 0.623711 | precision |\n",
      "| 2    | 1        | 0.756579 | precision |\n",
      "| 3    | 0.995021 | 0.911439 | precision |\n",
      "| 4    | 0.992735 | 0.960179 | precision |\n",
      "| Mean | 0.986464 | 0.837536 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.977352 | 0.924805 | recall   |\n",
      "| 1    | 0.835955 | 0.916667 | recall   |\n",
      "| 2    | 0.723881 | 1        | recall   |\n",
      "| 3    | 0.980376 | 0.976285 | recall   |\n",
      "| 4    | 0.985637 | 0.979599 | recall   |\n",
      "| Mean | 0.90064  | 0.959471 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.975313 | 0.930255 | f1-score |\n",
      "| 1    | 0.898551 | 0.742331 | f1-score |\n",
      "| 2    | 0.839827 | 0.861423 | f1-score |\n",
      "| 3    | 0.987644 | 0.942748 | f1-score |\n",
      "| 4    | 0.989174 | 0.969792 | f1-score |\n",
      "| Mean | 0.938102 | 0.88931  | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_glove200, y_train)\n",
    "y_pred = rf.predict(X_test_glove200)\n",
    "y_pred_1 = rf.predict(X_1_glove200)\n",
    "y_pred_2 = rf.predict(X_2_glove200)\n",
    "y_pred_3 = rf.predict(X_3_glove200)\n",
    "y_pred_4 = rf.predict(X_glove200)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove300 = sentences_to_embeddings(X_train, glove300) \n",
    "X_test_glove300 = sentences_to_embeddings(X_test, glove300)\n",
    "\n",
    "X_1_glove300 = sentences_to_embeddings(X_1, glove300)\n",
    "X_2_glove300 = sentences_to_embeddings(X_2, glove300)\n",
    "X_3_glove300 = sentences_to_embeddings(X_3, glove300)\n",
    "X_glove300 = sentences_to_embeddings(X, glove300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.970609 | 0.937126 | precision |\n",
      "| 1    | 0.969152 | 0.638298 | precision |\n",
      "| 2    | 0.906542 | 0.739437 | precision |\n",
      "| 3    | 0.996675 | 0.912088 | precision |\n",
      "| 4    | 0.991683 | 0.961226 | precision |\n",
      "| Mean | 0.966932 | 0.837635 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.978049 | 0.916992 | recall   |\n",
      "| 1    | 0.847191 | 0.909091 | recall   |\n",
      "| 2    | 0.723881 | 0.913043 | recall   |\n",
      "| 3    | 0.980376 | 0.98419  | recall   |\n",
      "| 4    | 0.986072 | 0.976609 | recall   |\n",
      "| Mean | 0.903114 | 0.939985 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.974314 | 0.92695  | f1-score |\n",
      "| 1    | 0.904077 | 0.75     | f1-score |\n",
      "| 2    | 0.804979 | 0.817121 | f1-score |\n",
      "| 3    | 0.988458 | 0.946768 | f1-score |\n",
      "| 4    | 0.98887  | 0.968856 | f1-score |\n",
      "| Mean | 0.93214  | 0.881939 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_glove300, y_train)\n",
    "y_pred = rf.predict(X_test_glove300)\n",
    "y_pred_1 = rf.predict(X_1_glove300)\n",
    "y_pred_2 = rf.predict(X_2_glove300)\n",
    "y_pred_3 = rf.predict(X_3_glove300)\n",
    "y_pred_4 = rf.predict(X_glove300)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15573/15573 [00:14<00:00, 1105.83it/s]\n",
      "100%|██████████| 3894/3894 [00:03<00:00, 983.33it/s] \n",
      "100%|██████████| 577/577 [00:00<00:00, 1004.06it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 808.25it/s] \n",
      "100%|██████████| 1476/1476 [00:01<00:00, 1475.24it/s]\n",
      "100%|██████████| 21769/21769 [00:19<00:00, 1141.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "model = FastText(vector_size=500, window=5, min_count=10, workers=6)\n",
    "\n",
    "model.build_vocab(X_train)\n",
    "\n",
    "model.train(X_train, total_examples=len(X_train), epochs=10)\n",
    "\n",
    "X_train_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_train)]\n",
    "X_test_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_test)]\n",
    "\n",
    "X_1_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_1)]\n",
    "X_2_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_2)]\n",
    "X_3_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_3)]\n",
    "X_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.98768  | 0.939221 | precision |\n",
      "| 1    | 0.987562 | 0.725714 | precision |\n",
      "| 2    | 0.965812 | 0.840909 | precision |\n",
      "| 3    | 0.983375 | 0.85348  | precision |\n",
      "| 4    | 0.995993 | 0.970147 | precision |\n",
      "| Mean | 0.984085 | 0.865894 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.9777   | 0.96582  | recall   |\n",
      "| 1    | 0.892135 | 0.962121 | recall   |\n",
      "| 2    | 0.843284 | 0.965217 | recall   |\n",
      "| 3    | 0.967294 | 0.920949 | recall   |\n",
      "| 4    | 0.989243 | 0.988744 | recall   |\n",
      "| Mean | 0.933931 | 0.96057  | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.982665 | 0.952335 | f1-score |\n",
      "| 1    | 0.937426 | 0.827362 | f1-score |\n",
      "| 2    | 0.900398 | 0.898785 | f1-score |\n",
      "| 3    | 0.975268 | 0.885932 | f1-score |\n",
      "| 4    | 0.992607 | 0.979357 | f1-score |\n",
      "| Mean | 0.957673 | 0.908754 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_ft, y_train)\n",
    "y_pred = rf.predict(X_test_ft)\n",
    "y_pred_1 = rf.predict(X_1_ft)\n",
    "y_pred_2 = rf.predict(X_2_ft)\n",
    "y_pred_3 = rf.predict(X_3_ft)\n",
    "y_pred_4 = rf.predict(X_ft)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15573/15573 [00:00<00:00, 92867.90it/s] \n",
      "100%|██████████| 3894/3894 [00:00<00:00, 98637.06it/s]\n",
      "100%|██████████| 577/577 [00:00<00:00, 90685.11it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 62113.82it/s]\n",
      "100%|██████████| 1476/1476 [00:00<00:00, 99362.69it/s]\n",
      "100%|██████████| 21769/21769 [00:00<00:00, 86925.29it/s] \n"
     ]
    }
   ],
   "source": [
    "X_train_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X_train)]\n",
    "X_test_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X_test)]\n",
    "\n",
    "X_1_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X_1)]\n",
    "X_2_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X_2)]\n",
    "X_3_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X_3)]\n",
    "X_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15573/15573 [00:12<00:00, 1201.23it/s]\n",
      "100%|██████████| 3894/3894 [00:03<00:00, 1115.67it/s]\n",
      "100%|██████████| 577/577 [00:00<00:00, 1155.23it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 910.85it/s] \n",
      "100%|██████████| 1476/1476 [00:00<00:00, 1590.18it/s]\n",
      "100%|██████████| 21769/21769 [00:18<00:00, 1204.94it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "model = FastText(vector_size=500, window=5, min_count=10, workers=6)\n",
    "\n",
    "model.build_vocab(X_train_temp)\n",
    "\n",
    "model.train(X_train_temp, total_examples=len(X_train_temp), epochs=10)\n",
    "\n",
    "X_train_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_train_temp)]\n",
    "X_test_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_test_temp)]\n",
    "\n",
    "X_1_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_1_temp)]\n",
    "X_2_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_2_temp)]\n",
    "X_3_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_3_temp)]\n",
    "X_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_temp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.98518  | 0.926415 | precision |\n",
      "| 1    | 0.981865 | 0.65445  | precision |\n",
      "| 2    | 0.936937 | 0.782609 | precision |\n",
      "| 3    | 0.979236 | 0.838235 | precision |\n",
      "| 4    | 0.99492  | 0.962562 | precision |\n",
      "| Mean | 0.975628 | 0.832854 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.972822 | 0.958984 | recall   |\n",
      "| 1    | 0.851685 | 0.94697  | recall   |\n",
      "| 2    | 0.776119 | 0.93913  | recall   |\n",
      "| 3    | 0.964023 | 0.901186 | recall   |\n",
      "| 4    | 0.986445 | 0.985754 | recall   |\n",
      "| Mean | 0.910219 | 0.946405 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.978962 | 0.942418 | f1-score |\n",
      "| 1    | 0.912154 | 0.773994 | f1-score |\n",
      "| 2    | 0.84898  | 0.853755 | f1-score |\n",
      "| 3    | 0.97157  | 0.868571 | f1-score |\n",
      "| 4    | 0.990665 | 0.97402  | f1-score |\n",
      "| Mean | 0.940466 | 0.882552 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_ft, y_train)\n",
    "y_pred = rf.predict(X_test_ft)\n",
    "y_pred_1 = rf.predict(X_1_ft)\n",
    "y_pred_2 = rf.predict(X_2_ft)\n",
    "y_pred_3 = rf.predict(X_3_ft)\n",
    "y_pred_4 = rf.predict(X_ft)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT (Sentence Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers as st\n",
    "\n",
    "class BertSentenceEmbedder():\n",
    "    '''\n",
    "    An interface for converting given text into sentence BERT embeddings.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Load the pre-trained model and tokenizer\n",
    "        '''\n",
    "        self.model = st.SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def embed(self, sentences):\n",
    "        '''\n",
    "        Convert the given sentences into BERT embeddings.\n",
    "        :param sentences: A list of sentences to convert into BERT embeddings.\n",
    "        :return: A list of BERT embeddings for the given sentences.\n",
    "        '''\n",
    "        return self.model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "embedder = BertSentenceEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_st = [embedder.embed(sentence) for sentence in tqdm(X_train)]\n",
    "X_test_st = [embedder.embed(sentence) for sentence in tqdm(X_test)]\n",
    "\n",
    "X_1_st = [embedder.embed(sentence) for sentence in tqdm(X_1)]\n",
    "X_2_st = [embedder.embed(sentence) for sentence in tqdm(X_2)]\n",
    "X_3_st = [embedder.embed(sentence) for sentence in tqdm(X_3)]\n",
    "X_st = [embedder.embed(sentence) for sentence in tqdm(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_train_st])\n",
    "X_train_st = X_train_st.reshape(-1, 1)\n",
    "X_test_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_test_st])\n",
    "X_test_st = X_test_st.reshape(-1, 1)\n",
    "X_1_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_1_st])\n",
    "X_1_st = X_1_st.reshape(-1, 1)\n",
    "X_2_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_2_st])\n",
    "X_2_st = X_2_st.reshape(-1, 1)\n",
    "X_3_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_3_st])\n",
    "X_3_st = X_3_st.reshape(-1, 1)\n",
    "X_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_st])\n",
    "X_st = X_st.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.764063 | 0.37037  | precision |\n",
      "| 1    | 0.791932 | 0.320755 | precision |\n",
      "| 2    | 0.5625   | 0.54386  | precision |\n",
      "| 3    | 0.872541 | 0.338762 | precision |\n",
      "| 4    | 0.812331 | 0.586929 | precision |\n",
      "| Mean | 0.76     | 0.43     | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.828223 | 0.283203 | recall   |\n",
      "| 1    | 0.838202 | 0.257576 | recall   |\n",
      "| 2    | 0.80597  | 0.269565 | recall   |\n",
      "| 3    | 0.834015 | 0.411067 | recall   |\n",
      "| 4    | 0.897034 | 0.413823 | recall   |\n",
      "| Mean | 0.84     | 0.33     | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.79485  | 0.320974 | f1-score |\n",
      "| 1    | 0.81441  | 0.285714 | f1-score |\n",
      "| 2    | 0.662577 | 0.360465 | f1-score |\n",
      "| 3    | 0.852843 | 0.371429 | f1-score |\n",
      "| 4    | 0.852584 | 0.485405 | f1-score |\n",
      "| Mean | 0.8      | 0.36     | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_st, y_train)\n",
    "y_pred = rf.predict(X_test_st)\n",
    "y_pred_1 = rf.predict(X_1_st)\n",
    "y_pred_2 = rf.predict(X_2_st)\n",
    "y_pred_3 = rf.predict(X_3_st)\n",
    "y_pred_4 = rf.predict(X_st)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4665133, 15551690)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "model = Word2Vec(vector_size=500, window=5, min_count=1, workers=6)\n",
    "model.build_vocab(X_train)\n",
    "model.train(X_train, total_examples=len(X_train), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_sentence_embeddings(sentences, model):\n",
    "    sentence_embeddings = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        word_vectors = []\n",
    "        for word in sentence.split():\n",
    "            try:\n",
    "                word_vectors.append(model.wv.get_vector(word.lower()))\n",
    "            except KeyError:\n",
    "                word_vectors.append(np.zeros(500))\n",
    "        if word_vectors:\n",
    "            sentence_embeddings.append(np.mean(np.mean(word_vectors, axis=0), axis=0))\n",
    "    sentence_embeddings = np.array(sentence_embeddings)\n",
    "    sentence_embeddings = sentence_embeddings.reshape(-1, 1)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15573/15573 [00:01<00:00, 14584.76it/s]\n",
      "100%|██████████| 3894/3894 [00:00<00:00, 13863.01it/s]\n",
      "100%|██████████| 577/577 [00:00<00:00, 10798.29it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 12864.85it/s]\n",
      "100%|██████████| 1476/1476 [00:00<00:00, 16889.86it/s]\n",
      "100%|██████████| 21769/21769 [00:01<00:00, 14976.38it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_word2vec = get_word2vec_sentence_embeddings(X_train, model) \n",
    "X_test_word2vec = get_word2vec_sentence_embeddings(X_test, model)\n",
    "\n",
    "X_1_word2vec = get_word2vec_sentence_embeddings(X_1, model)\n",
    "X_2_word2vec = get_word2vec_sentence_embeddings(X_2, model)\n",
    "X_3_word2vec = get_word2vec_sentence_embeddings(X_3, model)\n",
    "X_word2vec = get_word2vec_sentence_embeddings(X, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.782076 | 0.800664 | precision |\n",
      "| 1    | 0.771058 | 0.22807  | precision |\n",
      "| 2    | 0.565657 | 0.568627 | precision |\n",
      "| 3    | 0.857445 | 0.864407 | precision |\n",
      "| 4    | 0.791227 | 0.851351 | precision |\n",
      "| Mean | 0.753493 | 0.662624 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.979094 | 0.235352 | recall   |\n",
      "| 1    | 0.802247 | 0.19697  | recall   |\n",
      "| 2    | 0.835821 | 0.252174 | recall   |\n",
      "| 3    | 0.993459 | 0.201581 | recall   |\n",
      "| 4    | 0.983585 | 0.265916 | recall   |\n",
      "| Mean | 0.918841 | 0.230398 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.869565 | 0.363774 | f1-score |\n",
      "| 1    | 0.786344 | 0.211382 | f1-score |\n",
      "| 2    | 0.674699 | 0.349398 | f1-score |\n",
      "| 3    | 0.920455 | 0.326923 | f1-score |\n",
      "| 4    | 0.876982 | 0.405253 | f1-score |\n",
      "| Mean | 0.825609 | 0.331346 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_word2vec, y_train)\n",
    "y_pred = rf.predict(X_test_word2vec)\n",
    "y_pred_1 = rf.predict(X_1_word2vec)\n",
    "y_pred_2 = rf.predict(X_2_word2vec)\n",
    "y_pred_3 = rf.predict(X_3_word2vec)\n",
    "y_pred_4 = rf.predict(X_word2vec)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
