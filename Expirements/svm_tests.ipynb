{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jimbo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jimbo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(sentences, labels, mode=-1):\n",
    "    sentences = sentences.to_list()\n",
    "    labels = labels.to_list()\n",
    "    # Remove stopwords and extra spaces\n",
    "    if mode == 0:\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for word in sentence.split():\n",
    "                if word in stop_words:\n",
    "                    sentence = sentence.replace(word, '')\n",
    "                else:\n",
    "                    sentence = sentence.replace(word, word.lower())\n",
    "            sentence = sentence.replace('  ', ' ')\n",
    "            sentences[i] = sentence\n",
    "    # Replace (c), (C), © with COPYRIGHT_SYMBOL then use the word_tokenize function instaed of split + previous\n",
    "    elif mode > 0:\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentence = sentence.replace('(c)', 'COPYRIGHT_SYMBOL') # (c) -> '(' 'c' ')\n",
    "            sentence = sentence.replace('(C)', 'COPYRIGHT_SYMBOL')\n",
    "            sentence = sentence.replace('©', 'COPYRIGHT_SYMBOL')\n",
    "            tokens = word_tokenize(sentence)\n",
    "            if mode > 1: # Change tokens to lower case\n",
    "                tokens = [token.lower() for token in tokens]\n",
    "            if mode > 2: # Apply lemmatization\n",
    "                tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "            sentences[i] = tokens\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 Percentage:  0.738802884836235\n",
      "Class 1 Percentage:  0.261197115163765\n"
     ]
    }
   ],
   "source": [
    "data_0 = pd.read_csv('../cleared_datasets/fossology-master.csv')\n",
    "X_0 = data_0[\"copyright\"]\n",
    "y_0 = data_0[\"falsePositive\"]\n",
    "X_0 = X_0.drop_duplicates()\n",
    "y_0 = y_0[X_0.index]\n",
    "\n",
    "data_1 = pd.read_csv('../cleared_datasets/kubernetes-master.csv')\n",
    "X_1 = data_1[\"copyright\"]\n",
    "y_1 = data_1[\"falsePositive\"]\n",
    "X_1 = X_1.drop_duplicates()\n",
    "y_1 = y_1[X_1.index]\n",
    "\n",
    "data_2 = pd.read_csv('../cleared_datasets/tensorflow-master.csv')\n",
    "X_2 = data_2[\"copyright\"]\n",
    "y_2 = data_2[\"falsePositive\"]\n",
    "X_2 = X_2.drop_duplicates()\n",
    "y_2 = y_2[X_2.index]\n",
    "\n",
    "data_3 = pd.read_csv('../Fossology-Provided-Dataset-1.csv')\n",
    "\n",
    "X_3 = data_3['scanner_content']\n",
    "y_3 = data_3['falsePositive']\n",
    "X_3 = X_3.drop_duplicates()\n",
    "y_3 = y_3[X_3.index]\n",
    "\n",
    "X = pd.concat([X_0, X_1, X_2, X_3])\n",
    "y = pd.concat([y_0, y_1, y_2, y_3])\n",
    "\n",
    "print('Class 0 Percentage: ', len(y[y == 0]) / len(y))\n",
    "print('Class 1 Percentage: ', len(y[y == 1]) / len(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_0, y_0, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_reports(reports):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    dfs = []\n",
    "    for metric in ['precision', 'recall', 'f1-score']:\n",
    "        scores = []\n",
    "        for report in reports:\n",
    "            scores.append([report['0'][metric], report['1'][metric]])\n",
    "        scores = np.array(scores)\n",
    "        scores = scores[:, :2]\n",
    "        mean_scores = np.mean(scores, axis=0)\n",
    "        mean_scores = [f\"{score:.6f}\" for score in mean_scores]\n",
    "        df = pd.DataFrame(scores, columns=['0', '1'])\n",
    "        df.loc['Mean'] = mean_scores\n",
    "        df['Metric'] = metric\n",
    "        dfs.append(df)\n",
    "    print(\"## Precision\")\n",
    "    print(dfs[0].to_markdown())\n",
    "    print(\"## Recall\")\n",
    "    print(dfs[1].to_markdown())\n",
    "    print(\"## F1-score\")\n",
    "    print(dfs[2].to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "X_1_bow = vectorizer.transform(X_1)\n",
    "\n",
    "X_2_bow = vectorizer.transform(X_2)\n",
    "\n",
    "X_3_bow = vectorizer.transform(X_3)\n",
    "\n",
    "X_bow = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.994346 | 0.947368 | precision |\n",
      "| 1    | 0.986911 | 0.651282 | precision |\n",
      "| 2    | 1        | 0.751634 | precision |\n",
      "| 3    | 1        | 0.893993 | precision |\n",
      "| 4    | 0.996347 | 0.955356 | precision |\n",
      "| Mean | 0.995521 | 0.839927 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.980488 | 0.984375 | recall   |\n",
      "| 1    | 0.847191 | 0.962121 | recall   |\n",
      "| 2    | 0.716418 | 1        | recall   |\n",
      "| 3    | 0.97547  | 1        | recall   |\n",
      "| 4    | 0.983647 | 0.9898   | recall   |\n",
      "| Mean | 0.900643 | 0.987259 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.987368 | 0.965517 | f1-score |\n",
      "| 1    | 0.911729 | 0.776758 | f1-score |\n",
      "| 2    | 0.834783 | 0.858209 | f1-score |\n",
      "| 3    | 0.987583 | 0.94403  | f1-score |\n",
      "| 4    | 0.989957 | 0.972273 | f1-score |\n",
      "| Mean | 0.942284 | 0.903357 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_bow, y_train)\n",
    "y_pred = svm.predict(X_test_bow)\n",
    "y_pred_1 = svm.predict(X_1_bow)\n",
    "y_pred_2 = svm.predict(X_2_bow)\n",
    "y_pred_3 = svm.predict(X_3_bow)\n",
    "y_pred_4 = svm.predict(X_bow)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "X_1_tfidf = vectorizer.transform(X_1)\n",
    "\n",
    "X_2_tfidf = vectorizer.transform(X_2)\n",
    "\n",
    "X_3_tfidf = vectorizer.transform(X_3)\n",
    "\n",
    "X_tfidf = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm = SVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict_proba(X_test_tfidf)\n",
    "y_pred = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.991262 | 0.967086 | precision |\n",
      "| 1    | 0.97284  | 0.703488 | precision |\n",
      "| 2    | 0.945312 | 0.892562 | precision |\n",
      "| 3    | 0.991701 | 0.896679 | precision |\n",
      "| 4    | 0.995004 | 0.974109 | precision |\n",
      "| Mean | 0.979224 | 0.886785 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.988153 | 0.975586 | recall   |\n",
      "| 1    | 0.885393 | 0.916667 | recall   |\n",
      "| 2    | 0.902985 | 0.93913  | recall   |\n",
      "| 3    | 0.977105 | 0.960474 | recall   |\n",
      "| 4    | 0.990736 | 0.98593  | recall   |\n",
      "| Mean | 0.948875 | 0.955558 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.989705 | 0.971317 | f1-score |\n",
      "| 1    | 0.927059 | 0.796053 | f1-score |\n",
      "| 2    | 0.923664 | 0.915254 | f1-score |\n",
      "| 3    | 0.984349 | 0.927481 | f1-score |\n",
      "| 4    | 0.992865 | 0.979984 | f1-score |\n",
      "| Mean | 0.963529 | 0.918018 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "y_pred_1 = svm.predict(X_1_tfidf)\n",
    "y_pred_2 = svm.predict(X_2_tfidf)\n",
    "y_pred_3 = svm.predict(X_3_tfidf)\n",
    "y_pred_4 = svm.predict(X_tfidf)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "import numpy as np\n",
    "def load_glove(file):\n",
    "    \"\"\"Load GloVe embeddings from a text file.\n",
    "    Args:\n",
    "        file (str): path to the glove file.\n",
    "    Returns:\n",
    "        dict: a dictionary mapping words to their vector representations.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove50 = load_glove('../glove.6B/glove.6B.50d.txt')\n",
    "glove100 = load_glove('../glove.6B/glove.6B.100d.txt')\n",
    "glove200 = load_glove('../glove.6B/glove.6B.200d.txt')\n",
    "glove300 = load_glove('../glove.6B/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_embeddings(sentences, embeddings):\n",
    "    \"\"\"\n",
    "        Convert a list of sentences into a matrix of embeddings. \n",
    "        \n",
    "        Args:\n",
    "            sentences (list): a list of strings, each representing a sentence.\n",
    "            embeddings (dict): a dictionary mapping words to their vector representations.\n",
    "\n",
    "        Returns: \n",
    "            np.array: a 2D array of shape (len(sentences), len(embeddings[word])), where each\n",
    "                      row is the average of the word vectors in the sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    matrix = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        vectors = [embeddings.get(word.lower(), np.zeros(len(embeddings['the']))) for word in words] \n",
    "        mean = np.mean(vectors, axis=0) \n",
    "        matrix.append(mean)\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove50 = sentences_to_embeddings(X_train, glove50) \n",
    "X_test_glove50 = sentences_to_embeddings(X_test, glove50)\n",
    "\n",
    "X_1_glove50 = sentences_to_embeddings(X_1, glove50)\n",
    "X_2_glove50 = sentences_to_embeddings(X_2, glove50)\n",
    "X_3_glove50 = sentences_to_embeddings(X_3, glove50)\n",
    "X_glove50 = sentences_to_embeddings(X, glove50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.971258 | 0.904899 | precision |\n",
      "| 1    | 0.986339 | 0.601896 | precision |\n",
      "| 2    | 0.915254 | 0.801527 | precision |\n",
      "| 3    | 0.995847 | 0.911765 | precision |\n",
      "| 4    | 0.979161 | 0.902747 | precision |\n",
      "| Mean | 0.969572 | 0.824567 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.965505 | 0.919922 | recall   |\n",
      "| 1    | 0.811236 | 0.962121 | recall   |\n",
      "| 2    | 0.80597  | 0.913043 | recall   |\n",
      "| 3    | 0.980376 | 0.980237 | recall   |\n",
      "| 4    | 0.964124 | 0.941963 | recall   |\n",
      "| Mean | 0.905442 | 0.943457 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.968373 | 0.912349 | f1-score |\n",
      "| 1    | 0.890259 | 0.740525 | f1-score |\n",
      "| 2    | 0.857143 | 0.853659 | f1-score |\n",
      "| 3    | 0.988051 | 0.944762 | f1-score |\n",
      "| 4    | 0.971584 | 0.921938 | f1-score |\n",
      "| Mean | 0.935082 | 0.874646 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_glove50, y_train)\n",
    "y_pred = svm.predict(X_test_glove50)\n",
    "y_pred_1 = svm.predict(X_1_glove50)\n",
    "y_pred_2 = svm.predict(X_2_glove50)\n",
    "y_pred_3 = svm.predict(X_3_glove50)\n",
    "y_pred_4 = svm.predict(X_glove50)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove100 = sentences_to_embeddings(X_train, glove100) \n",
    "X_test_glove100 = sentences_to_embeddings(X_test, glove100)\n",
    "\n",
    "X_1_glove100 = sentences_to_embeddings(X_1, glove100)\n",
    "X_2_glove100 = sentences_to_embeddings(X_2, glove100)\n",
    "X_3_glove100 = sentences_to_embeddings(X_3, glove100)\n",
    "X_glove100 = sentences_to_embeddings(X, glove100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.977528 | 0.917782 | precision |\n",
      "| 1    | 0.982005 | 0.664894 | precision |\n",
      "| 2    | 1        | 0.771812 | precision |\n",
      "| 3    | 0.9975   | 0.905797 | precision |\n",
      "| 4    | 0.983936 | 0.921289 | precision |\n",
      "| Mean | 0.988194 | 0.836315 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.970035 | 0.9375   | recall   |\n",
      "| 1    | 0.858427 | 0.94697  | recall   |\n",
      "| 2    | 0.746269 | 1        | recall   |\n",
      "| 3    | 0.978741 | 0.988142 | recall   |\n",
      "| 4    | 0.97115  | 0.955153 | recall   |\n",
      "| Mean | 0.904924 | 0.965553 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.973767 | 0.927536 | f1-score |\n",
      "| 1    | 0.916067 | 0.78125  | f1-score |\n",
      "| 2    | 0.854701 | 0.871212 | f1-score |\n",
      "| 3    | 0.988031 | 0.94518  | f1-score |\n",
      "| 4    | 0.977501 | 0.937916 | f1-score |\n",
      "| Mean | 0.942013 | 0.892619 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_glove100, y_train)\n",
    "y_pred = svm.predict(X_test_glove100)\n",
    "y_pred_1 = svm.predict(X_1_glove100)\n",
    "y_pred_2 = svm.predict(X_2_glove100)\n",
    "y_pred_3 = svm.predict(X_3_glove100)\n",
    "y_pred_4 = svm.predict(X_glove100)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove200 = sentences_to_embeddings(X_train, glove200) \n",
    "X_test_glove200 = sentences_to_embeddings(X_test, glove200)\n",
    "\n",
    "X_1_glove200 = sentences_to_embeddings(X_1, glove200)\n",
    "X_2_glove200 = sentences_to_embeddings(X_2, glove200)\n",
    "X_3_glove200 = sentences_to_embeddings(X_3, glove200)\n",
    "X_glove200 = sentences_to_embeddings(X, glove200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.98208  | 0.928435 | precision |\n",
      "| 1    | 0.984772 | 0.688525 | precision |\n",
      "| 2    | 0.924528 | 0.748252 | precision |\n",
      "| 3    | 0.9975   | 0.905797 | precision |\n",
      "| 4    | 0.985766 | 0.926838 | precision |\n",
      "| Mean | 0.974929 | 0.839569 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.973868 | 0.950195 | recall   |\n",
      "| 1    | 0.87191  | 0.954545 | recall   |\n",
      "| 2    | 0.731343 | 0.930435 | recall   |\n",
      "| 3    | 0.978741 | 0.988142 | recall   |\n",
      "| 4    | 0.973202 | 0.960253 | recall   |\n",
      "| Mean | 0.905813 | 0.956714 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.977957 | 0.939189 | f1-score |\n",
      "| 1    | 0.924911 | 0.8      | f1-score |\n",
      "| 2    | 0.816667 | 0.829457 | f1-score |\n",
      "| 3    | 0.988031 | 0.94518  | f1-score |\n",
      "| 4    | 0.979444 | 0.94325  | f1-score |\n",
      "| Mean | 0.937402 | 0.891415 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_glove200, y_train)\n",
    "y_pred = svm.predict(X_test_glove200)\n",
    "y_pred_1 = svm.predict(X_1_glove200)\n",
    "y_pred_2 = svm.predict(X_2_glove200)\n",
    "y_pred_3 = svm.predict(X_3_glove200)\n",
    "y_pred_4 = svm.predict(X_glove200)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove300 = sentences_to_embeddings(X_train, glove300) \n",
    "X_test_glove300 = sentences_to_embeddings(X_test, glove300)\n",
    "\n",
    "X_1_glove300 = sentences_to_embeddings(X_1, glove300)\n",
    "X_2_glove300 = sentences_to_embeddings(X_2, glove300)\n",
    "X_3_glove300 = sentences_to_embeddings(X_3, glove300)\n",
    "X_glove300 = sentences_to_embeddings(X, glove300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.981754 | 0.931034 | precision |\n",
      "| 1    | 0.989691 | 0.677249 | precision |\n",
      "| 2    | 0.888889 | 0.730496 | precision |\n",
      "| 3    | 0.99584  | 0.905109 | precision |\n",
      "| 4    | 0.986601 | 0.93205  | precision |\n",
      "| Mean | 0.968555 | 0.835188 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.974913 | 0.949219 | recall   |\n",
      "| 1    | 0.862921 | 0.969697 | recall   |\n",
      "| 2    | 0.716418 | 0.895652 | recall   |\n",
      "| 3    | 0.978741 | 0.980237 | recall   |\n",
      "| 4    | 0.975191 | 0.96254  | recall   |\n",
      "| Mean | 0.901637 | 0.951469 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.978322 | 0.940039 | f1-score |\n",
      "| 1    | 0.921969 | 0.797508 | f1-score |\n",
      "| 2    | 0.793388 | 0.804688 | f1-score |\n",
      "| 3    | 0.987216 | 0.941176 | f1-score |\n",
      "| 4    | 0.980863 | 0.94705  | f1-score |\n",
      "| Mean | 0.932352 | 0.886092 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_glove300, y_train)\n",
    "y_pred = svm.predict(X_test_glove300)\n",
    "y_pred_1 = svm.predict(X_1_glove300)\n",
    "y_pred_2 = svm.predict(X_2_glove300)\n",
    "y_pred_3 = svm.predict(X_3_glove300)\n",
    "y_pred_4 = svm.predict(X_glove300)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText, load_facebook_model\n",
    "\n",
    "model = FastText(vector_size=500, window=5, min_count=10, workers=6)\n",
    "\n",
    "model.build_vocab(X_train)\n",
    "\n",
    "model.train(X_train, total_examples=len(X_train), epochs=10)\n",
    "\n",
    "X_train_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_train]\n",
    "X_test_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_test]\n",
    "\n",
    "X_1_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_1]\n",
    "X_2_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_2]\n",
    "X_3_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_3]\n",
    "X_ft = [model.wv.get_sentence_vector(sentence) for sentence in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.980966 | 0.917692 | precision |\n",
      "| 1    | 0.97954  | 0.666667 | precision |\n",
      "| 2    | 0.973913 | 0.835821 | precision |\n",
      "| 3    | 0.978477 | 0.847015 | precision |\n",
      "| 4    | 0.984705 | 0.91542  | precision |\n",
      "| Mean | 0.97952  | 0.836523 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.969686 | 0.947266 | recall   |\n",
      "| 1    | 0.860674 | 0.939394 | recall   |\n",
      "| 2    | 0.835821 | 0.973913 | recall   |\n",
      "| 3    | 0.966476 | 0.897233 | recall   |\n",
      "| 4    | 0.968725 | 0.957439 | recall   |\n",
      "| Mean | 0.920276 | 0.943049 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.975293 | 0.932244 | f1-score |\n",
      "| 1    | 0.916268 | 0.779874 | f1-score |\n",
      "| 2    | 0.899598 | 0.899598 | f1-score |\n",
      "| 3    | 0.972439 | 0.871401 | f1-score |\n",
      "| 4    | 0.976649 | 0.935958 | f1-score |\n",
      "| Mean | 0.94805  | 0.883815 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_ft, y_train)\n",
    "y_pred = svm.predict(X_test_ft)\n",
    "y_pred_1 = svm.predict(X_1_ft)\n",
    "y_pred_2 = svm.predict(X_2_ft)\n",
    "y_pred_3 = svm.predict(X_3_ft)\n",
    "y_pred_4 = svm.predict(X_ft)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15573/15573 [00:00<00:00, 90872.90it/s]\n",
      "100%|██████████| 3894/3894 [00:00<00:00, 101318.35it/s]\n",
      "100%|██████████| 577/577 [00:00<00:00, 102586.30it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 52109.65it/s]\n",
      "100%|██████████| 1476/1476 [00:00<00:00, 88529.69it/s]\n",
      "100%|██████████| 21769/21769 [00:00<00:00, 105235.51it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X_train)]\n",
    "X_test_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X_test)]\n",
    "\n",
    "X_1_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X_1)]\n",
    "X_2_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X_2)]\n",
    "X_3_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X_3)]\n",
    "X_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15573/15573 [00:13<00:00, 1159.99it/s]\n",
      "100%|██████████| 3894/3894 [00:03<00:00, 1202.50it/s]\n",
      "100%|██████████| 577/577 [00:00<00:00, 1194.95it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 826.28it/s] \n",
      "100%|██████████| 1476/1476 [00:00<00:00, 1583.30it/s]\n",
      "100%|██████████| 21769/21769 [00:18<00:00, 1204.99it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "model = FastText(vector_size=500, window=5, min_count=10, workers=6)\n",
    "\n",
    "model.build_vocab(X_train_temp)\n",
    "\n",
    "model.train(X_train_temp, total_examples=len(X_train_temp), epochs=10)\n",
    "\n",
    "X_train_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_train_temp)]\n",
    "X_test_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_test_temp)]\n",
    "\n",
    "X_1_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_1_temp)]\n",
    "X_2_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_2_temp)]\n",
    "X_3_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_3_temp)]\n",
    "X_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_temp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.979809 | 0.902894 | precision |\n",
      "| 1    | 0.984    | 0.623762 | precision |\n",
      "| 2    | 0.962264 | 0.776224 | precision |\n",
      "| 3    | 0.969547 | 0.827586 | precision |\n",
      "| 4    | 0.981653 | 0.896959 | precision |\n",
      "| Mean | 0.975455 | 0.805485 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.963763 | 0.944336 | recall   |\n",
      "| 1    | 0.829213 | 0.954545 | recall   |\n",
      "| 2    | 0.761194 | 0.965217 | recall   |\n",
      "| 3    | 0.963205 | 0.853755 | recall   |\n",
      "| 4    | 0.96145  | 0.949173 | recall   |\n",
      "| Mean | 0.895765 | 0.933405 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.97172  | 0.92315  | f1-score |\n",
      "| 1    | 0.9      | 0.754491 | f1-score |\n",
      "| 2    | 0.85     | 0.860465 | f1-score |\n",
      "| 3    | 0.966366 | 0.840467 | f1-score |\n",
      "| 4    | 0.971447 | 0.922328 | f1-score |\n",
      "| Mean | 0.931906 | 0.86018  | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_ft, y_train)\n",
    "y_pred = svm.predict(X_test_ft)\n",
    "y_pred_1 = svm.predict(X_1_ft)\n",
    "y_pred_2 = svm.predict(X_2_ft)\n",
    "y_pred_3 = svm.predict(X_3_ft)\n",
    "y_pred_4 = svm.predict(X_ft)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply grid search to FastText parameters to get the best embedding parameters.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.sklearn_api.ftmodel import FastText\n",
    "\n",
    "model = FastText(vector_size=300, window=5, min_count=10, workers=4)\n",
    "pipeline = Pipeline([\n",
    "    ('ft', model),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'ft__vector_size': [50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "    'ft__window': [1, 3, 5, 7, 9, 11, 13, 15],\n",
    "    'ft__min_count': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='accuracy', cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT (Sentence Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers as st\n",
    "\n",
    "class BertSentenceEmbedder():\n",
    "    '''\n",
    "    An interface for converting given text into sentence BERT embeddings.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Load the pre-trained model and tokenizer\n",
    "        '''\n",
    "        self.model = st.SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def embed(self, sentences):\n",
    "        '''\n",
    "        Convert the given sentences into BERT embeddings.\n",
    "        :param sentences: A list of sentences to convert into BERT embeddings.\n",
    "        :return: A list of BERT embeddings for the given sentences.\n",
    "        '''\n",
    "        return self.model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "embedder = BertSentenceEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15573/15573 [02:01<00:00, 128.12it/s]\n",
      "100%|██████████| 3894/3894 [00:29<00:00, 130.88it/s]\n",
      "100%|██████████| 577/577 [00:04<00:00, 133.46it/s]\n",
      "100%|██████████| 249/249 [00:01<00:00, 128.29it/s]\n",
      "100%|██████████| 1476/1476 [00:11<00:00, 132.90it/s]\n",
      "100%|██████████| 21769/21769 [02:46<00:00, 131.10it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_st = [embedder.embed(sentence) for sentence in tqdm(X_train)]\n",
    "X_test_st = [embedder.embed(sentence) for sentence in tqdm(X_test)]\n",
    "\n",
    "X_1_st = [embedder.embed(sentence) for sentence in tqdm(X_1)]\n",
    "X_2_st = [embedder.embed(sentence) for sentence in tqdm(X_2)]\n",
    "X_3_st = [embedder.embed(sentence) for sentence in tqdm(X_3)]\n",
    "X_st = [embedder.embed(sentence) for sentence in tqdm(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back to cpu\n",
    "X_train_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_train_st])\n",
    "X_train_st = X_train_st.reshape(-1, 1)\n",
    "X_test_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_test_st])\n",
    "X_test_st = X_test_st.reshape(-1, 1)\n",
    "X_1_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_1_st])\n",
    "X_1_st = X_1_st.reshape(-1, 1)\n",
    "X_2_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_2_st])\n",
    "X_2_st = X_2_st.reshape(-1, 1)\n",
    "X_3_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_3_st])\n",
    "X_3_st = X_3_st.reshape(-1, 1)\n",
    "X_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_st])\n",
    "X_st = X_st.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.758685 | 0.926829 | precision |\n",
      "| 1    | 0.771777 | 0.333333 | precision |\n",
      "| 2    | 0.536585 | 0.333333 | precision |\n",
      "| 3    | 0.83871  | 0.947368 | precision |\n",
      "| 4    | 0.758919 | 0.930709 | precision |\n",
      "| Mean | 0.73     | 0.69     | precision |\n",
      "## Recall\n",
      "|      |        0 |          1 | Metric   |\n",
      "|:-----|---------:|-----------:|:---------|\n",
      "| 0    | 0.996864 | 0.111328   | recall   |\n",
      "| 1    | 0.995506 | 0.00757576 | recall   |\n",
      "| 2    | 0.985075 | 0.00869565 | recall   |\n",
      "| 3    | 0.999182 | 0.0711462  | recall   |\n",
      "| 4    | 0.997264 | 0.10394    | recall   |\n",
      "| Mean | 0.99     | 0.06       | recall   |\n",
      "## F1-score\n",
      "|      |        0 |         1 | Metric   |\n",
      "|:-----|---------:|----------:|:---------|\n",
      "| 0    | 0.861617 | 0.198779  | f1-score |\n",
      "| 1    | 0.86948  | 0.0148148 | f1-score |\n",
      "| 2    | 0.694737 | 0.0169492 | f1-score |\n",
      "| 3    | 0.91194  | 0.132353  | f1-score |\n",
      "| 4    | 0.861918 | 0.186996  | f1-score |\n",
      "| Mean | 0.84     | 0.11      | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_st, y_train)\n",
    "y_pred = svm.predict(X_test_st)\n",
    "y_pred_1 = svm.predict(X_1_st)\n",
    "y_pred_2 = svm.predict(X_2_st)\n",
    "y_pred_3 = svm.predict(X_3_st)\n",
    "y_pred_4 = svm.predict(X_st)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4669100, 15551690)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "model = Word2Vec(vector_size=500, window=5, min_count=1, workers=6)\n",
    "model.build_vocab(X_train)\n",
    "model.train(X_train, total_examples=len(X_train), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_sentence_embeddings(sentences, model):\n",
    "    sentence_embeddings = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        word_vectors = []\n",
    "        for word in sentence.split():\n",
    "            try:\n",
    "                word_vectors.append(model.wv.get_vector(word.lower()))\n",
    "            except KeyError:\n",
    "                word_vectors.append(np.zeros(500))\n",
    "        if word_vectors:\n",
    "            sentence_embeddings.append(np.mean(np.mean(word_vectors, axis=0), axis=0))\n",
    "    sentence_embeddings = np.array(sentence_embeddings)\n",
    "    sentence_embeddings = sentence_embeddings.reshape(-1, 1)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15573/15573 [00:01<00:00, 15093.93it/s]\n",
      "100%|██████████| 3894/3894 [00:00<00:00, 14176.28it/s]\n",
      "100%|██████████| 577/577 [00:00<00:00, 13687.34it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 12501.28it/s]\n",
      "100%|██████████| 1476/1476 [00:00<00:00, 16481.57it/s]\n",
      "100%|██████████| 21769/21769 [00:01<00:00, 15875.40it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_word2vec = get_word2vec_sentence_embeddings(X_train, model) \n",
    "X_test_word2vec = get_word2vec_sentence_embeddings(X_test, model)\n",
    "\n",
    "X_1_word2vec = get_word2vec_sentence_embeddings(X_1, model)\n",
    "X_2_word2vec = get_word2vec_sentence_embeddings(X_2, model)\n",
    "X_3_word2vec = get_word2vec_sentence_embeddings(X_3, model)\n",
    "X_word2vec = get_word2vec_sentence_embeddings(X, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.765232 | 0.781095 | precision |\n",
      "| 1    | 0.766478 | 0.173913 | precision |\n",
      "| 2    | 0.59276  | 0.892857 | precision |\n",
      "| 3    | 0.854093 | 0.676056 | precision |\n",
      "| 4    | 0.768065 | 0.767227 | precision |\n",
      "| Mean | 0.749325 | 0.65823  | precision |\n",
      "## Recall\n",
      "|      |        0 |         1 | Metric   |\n",
      "|:-----|---------:|----------:|:---------|\n",
      "| 0    | 0.984669 | 0.15332   | recall   |\n",
      "| 1    | 0.914607 | 0.0606061 | recall   |\n",
      "| 2    | 0.977612 | 0.217391  | recall   |\n",
      "| 3    | 0.981194 | 0.189723  | recall   |\n",
      "| 4    | 0.982777 | 0.16057   | recall   |\n",
      "| Mean | 0.968172 | 0.156322  | recall   |\n",
      "## F1-score\n",
      "|      |        0 |         1 | Metric   |\n",
      "|:-----|---------:|----------:|:---------|\n",
      "| 0    | 0.861192 | 0.256327  | f1-score |\n",
      "| 1    | 0.834016 | 0.0898876 | f1-score |\n",
      "| 2    | 0.738028 | 0.34965   | f1-score |\n",
      "| 3    | 0.913242 | 0.296296  | f1-score |\n",
      "| 4    | 0.862255 | 0.265561  | f1-score |\n",
      "| Mean | 0.841747 | 0.251544  | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_word2vec, y_train)\n",
    "y_pred = svm.predict(X_test_word2vec)\n",
    "y_pred_1 = svm.predict(X_1_word2vec)\n",
    "y_pred_2 = svm.predict(X_2_word2vec)\n",
    "y_pred_3 = svm.predict(X_3_word2vec)\n",
    "y_pred_4 = svm.predict(X_word2vec)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
