{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jimbo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/jimbo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jimbo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentences(sentences, labels=None, mode=-1, lower=False, remove_stopwords=False, lemmatize=False, remove_punctuation=False):\n",
    "    sentences = sentences.to_list()\n",
    "    # Remove stopwords and extra spaces\n",
    "    if mode == 0:\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            for word in sentence.split():\n",
    "                if word in stop_words:\n",
    "                    sentence = sentence.replace(word, '')\n",
    "                else:\n",
    "                    sentence = sentence.replace(word, word.lower())\n",
    "            sentence = sentence.replace('  ', ' ')\n",
    "            sentences[i] = sentence\n",
    "    # Replace (c), (C), © with COPYRIGHT_SYMBOL then use the word_tokenize function instaed of split + previous\n",
    "    elif mode == 1:\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            sentence = sentence.replace('(c)', 'COPYRIGHT_SYMBOL') # (c) -> '(' 'c' ')\n",
    "            sentence = sentence.replace('(C)', 'COPYRIGHT_SYMBOL')\n",
    "            sentence = sentence.replace('©', 'COPYRIGHT_SYMBOL')\n",
    "            tokens = word_tokenize(sentence)\n",
    "            if lower:\n",
    "                tokens = [token.lower() for token in tokens]\n",
    "            if remove_stopwords:\n",
    "                tokens = [token for token in tokens if token not in stop_words]\n",
    "            if remove_punctuation:\n",
    "                tokens = [token for token in tokens if token not in string.punctuation]\n",
    "            if lemmatize:\n",
    "                tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "            sentences[i] = tokens\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 Percentage:  0.7385852090032154\n",
      "Class 1 Percentage:  0.26141479099678455\n"
     ]
    }
   ],
   "source": [
    "data_0 = pd.read_csv('../datasets/fossology-master.csv')\n",
    "X_0 = data_0[\"copyright\"]\n",
    "y_0 = data_0[\"falsePositive\"]\n",
    "X_0 = X_0.drop_duplicates()\n",
    "y_0 = y_0[X_0.index]\n",
    "\n",
    "data_1 = pd.read_csv('../datasets/kubernetes-master.csv')\n",
    "X_1 = data_1[\"copyright\"]\n",
    "y_1 = data_1[\"falsePositive\"]\n",
    "X_1 = X_1.drop_duplicates()\n",
    "y_1 = y_1[X_1.index]\n",
    "\n",
    "data_2 = pd.read_csv('../datasets/tensorflow-master.csv')\n",
    "X_2 = data_2[\"copyright\"]\n",
    "y_2 = data_2[\"falsePositive\"]\n",
    "X_2 = X_2.drop_duplicates()\n",
    "y_2 = y_2[X_2.index]\n",
    "\n",
    "data_3 = pd.read_csv('../datasets/Fossology-Provided-Dataset-1.csv')\n",
    "\n",
    "X_3 = data_3['scanner_content']\n",
    "y_3 = data_3['falsePositive']\n",
    "X_3 = X_3.drop_duplicates()\n",
    "y_3 = y_3[X_3.index]\n",
    "\n",
    "X = pd.concat([X_0, X_1, X_2, X_3])\n",
    "y = pd.concat([y_0, y_1, y_2, y_3])\n",
    "\n",
    "print('Class 0 Percentage: ', len(y[y == 0]) / len(y))\n",
    "print('Class 1 Percentage: ', len(y[y == 1]) / len(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_0, y_0, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_reports(reports, print_aggregates=True):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    dfs = []\n",
    "    for metric in ['precision', 'recall', 'f1-score']:\n",
    "        scores = []\n",
    "        for report in reports:\n",
    "            scores.append([report['0'][metric], report['1'][metric]])\n",
    "        scores = np.array(scores)\n",
    "        scores = scores[:, :2]\n",
    "        mean_scores = np.mean(scores, axis=0)\n",
    "        mean_scores = [f\"{score:.6f}\" for score in mean_scores]\n",
    "        df = pd.DataFrame(scores, columns=['0', '1'])\n",
    "        df.loc['Mean'] = mean_scores\n",
    "        df['Metric'] = metric\n",
    "        dfs.append(df)\n",
    "    if print_aggregates:\n",
    "        print(\"## Precision\")\n",
    "        print(dfs[0].to_markdown())\n",
    "        print(\"## Recall\")\n",
    "        print(dfs[1].to_markdown())\n",
    "        print(\"## F1-score\")\n",
    "        print(dfs[2].to_markdown())\n",
    "    else:\n",
    "        return dfs[0], dfs[1], dfs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "\n",
    "X_1_bow = vectorizer.transform(X_1)\n",
    "\n",
    "X_2_bow = vectorizer.transform(X_2)\n",
    "\n",
    "X_3_bow = vectorizer.transform(X_3)\n",
    "\n",
    "X_bow = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.994346 | 0.947368 | precision |\n",
      "| 1    | 0.986911 | 0.651282 | precision |\n",
      "| 2    | 1        | 0.751634 | precision |\n",
      "| 3    | 1        | 0.908451 | precision |\n",
      "| 4    | 0.996347 | 0.956042 | precision |\n",
      "| Mean | 0.995521 | 0.842955 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.980488 | 0.984375 | recall   |\n",
      "| 1    | 0.847191 | 0.962121 | recall   |\n",
      "| 2    | 0.716418 | 1        | recall   |\n",
      "| 3    | 0.978671 | 1        | recall   |\n",
      "| 4    | 0.983892 | 0.989808 | recall   |\n",
      "| Mean | 0.901332 | 0.987261 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.987368 | 0.965517 | f1-score |\n",
      "| 1    | 0.911729 | 0.776758 | f1-score |\n",
      "| 2    | 0.834783 | 0.858209 | f1-score |\n",
      "| 3    | 0.989221 | 0.95203  | f1-score |\n",
      "| 4    | 0.99008  | 0.972632 | f1-score |\n",
      "| Mean | 0.942636 | 0.905029 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_bow, y_train)\n",
    "y_pred = svm.predict(X_test_bow)\n",
    "y_pred_1 = svm.predict(X_1_bow)\n",
    "y_pred_2 = svm.predict(X_2_bow)\n",
    "y_pred_3 = svm.predict(X_3_bow)\n",
    "y_pred_4 = svm.predict(X_bow)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.995048 | 0.946579 | precision |\n",
      "| 1    | 0.981818 | 0.651042 | precision |\n",
      "| 2    | 0.99     | 0.765101 | precision |\n",
      "| 3    | 0.999158 | 0.886207 | precision |\n",
      "| 4    | 0.996469 | 0.953146 | precision |\n",
      "| Mean | 0.992498 | 0.840415 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.980139 | 0.986328 | recall   |\n",
      "| 1    | 0.849438 | 0.94697  | recall   |\n",
      "| 2    | 0.738806 | 0.991304 | recall   |\n",
      "| 3    | 0.972929 | 0.996124 | recall   |\n",
      "| 4    | 0.982773 | 0.99016  | recall   |\n",
      "| Mean | 0.904817 | 0.982177 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.987537 | 0.966045 | f1-score |\n",
      "| 1    | 0.910843 | 0.771605 | f1-score |\n",
      "| 2    | 0.846154 | 0.863636 | f1-score |\n",
      "| 3    | 0.985869 | 0.937956 | f1-score |\n",
      "| 4    | 0.989573 | 0.971301 | f1-score |\n",
      "| Mean | 0.943995 | 0.902109 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 0 only\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = vectorizer.fit_transform(preprocess_sentences(X_train, y_train, mode=0))\n",
    "X_test_bow = vectorizer.transform(preprocess_sentences(X_test, y_test, mode=0))\n",
    "X_1_bow = vectorizer.transform(preprocess_sentences(X_1, y_1, mode=0))\n",
    "X_2_bow = vectorizer.transform(preprocess_sentences(X_2, y_2, mode=0))\n",
    "X_3_bow = vectorizer.transform(preprocess_sentences(X_3, y_3, mode=0))\n",
    "X_bow = vectorizer.transform(preprocess_sentences(X, y, mode=0))\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_bow, y_train)\n",
    "y_pred = svm.predict(X_test_bow)\n",
    "y_pred_1 = svm.predict(X_1_bow)\n",
    "y_pred_2 = svm.predict(X_2_bow)\n",
    "y_pred_3 = svm.predict(X_3_bow)\n",
    "y_pred_4 = svm.predict(X_bow)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.993651 | 0.949953 | precision |\n",
      "| 1    | 0.994667 | 0.643564 | precision |\n",
      "| 2    | 0.988235 | 0.695122 | precision |\n",
      "| 3    | 0.991756 | 0.939394 | precision |\n",
      "| 4    | 0.994847 | 0.957821 | precision |\n",
      "| Mean | 0.992631 | 0.837171 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.981533 | 0.982422 | recall   |\n",
      "| 1    | 0.838202 | 0.984848 | recall   |\n",
      "| 2    | 0.626866 | 0.991304 | recall   |\n",
      "| 3    | 0.986874 | 0.96124  | recall   |\n",
      "| 4    | 0.984638 | 0.985591 | recall   |\n",
      "| Mean | 0.883623 | 0.981081 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.987555 | 0.965915 | f1-score |\n",
      "| 1    | 0.909756 | 0.778443 | f1-score |\n",
      "| 2    | 0.767123 | 0.817204 | f1-score |\n",
      "| 3    | 0.989309 | 0.950192 | f1-score |\n",
      "| 4    | 0.989717 | 0.971508 | f1-score |\n",
      "| Mean | 0.928692 | 0.896652 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 1, lower=True\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = preprocess_sentences(X_train, y_train, mode=1, lower=True)\n",
    "X_trainB_bow = [' '.join(sentence_words) for sentence_words in X_train_bow]\n",
    "X_train_bow = vectorizer.fit_transform(X_trainB_bow)\n",
    "X_test_bow = preprocess_sentences(X_test, y_test, mode=1, lower=True)\n",
    "X_testB_bow = [' '.join(sentence_words) for sentence_words in X_test_bow]\n",
    "X_test_bow = vectorizer.transform(X_testB_bow)\n",
    "X_1_bow = preprocess_sentences(X_1, y_1, mode=1, lower=True)\n",
    "X_1B_bow = [' '.join(sentence_words) for sentence_words in X_1_bow]\n",
    "X_1_bow = vectorizer.transform(X_1B_bow)\n",
    "X_2_bow = preprocess_sentences(X_2, y_2, mode=1, lower=True)\n",
    "X_2B_bow = [' '.join(sentence_words) for sentence_words in X_2_bow]\n",
    "X_2_bow = vectorizer.transform(X_2B_bow)\n",
    "X_3_bow = preprocess_sentences(X_3, y_3, mode=1, lower=True)\n",
    "X_3B_bow = [' '.join(sentence_words) for sentence_words in X_3_bow]\n",
    "X_3_bow = vectorizer.transform(X_3B_bow)\n",
    "X_bow = preprocess_sentences(X, y, mode=1, lower=True)\n",
    "XB_bow = [' '.join(sentence_words) for sentence_words in X_bow]\n",
    "X_bow = vectorizer.transform(XB_bow)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_bow, y_train)\n",
    "y_pred = svm.predict(X_test_bow)\n",
    "y_pred_1 = svm.predict(X_1_bow)\n",
    "y_pred_2 = svm.predict(X_2_bow)\n",
    "y_pred_3 = svm.predict(X_3_bow)\n",
    "y_pred_4 = svm.predict(X_bow)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.99541  | 0.951977 | precision |\n",
      "| 1    | 0.99455  | 0.619048 | precision |\n",
      "| 2    | 1        | 0.692771 | precision |\n",
      "| 3    | 0.993394 | 0.93985  | precision |\n",
      "| 4    | 0.996031 | 0.954384 | precision |\n",
      "| Mean | 0.995877 | 0.831606 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.98223  | 0.987305 | recall   |\n",
      "| 1    | 0.820225 | 0.984848 | recall   |\n",
      "| 2    | 0.619403 | 1        | recall   |\n",
      "| 3    | 0.986874 | 0.968992 | recall   |\n",
      "| 4    | 0.98327  | 0.98893  | recall   |\n",
      "| Mean | 0.8784   | 0.986015 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.988776 | 0.969319 | f1-score |\n",
      "| 1    | 0.899015 | 0.760234 | f1-score |\n",
      "| 2    | 0.764977 | 0.818505 | f1-score |\n",
      "| 3    | 0.990123 | 0.954198 | f1-score |\n",
      "| 4    | 0.989609 | 0.97135  | f1-score |\n",
      "| Mean | 0.9265   | 0.894721 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 1, lower=True, remove_stopwords=True\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = preprocess_sentences(X_train, y_train, mode=1, lower=True, remove_stopwords=True)\n",
    "X_trainB_bow = [' '.join(sentence_words) for sentence_words in X_train_bow]\n",
    "X_train_bow = vectorizer.fit_transform(X_trainB_bow)\n",
    "X_test_bow = preprocess_sentences(X_test, y_test, mode=1, lower=True, remove_stopwords=True)\n",
    "X_testB_bow = [' '.join(sentence_words) for sentence_words in X_test_bow]\n",
    "X_test_bow = vectorizer.transform(X_testB_bow)\n",
    "X_1_bow = preprocess_sentences(X_1, y_1, mode=1, lower=True, remove_stopwords=True)\n",
    "X_1B_bow = [' '.join(sentence_words) for sentence_words in X_1_bow]\n",
    "X_1_bow = vectorizer.transform(X_1B_bow)\n",
    "X_2_bow = preprocess_sentences(X_2, y_2, mode=1, lower=True, remove_stopwords=True)\n",
    "X_2B_bow = [' '.join(sentence_words) for sentence_words in X_2_bow]\n",
    "X_2_bow = vectorizer.transform(X_2B_bow)\n",
    "X_3_bow = preprocess_sentences(X_3, y_3, mode=1, lower=True, remove_stopwords=True)\n",
    "X_3B_bow = [' '.join(sentence_words) for sentence_words in X_3_bow]\n",
    "X_3_bow = vectorizer.transform(X_3B_bow)\n",
    "X_bow = preprocess_sentences(X, y, mode=1, lower=True, remove_stopwords=True)\n",
    "XB_bow = [' '.join(sentence_words) for sentence_words in X_bow]\n",
    "X_bow = vectorizer.transform(XB_bow)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_bow, y_train)\n",
    "y_pred = svm.predict(X_test_bow)\n",
    "y_pred_1 = svm.predict(X_1_bow)\n",
    "y_pred_2 = svm.predict(X_2_bow)\n",
    "y_pred_3 = svm.predict(X_3_bow)\n",
    "y_pred_4 = svm.predict(X_bow)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.995405 | 0.949296 | precision |\n",
      "| 1    | 0.991979 | 0.635468 | precision |\n",
      "| 2    | 1        | 0.737179 | precision |\n",
      "| 3    | 0.993394 | 0.93985  | precision |\n",
      "| 4    | 0.995908 | 0.95599  | precision |\n",
      "| Mean | 0.995337 | 0.843557 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.981185 | 0.987305 | recall   |\n",
      "| 1    | 0.833708 | 0.977273 | recall   |\n",
      "| 2    | 0.69403  | 1        | recall   |\n",
      "| 3    | 0.986874 | 0.968992 | recall   |\n",
      "| 4    | 0.983892 | 0.988578 | recall   |\n",
      "| Mean | 0.895938 | 0.98443  | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.988244 | 0.967927 | f1-score |\n",
      "| 1    | 0.905983 | 0.770149 | f1-score |\n",
      "| 2    | 0.819383 | 0.848708 | f1-score |\n",
      "| 3    | 0.990123 | 0.954198 | f1-score |\n",
      "| 4    | 0.989864 | 0.972011 | f1-score |\n",
      "| Mean | 0.938719 | 0.902599 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 1, lower=False, remove_stopwords=True\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = preprocess_sentences(X_train, y_train, mode=1, lower=False, remove_stopwords=True)\n",
    "X_trainB_bow = [' '.join(sentence_words) for sentence_words in X_train_bow]\n",
    "X_train_bow = vectorizer.fit_transform(X_trainB_bow)\n",
    "X_test_bow = preprocess_sentences(X_test, y_test, mode=1, lower=False, remove_stopwords=True)\n",
    "X_testB_bow = [' '.join(sentence_words) for sentence_words in X_test_bow]\n",
    "X_test_bow = vectorizer.transform(X_testB_bow)\n",
    "X_1_bow = preprocess_sentences(X_1, y_1, mode=1, lower=False, remove_stopwords=True)\n",
    "X_1B_bow = [' '.join(sentence_words) for sentence_words in X_1_bow]\n",
    "X_1_bow = vectorizer.transform(X_1B_bow)\n",
    "X_2_bow = preprocess_sentences(X_2, y_2, mode=1, lower=False, remove_stopwords=True)\n",
    "X_2B_bow = [' '.join(sentence_words) for sentence_words in X_2_bow]\n",
    "X_2_bow = vectorizer.transform(X_2B_bow)\n",
    "X_3_bow = preprocess_sentences(X_3, y_3, mode=1, lower=False, remove_stopwords=True)\n",
    "X_3B_bow = [' '.join(sentence_words) for sentence_words in X_3_bow]\n",
    "X_3_bow = vectorizer.transform(X_3B_bow)\n",
    "X_bow = preprocess_sentences(X, y, mode=1, lower=False, remove_stopwords=True)\n",
    "XB_bow = [' '.join(sentence_words) for sentence_words in X_bow]\n",
    "X_bow = vectorizer.transform(XB_bow)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_bow, y_train)\n",
    "y_pred = svm.predict(X_test_bow)\n",
    "y_pred_1 = svm.predict(X_1_bow)\n",
    "y_pred_2 = svm.predict(X_2_bow)\n",
    "y_pred_3 = svm.predict(X_3_bow)\n",
    "y_pred_4 = svm.predict(X_bow)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.993651 | 0.949953 | precision |\n",
      "| 1    | 0.994667 | 0.643564 | precision |\n",
      "| 2    | 0.988235 | 0.695122 | precision |\n",
      "| 3    | 0.991756 | 0.939394 | precision |\n",
      "| 4    | 0.994847 | 0.957821 | precision |\n",
      "| Mean | 0.992631 | 0.837171 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.981533 | 0.982422 | recall   |\n",
      "| 1    | 0.838202 | 0.984848 | recall   |\n",
      "| 2    | 0.626866 | 0.991304 | recall   |\n",
      "| 3    | 0.986874 | 0.96124  | recall   |\n",
      "| 4    | 0.984638 | 0.985591 | recall   |\n",
      "| Mean | 0.883623 | 0.981081 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.987555 | 0.965915 | f1-score |\n",
      "| 1    | 0.909756 | 0.778443 | f1-score |\n",
      "| 2    | 0.767123 | 0.817204 | f1-score |\n",
      "| 3    | 0.989309 | 0.950192 | f1-score |\n",
      "| 4    | 0.989717 | 0.971508 | f1-score |\n",
      "| Mean | 0.928692 | 0.896652 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 1, lower=False, remove_stopwords=False, and remove_punctuation=True\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = preprocess_sentences(X_train, y_train, mode=1, lower=False, remove_stopwords=False, remove_punctuation=True)\n",
    "X_trainB_bow = [' '.join(sentence_words) for sentence_words in X_train_bow]\n",
    "X_train_bow = vectorizer.fit_transform(X_trainB_bow)\n",
    "X_test_bow = preprocess_sentences(X_test, y_test, mode=1, lower=False, remove_stopwords=False, remove_punctuation=True)\n",
    "X_testB_bow = [' '.join(sentence_words) for sentence_words in X_test_bow]\n",
    "X_test_bow = vectorizer.transform(X_testB_bow)\n",
    "X_1_bow = preprocess_sentences(X_1, y_1, mode=1, lower=False, remove_stopwords=False, remove_punctuation=True)\n",
    "X_1B_bow = [' '.join(sentence_words) for sentence_words in X_1_bow]\n",
    "X_1_bow = vectorizer.transform(X_1B_bow)\n",
    "X_2_bow = preprocess_sentences(X_2, y_2, mode=1, lower=False, remove_stopwords=False, remove_punctuation=True)\n",
    "X_2B_bow = [' '.join(sentence_words) for sentence_words in X_2_bow]\n",
    "X_2_bow = vectorizer.transform(X_2B_bow)\n",
    "X_3_bow = preprocess_sentences(X_3, y_3, mode=1, lower=False, remove_stopwords=False, remove_punctuation=True)\n",
    "X_3B_bow = [' '.join(sentence_words) for sentence_words in X_3_bow]\n",
    "X_3_bow = vectorizer.transform(X_3B_bow)\n",
    "X_bow = preprocess_sentences(X, y, mode=1, lower=False, remove_stopwords=False, remove_punctuation=True)\n",
    "XB_bow = [' '.join(sentence_words) for sentence_words in X_bow]\n",
    "X_bow = vectorizer.transform(XB_bow)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_bow, y_train)\n",
    "y_pred = svm.predict(X_test_bow)\n",
    "y_pred_1 = svm.predict(X_1_bow)\n",
    "y_pred_2 = svm.predict(X_2_bow)\n",
    "y_pred_3 = svm.predict(X_3_bow)\n",
    "y_pred_4 = svm.predict(X_bow)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.994004 | 0.950897 | precision |\n",
      "| 1    | 0.99162  | 0.589041 | precision |\n",
      "| 2    | 1        | 0.684524 | precision |\n",
      "| 3    | 0.991756 | 0.939394 | precision |\n",
      "| 4    | 0.994966 | 0.954739 | precision |\n",
      "| Mean | 0.994469 | 0.823719 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.981882 | 0.983398 | recall   |\n",
      "| 1    | 0.797753 | 0.977273 | recall   |\n",
      "| 2    | 0.604478 | 1        | recall   |\n",
      "| 3    | 0.986874 | 0.96124  | recall   |\n",
      "| 4    | 0.983457 | 0.985943 | recall   |\n",
      "| Mean | 0.870889 | 0.981571 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.987905 | 0.966875 | f1-score |\n",
      "| 1    | 0.884184 | 0.735043 | f1-score |\n",
      "| 2    | 0.753488 | 0.812721 | f1-score |\n",
      "| 3    | 0.989309 | 0.950192 | f1-score |\n",
      "| 4    | 0.989178 | 0.97009  | f1-score |\n",
      "| Mean | 0.920813 | 0.886984 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 1, lower=True, remove_stopwords=False, and remove_punctuation=False, lemmatize=True\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = preprocess_sentences(X_train, y_train, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "X_trainB_bow = [' '.join(sentence_words) for sentence_words in X_train_bow]\n",
    "X_train_bow = vectorizer.fit_transform(X_trainB_bow)\n",
    "X_test_bow = preprocess_sentences(X_test, y_test, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "X_testB_bow = [' '.join(sentence_words) for sentence_words in X_test_bow]\n",
    "X_test_bow = vectorizer.transform(X_testB_bow)\n",
    "X_1_bow = preprocess_sentences(X_1, y_1, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "X_1B_bow = [' '.join(sentence_words) for sentence_words in X_1_bow]\n",
    "X_1_bow = vectorizer.transform(X_1B_bow)\n",
    "X_2_bow = preprocess_sentences(X_2, y_2, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "X_2B_bow = [' '.join(sentence_words) for sentence_words in X_2_bow]\n",
    "X_2_bow = vectorizer.transform(X_2B_bow)\n",
    "X_3_bow = preprocess_sentences(X_3, y_3, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "X_3B_bow = [' '.join(sentence_words) for sentence_words in X_3_bow]\n",
    "X_3_bow = vectorizer.transform(X_3B_bow)\n",
    "X_bow = preprocess_sentences(X, y, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "XB_bow = [' '.join(sentence_words) for sentence_words in X_bow]\n",
    "X_bow = vectorizer.transform(XB_bow)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_bow, y_train)\n",
    "y_pred = svm.predict(X_test_bow)\n",
    "y_pred_1 = svm.predict(X_1_bow)\n",
    "y_pred_2 = svm.predict(X_2_bow)\n",
    "y_pred_3 = svm.predict(X_3_bow)\n",
    "y_pred_4 = svm.predict(X_bow)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.99541  | 0.951977 | precision |\n",
      "| 1    | 0.994318 | 0.577778 | precision |\n",
      "| 2    | 1        | 0.680473 | precision |\n",
      "| 3    | 0.993388 | 0.93633  | precision |\n",
      "| 4    | 0.995963 | 0.95131  | precision |\n",
      "| Mean | 0.995816 | 0.819574 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.98223  | 0.987305 | recall   |\n",
      "| 1    | 0.786517 | 0.984848 | recall   |\n",
      "| 2    | 0.597015 | 1        | recall   |\n",
      "| 3    | 0.986054 | 0.968992 | recall   |\n",
      "| 4    | 0.982088 | 0.988754 | recall   |\n",
      "| Mean | 0.866781 | 0.98598  | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.988776 | 0.969319 | f1-score |\n",
      "| 1    | 0.878294 | 0.728291 | f1-score |\n",
      "| 2    | 0.747664 | 0.809859 | f1-score |\n",
      "| 3    | 0.989708 | 0.952381 | f1-score |\n",
      "| 4    | 0.988977 | 0.969671 | f1-score |\n",
      "| Mean | 0.918684 | 0.885904 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 1, lower=True, remove_stopwords=True, and remove_punctuation=False, lemmatize=True\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = preprocess_sentences(X_train, y_train, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "X_trainB_bow = [' '.join(sentence_words) for sentence_words in X_train_bow]\n",
    "X_train_bow = vectorizer.fit_transform(X_trainB_bow)\n",
    "X_test_bow = preprocess_sentences(X_test, y_test, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "X_testB_bow = [' '.join(sentence_words) for sentence_words in X_test_bow]\n",
    "X_test_bow = vectorizer.transform(X_testB_bow)\n",
    "X_1_bow = preprocess_sentences(X_1, y_1, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "X_1B_bow = [' '.join(sentence_words) for sentence_words in X_1_bow]\n",
    "X_1_bow = vectorizer.transform(X_1B_bow)\n",
    "X_2_bow = preprocess_sentences(X_2, y_2, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "X_2B_bow = [' '.join(sentence_words) for sentence_words in X_2_bow]\n",
    "X_2_bow = vectorizer.transform(X_2B_bow)\n",
    "X_3_bow = preprocess_sentences(X_3, y_3, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "X_3B_bow = [' '.join(sentence_words) for sentence_words in X_3_bow]\n",
    "X_3_bow = vectorizer.transform(X_3B_bow)\n",
    "X_bow = preprocess_sentences(X, y, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "XB_bow = [' '.join(sentence_words) for sentence_words in X_bow]\n",
    "X_bow = vectorizer.transform(XB_bow)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_bow, y_train)\n",
    "y_pred = svm.predict(X_test_bow)\n",
    "y_pred_1 = svm.predict(X_1_bow)\n",
    "y_pred_2 = svm.predict(X_2_bow)\n",
    "y_pred_3 = svm.predict(X_3_bow)\n",
    "y_pred_4 = svm.predict(X_bow)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.99541  | 0.951977 | precision |\n",
      "| 1    | 0.994318 | 0.577778 | precision |\n",
      "| 2    | 1        | 0.680473 | precision |\n",
      "| 3    | 0.993388 | 0.93633  | precision |\n",
      "| 4    | 0.995963 | 0.95131  | precision |\n",
      "| Mean | 0.995816 | 0.819574 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.98223  | 0.987305 | recall   |\n",
      "| 1    | 0.786517 | 0.984848 | recall   |\n",
      "| 2    | 0.597015 | 1        | recall   |\n",
      "| 3    | 0.986054 | 0.968992 | recall   |\n",
      "| 4    | 0.982088 | 0.988754 | recall   |\n",
      "| Mean | 0.866781 | 0.98598  | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.988776 | 0.969319 | f1-score |\n",
      "| 1    | 0.878294 | 0.728291 | f1-score |\n",
      "| 2    | 0.747664 | 0.809859 | f1-score |\n",
      "| 3    | 0.989708 | 0.952381 | f1-score |\n",
      "| 4    | 0.988977 | 0.969671 | f1-score |\n",
      "| Mean | 0.918684 | 0.885904 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 1, lower=True, remove_stopwords=True, and remove_punctuation=True, lemmatize=True\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_bow = preprocess_sentences(X_train, y_train, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True)\n",
    "X_trainB_bow = [' '.join(sentence_words) for sentence_words in X_train_bow]\n",
    "X_train_bow = vectorizer.fit_transform(X_trainB_bow)\n",
    "X_test_bow = preprocess_sentences(X_test, y_test, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True)\n",
    "X_testB_bow = [' '.join(sentence_words) for sentence_words in X_test_bow]\n",
    "X_test_bow = vectorizer.transform(X_testB_bow)\n",
    "X_1_bow = preprocess_sentences(X_1, y_1, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True)\n",
    "X_1B_bow = [' '.join(sentence_words) for sentence_words in X_1_bow]\n",
    "X_1_bow = vectorizer.transform(X_1B_bow)\n",
    "X_2_bow = preprocess_sentences(X_2, y_2, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True)\n",
    "X_2B_bow = [' '.join(sentence_words) for sentence_words in X_2_bow]\n",
    "X_2_bow = vectorizer.transform(X_2B_bow)\n",
    "X_3_bow = preprocess_sentences(X_3, y_3, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True)\n",
    "X_3B_bow = [' '.join(sentence_words) for sentence_words in X_3_bow]\n",
    "X_3_bow = vectorizer.transform(X_3B_bow)\n",
    "X_bow = preprocess_sentences(X, y, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True)\n",
    "XB_bow = [' '.join(sentence_words) for sentence_words in X_bow]\n",
    "X_bow = vectorizer.transform(XB_bow)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_bow, y_train)\n",
    "y_pred = svm.predict(X_test_bow)\n",
    "y_pred_1 = svm.predict(X_1_bow)\n",
    "y_pred_2 = svm.predict(X_2_bow)\n",
    "y_pred_3 = svm.predict(X_3_bow)\n",
    "y_pred_4 = svm.predict(X_bow)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "X_1_tfidf = vectorizer.transform(X_1)\n",
    "\n",
    "X_2_tfidf = vectorizer.transform(X_2)\n",
    "\n",
    "X_3_tfidf = vectorizer.transform(X_3)\n",
    "\n",
    "X_tfidf = vectorizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.991262 | 0.967086 | precision |\n",
      "| 1    | 0.97284  | 0.703488 | precision |\n",
      "| 2    | 0.945312 | 0.892562 | precision |\n",
      "| 3    | 0.991701 | 0.911765 | precision |\n",
      "| 4    | 0.995004 | 0.974809 | precision |\n",
      "| Mean | 0.979224 | 0.889942 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.988153 | 0.975586 | recall   |\n",
      "| 1    | 0.885393 | 0.916667 | recall   |\n",
      "| 2    | 0.902985 | 0.93913  | recall   |\n",
      "| 3    | 0.980312 | 0.96124  | recall   |\n",
      "| 4    | 0.990982 | 0.985943 | recall   |\n",
      "| Mean | 0.949565 | 0.955713 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.989705 | 0.971317 | f1-score |\n",
      "| 1    | 0.927059 | 0.796053 | f1-score |\n",
      "| 2    | 0.923664 | 0.915254 | f1-score |\n",
      "| 3    | 0.985974 | 0.935849 | f1-score |\n",
      "| 4    | 0.992989 | 0.980344 | f1-score |\n",
      "| Mean | 0.963878 | 0.919764 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "y_pred_1 = svm.predict(X_1_tfidf)\n",
    "y_pred_2 = svm.predict(X_2_tfidf)\n",
    "y_pred_3 = svm.predict(X_3_tfidf)\n",
    "y_pred_4 = svm.predict(X_tfidf)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.99335  | 0.969142 | precision |\n",
      "| 1    | 0.972705 | 0.695402 | precision |\n",
      "| 2    | 0.949153 | 0.832061 | precision |\n",
      "| 3    | 0.996667 | 0.916968 | precision |\n",
      "| 4    | 0.995748 | 0.973511 | precision |\n",
      "| Mean | 0.981524 | 0.877417 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.98885  | 0.981445 | recall   |\n",
      "| 1    | 0.880899 | 0.916667 | recall   |\n",
      "| 2    | 0.835821 | 0.947826 | recall   |\n",
      "| 3    | 0.981132 | 0.984496 | recall   |\n",
      "| 4    | 0.990484 | 0.988051 | recall   |\n",
      "| Mean | 0.935437 | 0.963697 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.991095 | 0.975255 | f1-score |\n",
      "| 1    | 0.924528 | 0.79085  | f1-score |\n",
      "| 2    | 0.888889 | 0.886179 | f1-score |\n",
      "| 3    | 0.988838 | 0.949533 | f1-score |\n",
      "| 4    | 0.993109 | 0.980727 | f1-score |\n",
      "| Mean | 0.957292 | 0.916509 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 0 only\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(preprocess_sentences(X_train, y_train, mode=0))\n",
    "X_test_tfidf = vectorizer.transform(preprocess_sentences(X_test, y_test, mode=0))\n",
    "X_1_tfidf = vectorizer.transform(preprocess_sentences(X_1, y_1, mode=0))\n",
    "X_2_tfidf = vectorizer.transform(preprocess_sentences(X_2, y_2, mode=0))\n",
    "X_3_tfidf = vectorizer.transform(preprocess_sentences(X_3, y_3, mode=0))\n",
    "X_tfidf = vectorizer.transform(preprocess_sentences(X, y, mode=0))\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "y_pred_1 = svm.predict(X_1_tfidf)\n",
    "y_pred_2 = svm.predict(X_2_tfidf)\n",
    "y_pred_3 = svm.predict(X_3_tfidf)\n",
    "y_pred_4 = svm.predict(X_tfidf)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.989878 | 0.966958 | precision |\n",
      "| 1    | 0.977612 | 0.702857 | precision |\n",
      "| 2    | 0.933333 | 0.829457 | precision |\n",
      "| 3    | 0.966909 | 0.911765 | precision |\n",
      "| 4    | 0.992832 | 0.973634 | precision |\n",
      "| Mean | 0.972113 | 0.876934 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.988153 | 0.97168  | recall   |\n",
      "| 1    | 0.883146 | 0.931818 | recall   |\n",
      "| 2    | 0.835821 | 0.930435 | recall   |\n",
      "| 3    | 0.982773 | 0.841085 | recall   |\n",
      "| 4    | 0.990609 | 0.979793 | recall   |\n",
      "| Mean | 0.9361   | 0.930962 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.989015 | 0.969313 | f1-score |\n",
      "| 1    | 0.927981 | 0.801303 | f1-score |\n",
      "| 2    | 0.88189  | 0.877049 | f1-score |\n",
      "| 3    | 0.974776 | 0.875    | f1-score |\n",
      "| 4    | 0.991719 | 0.976703 | f1-score |\n",
      "| Mean | 0.953076 | 0.899874 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 1, lower=True, remove_stopwords=False, and remove_punctuation=False, lemmatize=False\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = preprocess_sentences(X_train, y_train, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=False)\n",
    "X_trainB_tfidf = [' '.join(sentence_words) for sentence_words in X_train_tfidf]\n",
    "X_train_tfidf = vectorizer.fit_transform(X_trainB_tfidf)\n",
    "X_test_tfidf = preprocess_sentences(X_test, y_test, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=False)\n",
    "X_testB_tfidf = [' '.join(sentence_words) for sentence_words in X_test_tfidf]\n",
    "X_test_tfidf = vectorizer.transform(X_testB_tfidf)\n",
    "X_1_tfidf = preprocess_sentences(X_1, y_1, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=False)\n",
    "X_1B_tfidf = [' '.join(sentence_words) for sentence_words in X_1_tfidf]\n",
    "X_1_tfidf = vectorizer.transform(X_1B_tfidf)\n",
    "X_2_tfidf = preprocess_sentences(X_2, y_2, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=False)\n",
    "X_2B_tfidf = [' '.join(sentence_words) for sentence_words in X_2_tfidf]\n",
    "X_2_tfidf = vectorizer.transform(X_2B_tfidf)\n",
    "X_3_tfidf = preprocess_sentences(X_3, y_3, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=False)\n",
    "X_3B_tfidf = [' '.join(sentence_words) for sentence_words in X_3_tfidf]\n",
    "X_3_tfidf = vectorizer.transform(X_3B_tfidf)\n",
    "X_tfidf = preprocess_sentences(X, y, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=False)\n",
    "XB_tfidf = [' '.join(sentence_words) for sentence_words in X_tfidf]\n",
    "X_tfidf = vectorizer.transform(XB_tfidf)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "y_pred_1 = svm.predict(X_1_tfidf)\n",
    "y_pred_2 = svm.predict(X_2_tfidf)\n",
    "y_pred_3 = svm.predict(X_3_tfidf)\n",
    "y_pred_4 = svm.predict(X_tfidf)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.991969 | 0.971845 | precision |\n",
      "| 1    | 0.975124 | 0.697143 | precision |\n",
      "| 2    | 0.870229 | 0.830508 | precision |\n",
      "| 3    | 0.966184 | 0.919149 | precision |\n",
      "| 4    | 0.992526 | 0.974974 | precision |\n",
      "| Mean | 0.959206 | 0.878724 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.989895 | 0.977539 | recall   |\n",
      "| 1    | 0.880899 | 0.924242 | recall   |\n",
      "| 2    | 0.850746 | 0.852174 | recall   |\n",
      "| 3    | 0.984413 | 0.837209 | recall   |\n",
      "| 4    | 0.991106 | 0.978914 | recall   |\n",
      "| Mean | 0.939412 | 0.914016 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.990931 | 0.974684 | f1-score |\n",
      "| 1    | 0.92562  | 0.794788 | f1-score |\n",
      "| 2    | 0.860377 | 0.841202 | f1-score |\n",
      "| 3    | 0.975213 | 0.876268 | f1-score |\n",
      "| 4    | 0.991816 | 0.97694  | f1-score |\n",
      "| Mean | 0.948792 | 0.892776 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 1, lower=True, remove_stopwords=True, and remove_punctuation=False, lemmatize=False\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = preprocess_sentences(X_train, y_train, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=False)\n",
    "X_trainB_tfidf = [' '.join(sentence_words) for sentence_words in X_train_tfidf]\n",
    "X_train_tfidf = vectorizer.fit_transform(X_trainB_tfidf)\n",
    "X_test_tfidf = preprocess_sentences(X_test, y_test, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=False)\n",
    "X_testB_tfidf = [' '.join(sentence_words) for sentence_words in X_test_tfidf]\n",
    "X_test_tfidf = vectorizer.transform(X_testB_tfidf)\n",
    "X_1_tfidf = preprocess_sentences(X_1, y_1, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=False)\n",
    "X_1B_tfidf = [' '.join(sentence_words) for sentence_words in X_1_tfidf]\n",
    "X_1_tfidf = vectorizer.transform(X_1B_tfidf)\n",
    "X_2_tfidf = preprocess_sentences(X_2, y_2, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=False)\n",
    "X_2B_tfidf = [' '.join(sentence_words) for sentence_words in X_2_tfidf]\n",
    "X_2_tfidf = vectorizer.transform(X_2B_tfidf)\n",
    "X_3_tfidf = preprocess_sentences(X_3, y_3, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=False)\n",
    "X_3B_tfidf = [' '.join(sentence_words) for sentence_words in X_3_tfidf]\n",
    "X_3_tfidf = vectorizer.transform(X_3B_tfidf)\n",
    "X_tfidf = preprocess_sentences(X, y, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=False)\n",
    "XB_tfidf = [' '.join(sentence_words) for sentence_words in X_tfidf]\n",
    "X_tfidf = vectorizer.transform(XB_tfidf)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "y_pred_1 = svm.predict(X_1_tfidf)\n",
    "y_pred_2 = svm.predict(X_2_tfidf)\n",
    "y_pred_3 = svm.predict(X_3_tfidf)\n",
    "y_pred_4 = svm.predict(X_tfidf)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.989878 | 0.966958 | precision |\n",
      "| 1    | 0.977612 | 0.702857 | precision |\n",
      "| 2    | 0.933333 | 0.829457 | precision |\n",
      "| 3    | 0.966909 | 0.911765 | precision |\n",
      "| 4    | 0.992832 | 0.973634 | precision |\n",
      "| Mean | 0.972113 | 0.876934 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.988153 | 0.97168  | recall   |\n",
      "| 1    | 0.883146 | 0.931818 | recall   |\n",
      "| 2    | 0.835821 | 0.930435 | recall   |\n",
      "| 3    | 0.982773 | 0.841085 | recall   |\n",
      "| 4    | 0.990609 | 0.979793 | recall   |\n",
      "| Mean | 0.9361   | 0.930962 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.989015 | 0.969313 | f1-score |\n",
      "| 1    | 0.927981 | 0.801303 | f1-score |\n",
      "| 2    | 0.88189  | 0.877049 | f1-score |\n",
      "| 3    | 0.974776 | 0.875    | f1-score |\n",
      "| 4    | 0.991719 | 0.976703 | f1-score |\n",
      "| Mean | 0.953076 | 0.899874 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 1, lower=False, remove_stopwords=False, and remove_punctuation=True, lemmatize=False\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = preprocess_sentences(X_train, y_train, mode=1, lower=False, remove_stopwords=False, remove_punctuation=True, lemmatize=False)\n",
    "X_trainB_tfidf = [' '.join(sentence_words) for sentence_words in X_train_tfidf]\n",
    "X_train_tfidf = vectorizer.fit_transform(X_trainB_tfidf)\n",
    "X_test_tfidf = preprocess_sentences(X_test, y_test, mode=1, lower=False, remove_stopwords=False, remove_punctuation=True, lemmatize=False)\n",
    "X_testB_tfidf = [' '.join(sentence_words) for sentence_words in X_test_tfidf]\n",
    "X_test_tfidf = vectorizer.transform(X_testB_tfidf)\n",
    "X_1_tfidf = preprocess_sentences(X_1, y_1, mode=1, lower=False, remove_stopwords=False, remove_punctuation=True, lemmatize=False)\n",
    "X_1B_tfidf = [' '.join(sentence_words) for sentence_words in X_1_tfidf]\n",
    "X_1_tfidf = vectorizer.transform(X_1B_tfidf)\n",
    "X_2_tfidf = preprocess_sentences(X_2, y_2, mode=1, lower=False, remove_stopwords=False, remove_punctuation=True, lemmatize=False)\n",
    "X_2B_tfidf = [' '.join(sentence_words) for sentence_words in X_2_tfidf]\n",
    "X_2_tfidf = vectorizer.transform(X_2B_tfidf)\n",
    "X_3_tfidf = preprocess_sentences(X_3, y_3, mode=1, lower=False, remove_stopwords=False, remove_punctuation=True, lemmatize=False)\n",
    "X_3B_tfidf = [' '.join(sentence_words) for sentence_words in X_3_tfidf]\n",
    "X_3_tfidf = vectorizer.transform(X_3B_tfidf)\n",
    "X_tfidf = preprocess_sentences(X, y, mode=1, lower=False, remove_stopwords=False, remove_punctuation=True, lemmatize=False)\n",
    "XB_tfidf = [' '.join(sentence_words) for sentence_words in X_tfidf]\n",
    "X_tfidf = vectorizer.transform(XB_tfidf)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "y_pred_1 = svm.predict(X_1_tfidf)\n",
    "y_pred_2 = svm.predict(X_2_tfidf)\n",
    "y_pred_3 = svm.predict(X_3_tfidf)\n",
    "y_pred_4 = svm.predict(X_tfidf)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.989878 | 0.966958 | precision |\n",
      "| 1    | 0.977612 | 0.702857 | precision |\n",
      "| 2    | 0.933333 | 0.829457 | precision |\n",
      "| 3    | 0.966909 | 0.911765 | precision |\n",
      "| 4    | 0.992832 | 0.973634 | precision |\n",
      "| Mean | 0.972113 | 0.876934 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.988153 | 0.97168  | recall   |\n",
      "| 1    | 0.883146 | 0.931818 | recall   |\n",
      "| 2    | 0.835821 | 0.930435 | recall   |\n",
      "| 3    | 0.982773 | 0.841085 | recall   |\n",
      "| 4    | 0.990609 | 0.979793 | recall   |\n",
      "| Mean | 0.9361   | 0.930962 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.989015 | 0.969313 | f1-score |\n",
      "| 1    | 0.927981 | 0.801303 | f1-score |\n",
      "| 2    | 0.88189  | 0.877049 | f1-score |\n",
      "| 3    | 0.974776 | 0.875    | f1-score |\n",
      "| 4    | 0.991719 | 0.976703 | f1-score |\n",
      "| Mean | 0.953076 | 0.899874 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 1, lower=True, remove_stopwords=False, and remove_punctuation=True, lemmatize=False\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = preprocess_sentences(X_train, y_train, mode=1, lower=True, remove_stopwords=False, remove_punctuation=True, lemmatize=False)\n",
    "X_trainB_tfidf = [' '.join(sentence_words) for sentence_words in X_train_tfidf]\n",
    "X_train_tfidf = vectorizer.fit_transform(X_trainB_tfidf)\n",
    "X_test_tfidf = preprocess_sentences(X_test, y_test, mode=1, lower=True, remove_stopwords=False, remove_punctuation=True, lemmatize=False)\n",
    "X_testB_tfidf = [' '.join(sentence_words) for sentence_words in X_test_tfidf]\n",
    "X_test_tfidf = vectorizer.transform(X_testB_tfidf)\n",
    "X_1_tfidf = preprocess_sentences(X_1, y_1, mode=1, lower=True, remove_stopwords=False, remove_punctuation=True, lemmatize=False)\n",
    "X_1B_tfidf = [' '.join(sentence_words) for sentence_words in X_1_tfidf]\n",
    "X_1_tfidf = vectorizer.transform(X_1B_tfidf)\n",
    "X_2_tfidf = preprocess_sentences(X_2, y_2, mode=1, lower=True, remove_stopwords=False, remove_punctuation=True, lemmatize=False)\n",
    "X_2B_tfidf = [' '.join(sentence_words) for sentence_words in X_2_tfidf]\n",
    "X_2_tfidf = vectorizer.transform(X_2B_tfidf)\n",
    "X_3_tfidf = preprocess_sentences(X_3, y_3, mode=1, lower=True, remove_stopwords=False, remove_punctuation=True, lemmatize=False)\n",
    "X_3B_tfidf = [' '.join(sentence_words) for sentence_words in X_3_tfidf]\n",
    "X_3_tfidf = vectorizer.transform(X_3B_tfidf)\n",
    "X_tfidf = preprocess_sentences(X, y, mode=1, lower=True, remove_stopwords=False, remove_punctuation=True, lemmatize=False)\n",
    "XB_tfidf = [' '.join(sentence_words) for sentence_words in X_tfidf]\n",
    "X_tfidf = vectorizer.transform(XB_tfidf)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "y_pred_1 = svm.predict(X_1_tfidf)\n",
    "y_pred_2 = svm.predict(X_2_tfidf)\n",
    "y_pred_3 = svm.predict(X_3_tfidf)\n",
    "y_pred_4 = svm.predict(X_tfidf)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.990227 | 0.96793  | precision |\n",
      "| 1    | 0.975186 | 0.701149 | precision |\n",
      "| 2    | 0.933333 | 0.829457 | precision |\n",
      "| 3    | 0.966909 | 0.911765 | precision |\n",
      "| 4    | 0.992894 | 0.973638 | precision |\n",
      "| Mean | 0.97171  | 0.876788 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.988502 | 0.972656 | recall   |\n",
      "| 1    | 0.883146 | 0.924242 | recall   |\n",
      "| 2    | 0.835821 | 0.930435 | recall   |\n",
      "| 3    | 0.982773 | 0.841085 | recall   |\n",
      "| 4    | 0.990609 | 0.979968 | recall   |\n",
      "| Mean | 0.93617  | 0.929677 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.989364 | 0.970287 | f1-score |\n",
      "| 1    | 0.926887 | 0.797386 | f1-score |\n",
      "| 2    | 0.88189  | 0.877049 | f1-score |\n",
      "| 3    | 0.974776 | 0.875    | f1-score |\n",
      "| 4    | 0.99175  | 0.976793 | f1-score |\n",
      "| Mean | 0.952933 | 0.899303 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 1, lower=False, remove_stopwords=False, and remove_punctuation=False, lemmatize=True\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = preprocess_sentences(X_train, y_train, mode=1, lower=False, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "X_trainB_tfidf = [' '.join(sentence_words) for sentence_words in X_train_tfidf]\n",
    "X_train_tfidf = vectorizer.fit_transform(X_trainB_tfidf)\n",
    "X_test_tfidf = preprocess_sentences(X_test, y_test, mode=1, lower=False, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "X_testB_tfidf = [' '.join(sentence_words) for sentence_words in X_test_tfidf]\n",
    "X_test_tfidf = vectorizer.transform(X_testB_tfidf)\n",
    "X_1_tfidf = preprocess_sentences(X_1, y_1, mode=1, lower=False, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "X_1B_tfidf = [' '.join(sentence_words) for sentence_words in X_1_tfidf]\n",
    "X_1_tfidf = vectorizer.transform(X_1B_tfidf)\n",
    "X_2_tfidf = preprocess_sentences(X_2, y_2, mode=1, lower=False, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "X_2B_tfidf = [' '.join(sentence_words) for sentence_words in X_2_tfidf]\n",
    "X_2_tfidf = vectorizer.transform(X_2B_tfidf)\n",
    "X_3_tfidf = preprocess_sentences(X_3, y_3, mode=1, lower=False, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "X_3B_tfidf = [' '.join(sentence_words) for sentence_words in X_3_tfidf]\n",
    "X_3_tfidf = vectorizer.transform(X_3B_tfidf)\n",
    "X_tfidf = preprocess_sentences(X, y, mode=1, lower=False, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "XB_tfidf = [' '.join(sentence_words) for sentence_words in X_tfidf]\n",
    "X_tfidf = vectorizer.transform(XB_tfidf)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "y_pred_1 = svm.predict(X_1_tfidf)\n",
    "y_pred_2 = svm.predict(X_2_tfidf)\n",
    "y_pred_3 = svm.predict(X_3_tfidf)\n",
    "y_pred_4 = svm.predict(X_tfidf)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.990573 | 0.967961 | precision |\n",
      "| 1    | 0.982323 | 0.690608 | precision |\n",
      "| 2    | 0.90678  | 0.793893 | precision |\n",
      "| 3    | 0.966882 | 0.90795  | precision |\n",
      "| 4    | 0.992889 | 0.971941 | precision |\n",
      "| Mean | 0.967889 | 0.866471 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.988502 | 0.973633 | recall   |\n",
      "| 1    | 0.874157 | 0.94697  | recall   |\n",
      "| 2    | 0.798507 | 0.904348 | recall   |\n",
      "| 3    | 0.981952 | 0.841085 | recall   |\n",
      "| 4    | 0.989987 | 0.979968 | recall   |\n",
      "| Mean | 0.926621 | 0.929201 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.989536 | 0.970789 | f1-score |\n",
      "| 1    | 0.925089 | 0.798722 | f1-score |\n",
      "| 2    | 0.849206 | 0.845528 | f1-score |\n",
      "| 3    | 0.974359 | 0.873239 | f1-score |\n",
      "| 4    | 0.991436 | 0.975938 | f1-score |\n",
      "| Mean | 0.945925 | 0.892843 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 1, lower=True, remove_stopwords=False, and remove_punctuation=False, lemmatize=True\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = preprocess_sentences(X_train, y_train, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "X_trainB_tfidf = [' '.join(sentence_words) for sentence_words in X_train_tfidf]\n",
    "X_train_tfidf = vectorizer.fit_transform(X_trainB_tfidf)\n",
    "X_test_tfidf = preprocess_sentences(X_test, y_test, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "X_testB_tfidf = [' '.join(sentence_words) for sentence_words in X_test_tfidf]\n",
    "X_test_tfidf = vectorizer.transform(X_testB_tfidf)\n",
    "X_1_tfidf = preprocess_sentences(X_1, y_1, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "X_1B_tfidf = [' '.join(sentence_words) for sentence_words in X_1_tfidf]\n",
    "X_1_tfidf = vectorizer.transform(X_1B_tfidf)\n",
    "X_2_tfidf = preprocess_sentences(X_2, y_2, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "X_2B_tfidf = [' '.join(sentence_words) for sentence_words in X_2_tfidf]\n",
    "X_2_tfidf = vectorizer.transform(X_2B_tfidf)\n",
    "X_3_tfidf = preprocess_sentences(X_3, y_3, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "X_3B_tfidf = [' '.join(sentence_words) for sentence_words in X_3_tfidf]\n",
    "X_3_tfidf = vectorizer.transform(X_3B_tfidf)\n",
    "X_tfidf = preprocess_sentences(X, y, mode=1, lower=True, remove_stopwords=False, remove_punctuation=False, lemmatize=True)\n",
    "XB_tfidf = [' '.join(sentence_words) for sentence_words in X_tfidf]\n",
    "X_tfidf = vectorizer.transform(XB_tfidf)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "y_pred_1 = svm.predict(X_1_tfidf)\n",
    "y_pred_2 = svm.predict(X_2_tfidf)\n",
    "y_pred_3 = svm.predict(X_3_tfidf)\n",
    "y_pred_4 = svm.predict(X_tfidf)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.992318 | 0.972816 | precision |\n",
      "| 1    | 0.977273 | 0.679558 | precision |\n",
      "| 2    | 0.837209 | 0.783333 | precision |\n",
      "| 3    | 0.966989 | 0.923404 | precision |\n",
      "| 4    | 0.992459 | 0.973096 | precision |\n",
      "| Mean | 0.95325  | 0.866441 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.990244 | 0.978516 | recall   |\n",
      "| 1    | 0.869663 | 0.931818 | recall   |\n",
      "| 2    | 0.80597  | 0.817391 | recall   |\n",
      "| 3    | 0.985234 | 0.841085 | recall   |\n",
      "| 4    | 0.990422 | 0.978738 | recall   |\n",
      "| Mean | 0.928307 | 0.90951  | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.99128  | 0.975657 | f1-score |\n",
      "| 1    | 0.920333 | 0.785942 | f1-score |\n",
      "| 2    | 0.821293 | 0.8      | f1-score |\n",
      "| 3    | 0.976026 | 0.880325 | f1-score |\n",
      "| 4    | 0.99144  | 0.975909 | f1-score |\n",
      "| Mean | 0.940074 | 0.883567 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 1, lower=True, remove_stopwords=True, and remove_punctuation=False, lemmatize=True\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = preprocess_sentences(X_train, y_train, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "X_trainB_tfidf = [' '.join(sentence_words) for sentence_words in X_train_tfidf]\n",
    "X_train_tfidf = vectorizer.fit_transform(X_trainB_tfidf)\n",
    "X_test_tfidf = preprocess_sentences(X_test, y_test, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "X_testB_tfidf = [' '.join(sentence_words) for sentence_words in X_test_tfidf]\n",
    "X_test_tfidf = vectorizer.transform(X_testB_tfidf)\n",
    "X_1_tfidf = preprocess_sentences(X_1, y_1, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "X_1B_tfidf = [' '.join(sentence_words) for sentence_words in X_1_tfidf]\n",
    "X_1_tfidf = vectorizer.transform(X_1B_tfidf)\n",
    "X_2_tfidf = preprocess_sentences(X_2, y_2, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "X_2B_tfidf = [' '.join(sentence_words) for sentence_words in X_2_tfidf]\n",
    "X_2_tfidf = vectorizer.transform(X_2B_tfidf)\n",
    "X_3_tfidf = preprocess_sentences(X_3, y_3, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "X_3B_tfidf = [' '.join(sentence_words) for sentence_words in X_3_tfidf]\n",
    "X_3_tfidf = vectorizer.transform(X_3B_tfidf)\n",
    "X_tfidf = preprocess_sentences(X, y, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "XB_tfidf = [' '.join(sentence_words) for sentence_words in X_tfidf]\n",
    "X_tfidf = vectorizer.transform(XB_tfidf)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "y_pred_1 = svm.predict(X_1_tfidf)\n",
    "y_pred_2 = svm.predict(X_2_tfidf)\n",
    "y_pred_3 = svm.predict(X_3_tfidf)\n",
    "y_pred_4 = svm.predict(X_tfidf)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.992318 | 0.972816 | precision |\n",
      "| 1    | 0.977273 | 0.679558 | precision |\n",
      "| 2    | 0.837209 | 0.783333 | precision |\n",
      "| 3    | 0.966989 | 0.923404 | precision |\n",
      "| 4    | 0.992459 | 0.973096 | precision |\n",
      "| Mean | 0.95325  | 0.866441 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.990244 | 0.978516 | recall   |\n",
      "| 1    | 0.869663 | 0.931818 | recall   |\n",
      "| 2    | 0.80597  | 0.817391 | recall   |\n",
      "| 3    | 0.985234 | 0.841085 | recall   |\n",
      "| 4    | 0.990422 | 0.978738 | recall   |\n",
      "| Mean | 0.928307 | 0.90951  | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.99128  | 0.975657 | f1-score |\n",
      "| 1    | 0.920333 | 0.785942 | f1-score |\n",
      "| 2    | 0.821293 | 0.8      | f1-score |\n",
      "| 3    | 0.976026 | 0.880325 | f1-score |\n",
      "| 4    | 0.99144  | 0.975909 | f1-score |\n",
      "| Mean | 0.940074 | 0.883567 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Test out mode 1, lower=True, remove_stopwords=True, and remove_punctuation=True, lemmatize=True\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = preprocess_sentences(X_train, y_train, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "X_trainB_tfidf = [' '.join(sentence_words) for sentence_words in X_train_tfidf]\n",
    "X_train_tfidf = vectorizer.fit_transform(X_trainB_tfidf)\n",
    "X_test_tfidf = preprocess_sentences(X_test, y_test, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "X_testB_tfidf = [' '.join(sentence_words) for sentence_words in X_test_tfidf]\n",
    "X_test_tfidf = vectorizer.transform(X_testB_tfidf)\n",
    "X_1_tfidf = preprocess_sentences(X_1, y_1, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "X_1B_tfidf = [' '.join(sentence_words) for sentence_words in X_1_tfidf]\n",
    "X_1_tfidf = vectorizer.transform(X_1B_tfidf)\n",
    "X_2_tfidf = preprocess_sentences(X_2, y_2, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "X_2B_tfidf = [' '.join(sentence_words) for sentence_words in X_2_tfidf]\n",
    "X_2_tfidf = vectorizer.transform(X_2B_tfidf)\n",
    "X_3_tfidf = preprocess_sentences(X_3, y_3, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "X_3B_tfidf = [' '.join(sentence_words) for sentence_words in X_3_tfidf]\n",
    "X_3_tfidf = vectorizer.transform(X_3B_tfidf)\n",
    "X_tfidf = preprocess_sentences(X, y, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=True)\n",
    "XB_tfidf = [' '.join(sentence_words) for sentence_words in X_tfidf]\n",
    "X_tfidf = vectorizer.transform(XB_tfidf)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict(X_test_tfidf)\n",
    "y_pred_1 = svm.predict(X_1_tfidf)\n",
    "y_pred_2 = svm.predict(X_2_tfidf)\n",
    "y_pred_3 = svm.predict(X_3_tfidf)\n",
    "y_pred_4 = svm.predict(X_tfidf)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.991262 | 0.967086 | precision |\n",
      "| 1    | 0.97284  | 0.703488 | precision |\n",
      "| 2    | 0.945312 | 0.892562 | precision |\n",
      "| 3    | 0.991701 | 0.911765 | precision |\n",
      "| 4    | 0.995004 | 0.974809 | precision |\n",
      "| Mean | 0.979224 | 0.889942 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.988153 | 0.975586 | recall   |\n",
      "| 1    | 0.885393 | 0.916667 | recall   |\n",
      "| 2    | 0.902985 | 0.93913  | recall   |\n",
      "| 3    | 0.980312 | 0.96124  | recall   |\n",
      "| 4    | 0.990982 | 0.985943 | recall   |\n",
      "| Mean | 0.949565 | 0.955713 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.989705 | 0.971317 | f1-score |\n",
      "| 1    | 0.927059 | 0.796053 | f1-score |\n",
      "| 2    | 0.923664 | 0.915254 | f1-score |\n",
      "| 3    | 0.985974 | 0.935849 | f1-score |\n",
      "| 4    | 0.992989 | 0.980344 | f1-score |\n",
      "| Mean | 0.963878 | 0.919764 | f1-score |\n",
      "None\n",
      "Number of missclassifications in class 0:  145 out of a total sample of:  16079  - about  0.9 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  80 out of a total sample of:  5691  - about  1.41 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "X_1_tfidf = vectorizer.transform(X_1)\n",
    "X_2_tfidf = vectorizer.transform(X_2)\n",
    "X_3_tfidf = vectorizer.transform(X_3)\n",
    "X_tfidf = vectorizer.transform(X)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(probability=True)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict_proba(X_test_tfidf)\n",
    "y_pred_1 = svm.predict_proba(X_1_tfidf)\n",
    "y_pred_2 = svm.predict_proba(X_2_tfidf)\n",
    "y_pred_3 = svm.predict_proba(X_3_tfidf)\n",
    "y_pred_4 = svm.predict_proba(X_tfidf)\n",
    "y_pred_classification = np.argmax(y_pred, axis=1)\n",
    "y_pred_1_classification = np.argmax(y_pred_1, axis=1)\n",
    "y_pred_2_classification = np.argmax(y_pred_2, axis=1)\n",
    "y_pred_3_classification = np.argmax(y_pred_3, axis=1)\n",
    "y_pred_4_classification = np.argmax(y_pred_4, axis=1)\n",
    "report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))\n",
    "print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.815647 | 0.992084 | precision |\n",
      "| 1    | 0.789661 | 0.875    | precision |\n",
      "| 2    | 0.580087 | 1        | precision |\n",
      "| 3    | 0.848189 | 0.97561  | precision |\n",
      "| 4    | 0.797865 | 0.996308 | precision |\n",
      "| Mean | 0.76629  | 0.9678   | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.998955 | 0.367188 | recall   |\n",
      "| 1    | 0.995506 | 0.106061 | recall   |\n",
      "| 2    | 1        | 0.156522 | recall   |\n",
      "| 3    | 0.99918  | 0.155039 | recall   |\n",
      "| 4    | 0.999627 | 0.284484 | recall   |\n",
      "| Mean | 0.998653 | 0.213859 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.898042 | 0.535994 | f1-score |\n",
      "| 1    | 0.880716 | 0.189189 | f1-score |\n",
      "| 2    | 0.734247 | 0.270677 | f1-score |\n",
      "| 3    | 0.917514 | 0.267559 | f1-score |\n",
      "| 4    | 0.887423 | 0.442592 | f1-score |\n",
      "| Mean | 0.863588 | 0.341202 | f1-score |\n",
      "None\n",
      "Number of missclassifications in class 0:  6 out of a total sample of:  16079  - about  0.04 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  4072 out of a total sample of:  5691  - about  71.55 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "# Use a threshold of 0.999 otherwise set to class 0\n",
    "y_pred_classification = [np.argmax(p) if np.max(p) > 0.999 else 0 for p in y_pred]\n",
    "y_pred_1_classification = [np.argmax(p) if np.max(p) > 0.999 else 0 for p in y_pred_1]\n",
    "y_pred_2_classification = [np.argmax(p) if np.max(p) > 0.999 else 0 for p in y_pred_2]\n",
    "y_pred_3_classification = [np.argmax(p) if np.max(p) > 0.999 else 0 for p in y_pred_3]\n",
    "y_pred_4_classification = [np.argmax(p) if np.max(p) > 0.999 else 0 for p in y_pred_4]\n",
    "report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))\n",
    "print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.905517 | 0.981081 | precision |\n",
      "| 1    | 0.823091 | 0.925    | precision |\n",
      "| 2    | 0.653465 | 0.957447 | precision |\n",
      "| 3    | 0.901189 | 0.954198 | precision |\n",
      "| 4    | 0.957014 | 0.994597 | precision |\n",
      "| Mean | 0.848055 | 0.962465 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.995122 | 0.708984 | recall   |\n",
      "| 1    | 0.993258 | 0.280303 | recall   |\n",
      "| 2    | 0.985075 | 0.391304 | recall   |\n",
      "| 3    | 0.995078 | 0.484496 | recall   |\n",
      "| 4    | 0.998321 | 0.873309 | recall   |\n",
      "| Mean | 0.993371 | 0.547679 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.948207 | 0.823129 | f1-score |\n",
      "| 1    | 0.900204 | 0.430233 | f1-score |\n",
      "| 2    | 0.785714 | 0.555556 | f1-score |\n",
      "| 3    | 0.945809 | 0.642674 | f1-score |\n",
      "| 4    | 0.977231 | 0.930015 | f1-score |\n",
      "| Mean | 0.911433 | 0.676321 | f1-score |\n",
      "None\n",
      "Number of missclassifications in class 0:  27 out of a total sample of:  16079  - about  0.17 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  721 out of a total sample of:  5691  - about  12.67 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "# Use a threshold of 0.99 otherwise set to class 0\n",
    "y_pred_classification = [np.argmax(p) if np.max(p) > 0.99 else 0 for p in y_pred]\n",
    "y_pred_1_classification = [np.argmax(p) if np.max(p) > 0.99 else 0 for p in y_pred_1]\n",
    "y_pred_2_classification = [np.argmax(p) if np.max(p) > 0.99 else 0 for p in y_pred_2]\n",
    "y_pred_3_classification = [np.argmax(p) if np.max(p) > 0.99 else 0 for p in y_pred_3]\n",
    "y_pred_4_classification = [np.argmax(p) if np.max(p) > 0.99 else 0 for p in y_pred_4]\n",
    "report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))\n",
    "print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.956784 | 0.984598 | precision |\n",
      "| 1    | 0.870259 | 0.881579 | precision |\n",
      "| 2    | 0.747126 | 0.946667 | precision |\n",
      "| 3    | 0.932308 | 0.960452 | precision |\n",
      "| 4    | 0.976438 | 0.992329 | precision |\n",
      "| Mean | 0.896583 | 0.953125 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.995122 | 0.874023 | recall   |\n",
      "| 1    | 0.979775 | 0.507576 | recall   |\n",
      "| 2    | 0.970149 | 0.617391 | recall   |\n",
      "| 3    | 0.994258 | 0.658915 | recall   |\n",
      "| 4    | 0.99745  | 0.931998 | recall   |\n",
      "| Mean | 0.987351 | 0.717981 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.975576 | 0.926022 | f1-score |\n",
      "| 1    | 0.921776 | 0.644231 | f1-score |\n",
      "| 2    | 0.844156 | 0.747368 | f1-score |\n",
      "| 3    | 0.962287 | 0.781609 | f1-score |\n",
      "| 4    | 0.986832 | 0.961218 | f1-score |\n",
      "| Mean | 0.938125 | 0.81209  | f1-score |\n",
      "None\n",
      "Number of missclassifications in class 0:  41 out of a total sample of:  16079  - about  0.25 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  387 out of a total sample of:  5691  - about  6.8 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "# Use a threshold of 0.95 otherwise set to class 0\n",
    "\n",
    "y_pred_classification = [np.argmax(p) if np.max(p) > 0.95 else 0 for p in y_pred]\n",
    "y_pred_1_classification = [np.argmax(p) if np.max(p) > 0.95 else 0 for p in y_pred_1]\n",
    "y_pred_2_classification = [np.argmax(p) if np.max(p) > 0.95 else 0 for p in y_pred_2]\n",
    "y_pred_3_classification = [np.argmax(p) if np.max(p) > 0.95 else 0 for p in y_pred_3]\n",
    "y_pred_4_classification = [np.argmax(p) if np.max(p) > 0.95 else 0 for p in y_pred_4]\n",
    "report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))\n",
    "print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.969079 | 0.981073 | precision |\n",
      "| 1    | 0.891892 | 0.833333 | precision |\n",
      "| 2    | 0.787879 | 0.952381 | precision |\n",
      "| 3    | 0.95358  | 0.966019 | precision |\n",
      "| 4    | 0.982525 | 0.989929 | precision |\n",
      "| Mean | 0.916991 | 0.944547 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.993728 | 0.911133 | recall   |\n",
      "| 1    | 0.964045 | 0.606061 | recall   |\n",
      "| 2    | 0.970149 | 0.695652 | recall   |\n",
      "| 3    | 0.994258 | 0.771318 | recall   |\n",
      "| 4    | 0.996579 | 0.949921 | recall   |\n",
      "| Mean | 0.983752 | 0.786817 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.981249 | 0.94481  | f1-score |\n",
      "| 1    | 0.926566 | 0.701754 | f1-score |\n",
      "| 2    | 0.869565 | 0.80402  | f1-score |\n",
      "| 3    | 0.973494 | 0.857759 | f1-score |\n",
      "| 4    | 0.989502 | 0.969512 | f1-score |\n",
      "| Mean | 0.948075 | 0.855571 | f1-score |\n",
      "None\n",
      "Number of missclassifications in class 0:  55 out of a total sample of:  16079  - about  0.34 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  285 out of a total sample of:  5691  - about  5.01 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "# Use a threshold of 0.90 otherwise set to class 0\n",
    "\n",
    "y_pred_classification = [np.argmax(p) if np.max(p) > 0.90 else 0 for p in y_pred]\n",
    "y_pred_1_classification = [np.argmax(p) if np.max(p) > 0.90 else 0 for p in y_pred_1]\n",
    "y_pred_2_classification = [np.argmax(p) if np.max(p) > 0.90 else 0 for p in y_pred_2]\n",
    "y_pred_3_classification = [np.argmax(p) if np.max(p) > 0.90 else 0 for p in y_pred_3]\n",
    "y_pred_4_classification = [np.argmax(p) if np.max(p) > 0.90 else 0 for p in y_pred_4]\n",
    "report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))\n",
    "print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.981041 | 0.975831 | precision |\n",
      "| 1    | 0.909292 | 0.728    | precision |\n",
      "| 2    | 0.84106  | 0.928571 | precision |\n",
      "| 3    | 0.964968 | 0.968326 | precision |\n",
      "| 4    | 0.987771 | 0.984585 | precision |\n",
      "| Mean | 0.936826 | 0.917063 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.991638 | 0.946289 | recall   |\n",
      "| 1    | 0.923596 | 0.689394 | recall   |\n",
      "| 2    | 0.947761 | 0.791304 | recall   |\n",
      "| 3    | 0.994258 | 0.829457 | recall   |\n",
      "| 4    | 0.994651 | 0.965208 | recall   |\n",
      "| Mean | 0.970381 | 0.844331 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.986311 | 0.960833 | f1-score |\n",
      "| 1    | 0.916388 | 0.708171 | f1-score |\n",
      "| 2    | 0.891228 | 0.85446  | f1-score |\n",
      "| 3    | 0.979394 | 0.893528 | f1-score |\n",
      "| 4    | 0.991199 | 0.9748   | f1-score |\n",
      "| Mean | 0.952904 | 0.878359 | f1-score |\n",
      "None\n",
      "Number of missclassifications in class 0:  86 out of a total sample of:  16079  - about  0.53 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  198 out of a total sample of:  5691  - about  3.48 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "# Use a threshold of 0.80 otherwise set to class 0\n",
    "\n",
    "y_pred_classification = [np.argmax(p) if np.max(p) > 0.80 else 0 for p in y_pred]\n",
    "y_pred_1_classification = [np.argmax(p) if np.max(p) > 0.80 else 0 for p in y_pred_1]\n",
    "y_pred_2_classification = [np.argmax(p) if np.max(p) > 0.80 else 0 for p in y_pred_2]\n",
    "y_pred_3_classification = [np.argmax(p) if np.max(p) > 0.80 else 0 for p in y_pred_3]\n",
    "y_pred_4_classification = [np.argmax(p) if np.max(p) > 0.80 else 0 for p in y_pred_4]\n",
    "report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))\n",
    "print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.985779 | 0.972305 | precision |\n",
      "| 1    | 0.924658 | 0.71223  | precision |\n",
      "| 2    | 0.862069 | 0.913462 | precision |\n",
      "| 3    | 0.968498 | 0.916318 | precision |\n",
      "| 4    | 0.989831 | 0.979617 | precision |\n",
      "| Mean | 0.946167 | 0.898786 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.990244 | 0.959961 | recall   |\n",
      "| 1    | 0.910112 | 0.75     | recall   |\n",
      "| 2    | 0.932836 | 0.826087 | recall   |\n",
      "| 3    | 0.983593 | 0.848837 | recall   |\n",
      "| 4    | 0.992848 | 0.971183 | recall   |\n",
      "| Mean | 0.961927 | 0.871214 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.988006 | 0.966093 | f1-score |\n",
      "| 1    | 0.917327 | 0.730627 | f1-score |\n",
      "| 2    | 0.896057 | 0.86758  | f1-score |\n",
      "| 3    | 0.975987 | 0.881288 | f1-score |\n",
      "| 4    | 0.991337 | 0.975382 | f1-score |\n",
      "| Mean | 0.953743 | 0.884194 | f1-score |\n",
      "None\n",
      "Number of missclassifications in class 0:  115 out of a total sample of:  16079  - about  0.72 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  164 out of a total sample of:  5691  - about  2.88 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "# Use a threshold of 0.70 otherwise set to class 0\n",
    "\n",
    "y_pred_classification = [np.argmax(p) if np.max(p) > 0.70 else 0 for p in y_pred]\n",
    "y_pred_1_classification = [np.argmax(p) if np.max(p) > 0.70 else 0 for p in y_pred_1]\n",
    "y_pred_2_classification = [np.argmax(p) if np.max(p) > 0.70 else 0 for p in y_pred_2]\n",
    "y_pred_3_classification = [np.argmax(p) if np.max(p) > 0.70 else 0 for p in y_pred_3]\n",
    "y_pred_4_classification = [np.argmax(p) if np.max(p) > 0.70 else 0 for p in y_pred_4]\n",
    "report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))\n",
    "print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test our different TF-IDF parameters to improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.991262 | 0.967086 | precision |\n",
      "| 1    | 0.97284  | 0.703488 | precision |\n",
      "| 2    | 0.945312 | 0.892562 | precision |\n",
      "| 3    | 0.990879 | 0.911439 | precision |\n",
      "| 4    | 0.994942 | 0.974805 | precision |\n",
      "| Mean | 0.979047 | 0.889876 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.988153 | 0.975586 | recall   |\n",
      "| 1    | 0.885393 | 0.916667 | recall   |\n",
      "| 2    | 0.902985 | 0.93913  | recall   |\n",
      "| 3    | 0.980312 | 0.957364 | recall   |\n",
      "| 4    | 0.990982 | 0.985767 | recall   |\n",
      "| Mean | 0.949565 | 0.954903 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.989705 | 0.971317 | f1-score |\n",
      "| 1    | 0.927059 | 0.796053 | f1-score |\n",
      "| 2    | 0.923664 | 0.915254 | f1-score |\n",
      "| 3    | 0.985567 | 0.933837 | f1-score |\n",
      "| 4    | 0.992958 | 0.980255 | f1-score |\n",
      "| Mean | 0.963791 | 0.919343 | f1-score |\n",
      "None\n",
      "Number of missclassifications in class 0:  145 out of a total sample of:  16079  - about  0.9 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  81 out of a total sample of:  5691  - about  1.42 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "# stip_accents='unicode'\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(strip_accents='unicode')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "X_1_tfidf = vectorizer.transform(X_1)\n",
    "X_2_tfidf = vectorizer.transform(X_2)\n",
    "X_3_tfidf = vectorizer.transform(X_3)\n",
    "X_tfidf = vectorizer.transform(X)\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(probability=True)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict_proba(X_test_tfidf)\n",
    "y_pred_1 = svm.predict_proba(X_1_tfidf)\n",
    "y_pred_2 = svm.predict_proba(X_2_tfidf)\n",
    "y_pred_3 = svm.predict_proba(X_3_tfidf)\n",
    "y_pred_4 = svm.predict_proba(X_tfidf)\n",
    "y_pred_classification = np.argmax(y_pred, axis=1)\n",
    "y_pred_1_classification = np.argmax(y_pred_1, axis=1)\n",
    "y_pred_2_classification = np.argmax(y_pred_2, axis=1)\n",
    "y_pred_3_classification = np.argmax(y_pred_3, axis=1)\n",
    "y_pred_4_classification = np.argmax(y_pred_4, axis=1)\n",
    "report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))\n",
    "print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.985925 | 0.935361 | precision |\n",
      "| 1    | 0.981579 | 0.634518 | precision |\n",
      "| 2    | 0.962617 | 0.78169  | precision |\n",
      "| 3    | 0.983389 | 0.871795 | precision |\n",
      "| 4    | 0.988243 | 0.925366 | precision |\n",
      "| Mean | 0.980351 | 0.829746 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.976307 | 0.960938 | recall   |\n",
      "| 1    | 0.838202 | 0.94697  | recall   |\n",
      "| 2    | 0.768657 | 0.965217 | recall   |\n",
      "| 3    | 0.971288 | 0.922481 | recall   |\n",
      "| 4    | 0.972386 | 0.967317 | recall   |\n",
      "| Mean | 0.905368 | 0.952584 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.981092 | 0.947977 | f1-score |\n",
      "| 1    | 0.904242 | 0.759878 | f1-score |\n",
      "| 2    | 0.854772 | 0.863813 | f1-score |\n",
      "| 3    | 0.977301 | 0.896422 | f1-score |\n",
      "| 4    | 0.980251 | 0.945876 | f1-score |\n",
      "| Mean | 0.939532 | 0.882793 | f1-score |\n",
      "None\n",
      "Number of missclassifications in class 0:  444 out of a total sample of:  16079  - about  2.76 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  186 out of a total sample of:  5691  - about  3.27 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "# analyzer=char\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='char')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "X_1_tfidf = vectorizer.transform(X_1)\n",
    "X_2_tfidf = vectorizer.transform(X_2)\n",
    "X_3_tfidf = vectorizer.transform(X_3)\n",
    "X_tfidf = vectorizer.transform(X)\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(probability=True)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict_proba(X_test_tfidf)\n",
    "y_pred_1 = svm.predict_proba(X_1_tfidf)\n",
    "y_pred_2 = svm.predict_proba(X_2_tfidf)\n",
    "y_pred_3 = svm.predict_proba(X_3_tfidf)\n",
    "y_pred_4 = svm.predict_proba(X_tfidf)\n",
    "y_pred_classification = np.argmax(y_pred, axis=1)\n",
    "y_pred_1_classification = np.argmax(y_pred_1, axis=1)\n",
    "y_pred_2_classification = np.argmax(y_pred_2, axis=1)\n",
    "y_pred_3_classification = np.argmax(y_pred_3, axis=1)\n",
    "y_pred_4_classification = np.argmax(y_pred_4, axis=1)\n",
    "report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))\n",
    "print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.983462 | 0.928707 | precision |\n",
      "| 1    | 0.980663 | 0.581395 | precision |\n",
      "| 2    | 0.971429 | 0.777778 | precision |\n",
      "| 3    | 0.985845 | 0.873188 | precision |\n",
      "| 4    | 0.986952 | 0.916917 | precision |\n",
      "| Mean | 0.98167  | 0.815597 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.973868 | 0.954102 | recall   |\n",
      "| 1    | 0.797753 | 0.94697  | recall   |\n",
      "| 2    | 0.761194 | 0.973913 | recall   |\n",
      "| 3    | 0.971288 | 0.934109 | recall   |\n",
      "| 4    | 0.96909  | 0.963802 | recall   |\n",
      "| Mean | 0.894638 | 0.954579 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.978641 | 0.941233 | f1-score |\n",
      "| 1    | 0.879802 | 0.720461 | f1-score |\n",
      "| 2    | 0.853556 | 0.864865 | f1-score |\n",
      "| 3    | 0.978512 | 0.902622 | f1-score |\n",
      "| 4    | 0.97794  | 0.939776 | f1-score |\n",
      "| Mean | 0.93369  | 0.873791 | f1-score |\n",
      "None\n",
      "Number of missclassifications in class 0:  497 out of a total sample of:  16079  - about  3.09 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  206 out of a total sample of:  5691  - about  3.62 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "# analyzer=char_wb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(analyzer='char_wb')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "X_1_tfidf = vectorizer.transform(X_1)\n",
    "X_2_tfidf = vectorizer.transform(X_2)\n",
    "X_3_tfidf = vectorizer.transform(X_3)\n",
    "X_tfidf = vectorizer.transform(X)\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(probability=True)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict_proba(X_test_tfidf)\n",
    "y_pred_1 = svm.predict_proba(X_1_tfidf)\n",
    "y_pred_2 = svm.predict_proba(X_2_tfidf)\n",
    "y_pred_3 = svm.predict_proba(X_3_tfidf)\n",
    "y_pred_4 = svm.predict_proba(X_tfidf)\n",
    "y_pred_classification = np.argmax(y_pred, axis=1)\n",
    "y_pred_1_classification = np.argmax(y_pred_1, axis=1)\n",
    "y_pred_2_classification = np.argmax(y_pred_2, axis=1)\n",
    "y_pred_3_classification = np.argmax(y_pred_3, axis=1)\n",
    "y_pred_4_classification = np.argmax(y_pred_4, axis=1)\n",
    "report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))\n",
    "print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.991608 | 0.967118 | precision |\n",
      "| 1    | 0.985    | 0.711864 | precision |\n",
      "| 2    | 0.943089 | 0.857143 | precision |\n",
      "| 3    | 0.991708 | 0.915129 | precision |\n",
      "| 4    | 0.995566 | 0.976372 | precision |\n",
      "| Mean | 0.981394 | 0.885525 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.988153 | 0.976562 | recall   |\n",
      "| 1    | 0.885393 | 0.954545 | recall   |\n",
      "| 2    | 0.865672 | 0.93913  | recall   |\n",
      "| 3    | 0.981132 | 0.96124  | recall   |\n",
      "| 4    | 0.991542 | 0.987524 | recall   |\n",
      "| Mean | 0.942378 | 0.963801 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.989878 | 0.971817 | f1-score |\n",
      "| 1    | 0.932544 | 0.815534 | f1-score |\n",
      "| 2    | 0.902724 | 0.896266 | f1-score |\n",
      "| 3    | 0.986392 | 0.937618 | f1-score |\n",
      "| 4    | 0.99355  | 0.981917 | f1-score |\n",
      "| Mean | 0.961018 | 0.92063  | f1-score |\n",
      "None\n",
      "Number of missclassifications in class 0:  136 out of a total sample of:  16079  - about  0.85 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  71 out of a total sample of:  5691  - about  1.25 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "# ngram_range=(1, 3)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "X_1_tfidf = vectorizer.transform(X_1)\n",
    "X_2_tfidf = vectorizer.transform(X_2)\n",
    "X_3_tfidf = vectorizer.transform(X_3)\n",
    "X_tfidf = vectorizer.transform(X)\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(probability=True)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict_proba(X_test_tfidf)\n",
    "y_pred_1 = svm.predict_proba(X_1_tfidf)\n",
    "y_pred_2 = svm.predict_proba(X_2_tfidf)\n",
    "y_pred_3 = svm.predict_proba(X_3_tfidf)\n",
    "y_pred_4 = svm.predict_proba(X_tfidf)\n",
    "y_pred_classification = np.argmax(y_pred, axis=1)\n",
    "y_pred_1_classification = np.argmax(y_pred_1, axis=1)\n",
    "y_pred_2_classification = np.argmax(y_pred_2, axis=1)\n",
    "y_pred_3_classification = np.argmax(y_pred_3, axis=1)\n",
    "y_pred_4_classification = np.argmax(y_pred_4, axis=1)\n",
    "report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))\n",
    "print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.992316 | 0.971872 | precision |\n",
      "| 1    | 0.980676 | 0.760736 | precision |\n",
      "| 2    | 0.909091 | 0.880342 | precision |\n",
      "| 3    | 0.9688   | 0.964758 | precision |\n",
      "| 4    | 0.993095 | 0.979979 | precision |\n",
      "| Mean | 0.968796 | 0.911537 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.989895 | 0.978516 | recall   |\n",
      "| 1    | 0.91236  | 0.939394 | recall   |\n",
      "| 2    | 0.895522 | 0.895652 | recall   |\n",
      "| 3    | 0.993437 | 0.848837 | recall   |\n",
      "| 4    | 0.99291  | 0.980496 | recall   |\n",
      "| Mean | 0.956825 | 0.928579 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.991104 | 0.975182 | f1-score |\n",
      "| 1    | 0.945285 | 0.840678 | f1-score |\n",
      "| 2    | 0.902256 | 0.887931 | f1-score |\n",
      "| 3    | 0.980964 | 0.903093 | f1-score |\n",
      "| 4    | 0.993003 | 0.980237 | f1-score |\n",
      "| Mean | 0.962522 | 0.917424 | f1-score |\n",
      "None\n",
      "Number of missclassifications in class 0:  114 out of a total sample of:  16079  - about  0.71 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  111 out of a total sample of:  5691  - about  1.95 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "# change all 4 digit numbers (dates) to DATE - change (c), (C) & © to COPYRIGHT_SYMBOL\n",
    "import re\n",
    "\n",
    "def minor_preprocess(sentences):\n",
    "    sentences = [re.sub('\\d{4}', 'DATE', sentence)  for sentence in sentences]\n",
    "    sentences = [re.sub('(c)', 'COPYRIGHT_SYMBOL', sentence)  for sentence in sentences]\n",
    "    sentences = [re.sub('(C)', 'COPYRIGHT_SYMBOL', sentence)  for sentence in sentences]\n",
    "    sentences = [re.sub('©', 'COPYRIGHT_SYMBOL', sentence)  for sentence in sentences]\n",
    "    return sentences\n",
    "    \n",
    "\n",
    "X_train_temp = minor_preprocess(X_train)\n",
    "X_test_temp = minor_preprocess(X_test)\n",
    "X_1_temp = minor_preprocess(X_1)\n",
    "X_2_temp = minor_preprocess(X_2)\n",
    "X_3_temp = minor_preprocess(X_3)\n",
    "X_temp = minor_preprocess(X)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_temp)\n",
    "X_test_tfidf = vectorizer.transform(X_test_temp)\n",
    "X_1_tfidf = vectorizer.transform(X_1_temp)\n",
    "X_2_tfidf = vectorizer.transform(X_2_temp)\n",
    "X_3_tfidf = vectorizer.transform(X_3_temp)\n",
    "X_tfidf = vectorizer.transform(X_temp)\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(probability=True)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict_proba(X_test_tfidf)\n",
    "y_pred_1 = svm.predict_proba(X_1_tfidf)\n",
    "y_pred_2 = svm.predict_proba(X_2_tfidf)\n",
    "y_pred_3 = svm.predict_proba(X_3_tfidf)\n",
    "y_pred_4 = svm.predict_proba(X_tfidf)\n",
    "y_pred_classification = np.argmax(y_pred, axis=1)\n",
    "y_pred_1_classification = np.argmax(y_pred_1, axis=1)\n",
    "y_pred_2_classification = np.argmax(y_pred_2, axis=1)\n",
    "y_pred_3_classification = np.argmax(y_pred_3, axis=1)\n",
    "y_pred_4_classification = np.argmax(y_pred_4, axis=1)\n",
    "report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))\n",
    "print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.848225 | 0.994163 | precision |\n",
      "| 1    | 0.802536 | 0.92     | precision |\n",
      "| 2    | 0.590308 | 1        | precision |\n",
      "| 3    | 0.86814  | 0.986486 | precision |\n",
      "| 4    | 0.834701 | 0.997613 | precision |\n",
      "| Mean | 0.788782 | 0.979653 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.998955 | 0.499023 | recall   |\n",
      "| 1    | 0.995506 | 0.174242 | recall   |\n",
      "| 2    | 1        | 0.191304 | recall   |\n",
      "| 3    | 0.99918  | 0.282946 | recall   |\n",
      "| 4    | 0.999627 | 0.440696 | recall   |\n",
      "| Mean | 0.998653 | 0.317642 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.91744  | 0.664499 | f1-score |\n",
      "| 1    | 0.888666 | 0.292994 | f1-score |\n",
      "| 2    | 0.742382 | 0.321168 | f1-score |\n",
      "| 3    | 0.929062 | 0.439759 | f1-score |\n",
      "| 4    | 0.90975  | 0.611335 | f1-score |\n",
      "| Mean | 0.87746  | 0.465951 | f1-score |\n",
      "None\n",
      "Number of missclassifications in class 0:  6 out of a total sample of:  16079  - about  0.04 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  3183 out of a total sample of:  5691  - about  55.93 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "# change all 4 digit numbers (dates) to DATE - change (c), (C) & © to COPYRIGHT_SYMBOL - 0.99 threshold\n",
    "import re\n",
    "\n",
    "def minor_preprocess(sentences):\n",
    "    sentences = [re.sub('\\d{4}', 'DATE', sentence)  for sentence in sentences]\n",
    "    sentences = [re.sub('(c)', 'COPYRIGHT_SYMBOL', sentence)  for sentence in sentences]\n",
    "    sentences = [re.sub('(C)', 'COPYRIGHT_SYMBOL', sentence)  for sentence in sentences]\n",
    "    sentences = [re.sub('©', 'COPYRIGHT_SYMBOL', sentence)  for sentence in sentences]\n",
    "    return sentences\n",
    "    \n",
    "\n",
    "X_train_temp = minor_preprocess(X_train)\n",
    "X_test_temp = minor_preprocess(X_test)\n",
    "X_1_temp = minor_preprocess(X_1)\n",
    "X_2_temp = minor_preprocess(X_2)\n",
    "X_3_temp = minor_preprocess(X_3)\n",
    "X_temp = minor_preprocess(X)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_temp)\n",
    "X_test_tfidf = vectorizer.transform(X_test_temp)\n",
    "X_1_tfidf = vectorizer.transform(X_1_temp)\n",
    "X_2_tfidf = vectorizer.transform(X_2_temp)\n",
    "X_3_tfidf = vectorizer.transform(X_3_temp)\n",
    "X_tfidf = vectorizer.transform(X_temp)\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(probability=True)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict_proba(X_test_tfidf)\n",
    "y_pred_1 = svm.predict_proba(X_1_tfidf)\n",
    "y_pred_2 = svm.predict_proba(X_2_tfidf)\n",
    "y_pred_3 = svm.predict_proba(X_3_tfidf)\n",
    "y_pred_4 = svm.predict_proba(X_tfidf)\n",
    "y_pred_classification = [np.argmax(p) if np.max(p) > 0.99 else 0 for p in y_pred]\n",
    "y_pred_1_classification = [np.argmax(p) if np.max(p) > 0.99 else 0 for p in y_pred_1]\n",
    "y_pred_2_classification = [np.argmax(p) if np.max(p) > 0.99 else 0 for p in y_pred_2]\n",
    "y_pred_3_classification = [np.argmax(p) if np.max(p) > 0.99 else 0 for p in y_pred_3]\n",
    "y_pred_4_classification = [np.argmax(p) if np.max(p) > 0.99 else 0 for p in y_pred_4]\n",
    "report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))\n",
    "print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.953892 | 0.983352 | precision |\n",
      "| 1    | 0.837476 | 0.87037  | precision |\n",
      "| 2    | 0.665    | 0.979592 | precision |\n",
      "| 3    | 0.915789 | 0.993197 | precision |\n",
      "| 4    | 0.971965 | 0.994862 | precision |\n",
      "| Mean | 0.868825 | 0.964275 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.994774 | 0.865234 | recall   |\n",
      "| 1    | 0.98427  | 0.356061 | recall   |\n",
      "| 2    | 0.992537 | 0.417391 | recall   |\n",
      "| 3    | 0.99918  | 0.565891 | recall   |\n",
      "| 4    | 0.998321 | 0.918643 | recall   |\n",
      "| Mean | 0.993816 | 0.624644 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.973904 | 0.920519 | f1-score |\n",
      "| 1    | 0.904959 | 0.505376 | f1-score |\n",
      "| 2    | 0.796407 | 0.585366 | f1-score |\n",
      "| 3    | 0.955669 | 0.720988 | f1-score |\n",
      "| 4    | 0.984967 | 0.955235 | f1-score |\n",
      "| Mean | 0.923181 | 0.737497 | f1-score |\n",
      "None\n",
      "Number of missclassifications in class 0:  27 out of a total sample of:  16079  - about  0.17 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  463 out of a total sample of:  5691  - about  8.14 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "# change all 4 digit numbers (dates) to DATE - change (c), (C) & © to COPYRIGHT_SYMBOL - 0.95 threshold\n",
    "import re\n",
    "\n",
    "def minor_preprocess(sentences):\n",
    "    sentences = [re.sub('\\d{4}', 'DATE', sentence)  for sentence in sentences]\n",
    "    sentences = [re.sub('(c)', 'COPYRIGHT_SYMBOL', sentence)  for sentence in sentences]\n",
    "    sentences = [re.sub('(C)', 'COPYRIGHT_SYMBOL', sentence)  for sentence in sentences]\n",
    "    sentences = [re.sub('©', 'COPYRIGHT_SYMBOL', sentence)  for sentence in sentences]\n",
    "    return sentences\n",
    "    \n",
    "\n",
    "X_train_temp = minor_preprocess(X_train)\n",
    "X_test_temp = minor_preprocess(X_test)\n",
    "X_1_temp = minor_preprocess(X_1)\n",
    "X_2_temp = minor_preprocess(X_2)\n",
    "X_3_temp = minor_preprocess(X_3)\n",
    "X_temp = minor_preprocess(X)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_temp)\n",
    "X_test_tfidf = vectorizer.transform(X_test_temp)\n",
    "X_1_tfidf = vectorizer.transform(X_1_temp)\n",
    "X_2_tfidf = vectorizer.transform(X_2_temp)\n",
    "X_3_tfidf = vectorizer.transform(X_3_temp)\n",
    "X_tfidf = vectorizer.transform(X_temp)\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(probability=True)\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict_proba(X_test_tfidf)\n",
    "y_pred_1 = svm.predict_proba(X_1_tfidf)\n",
    "y_pred_2 = svm.predict_proba(X_2_tfidf)\n",
    "y_pred_3 = svm.predict_proba(X_3_tfidf)\n",
    "y_pred_4 = svm.predict_proba(X_tfidf)\n",
    "y_pred_classification = [np.argmax(p) if np.max(p) > 0.95 else 0 for p in y_pred]\n",
    "y_pred_1_classification = [np.argmax(p) if np.max(p) > 0.95 else 0 for p in y_pred_1]\n",
    "y_pred_2_classification = [np.argmax(p) if np.max(p) > 0.95 else 0 for p in y_pred_2]\n",
    "y_pred_3_classification = [np.argmax(p) if np.max(p) > 0.95 else 0 for p in y_pred_3]\n",
    "y_pred_4_classification = [np.argmax(p) if np.max(p) > 0.95 else 0 for p in y_pred_4]\n",
    "report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))\n",
    "print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8942/2237453622.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mrandom_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tfidf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1766\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1769\u001b[0m             ParameterSampler(\n\u001b[1;32m   1770\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1096\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    974\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 975\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    976\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    565\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    566\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/VSCode/Niffler/ENTER/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    432\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/VSCode/Niffler/ENTER/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Apply random search to improve over the default SVM parameters\n",
    "\n",
    "param_distributions = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': [0.01, 0.1, 1, 10],\n",
    "    'degree': [2, 3, 4, 5],\n",
    "    'coef0': [0, 0.1, 0.5, 1],\n",
    "    'shrinking': [True, False],\n",
    "    'probability': [True, False],\n",
    "    'tol': [1e-3, 1e-4, 1e-5],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(SVC(), param_distributions, n_iter=90, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "random_search.fit(X_tfidf, y)\n",
    "\n",
    "print(random_search.best_params_)\n",
    "print(random_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.990206 | 0.962319 | precision |\n",
      "| 1    | 0.956019 | 0.77931  | precision |\n",
      "| 2    | 0.871429 | 0.889908 | precision |\n",
      "| 3    | 0.97284  | 0.858779 | precision |\n",
      "| 4    | 0.993338 | 0.978105 | precision |\n",
      "| Mean | 0.956766 | 0.893684 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.986411 | 0.972656 | recall   |\n",
      "| 1    | 0.92809  | 0.856061 | recall   |\n",
      "| 2    | 0.910448 | 0.843478 | recall   |\n",
      "| 3    | 0.969647 | 0.872093 | recall   |\n",
      "| 4    | 0.992226 | 0.981198 | recall   |\n",
      "| Mean | 0.957364 | 0.905097 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.988305 | 0.96746  | f1-score |\n",
      "| 1    | 0.941847 | 0.815884 | f1-score |\n",
      "| 2    | 0.890511 | 0.866071 | f1-score |\n",
      "| 3    | 0.971241 | 0.865385 | f1-score |\n",
      "| 4    | 0.992782 | 0.979649 | f1-score |\n",
      "| Mean | 0.956937 | 0.89889  | f1-score |\n",
      "None\n",
      "Number of missclassifications in class 0:  125 out of a total sample of:  16079  - about  0.78 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  107 out of a total sample of:  5691  - about  1.88 % of the class was missclassified\n"
     ]
    }
   ],
   "source": [
    "# No threshhold (50% basically) - with best params\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "X_1_tfidf = vectorizer.transform(X_1)\n",
    "X_2_tfidf = vectorizer.transform(X_2)\n",
    "X_3_tfidf = vectorizer.transform(X_3)\n",
    "X_tfidf = vectorizer.transform(X)\n",
    "\n",
    "svm = SVC(probability=True, C= 100, gamma= 0.1, kernel= 'rbf')\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "y_pred = svm.predict_proba(X_test_tfidf)\n",
    "y_pred_1 = svm.predict_proba(X_1_tfidf)\n",
    "y_pred_2 = svm.predict_proba(X_2_tfidf)\n",
    "y_pred_3 = svm.predict_proba(X_3_tfidf)\n",
    "y_pred_4 = svm.predict_proba(X_tfidf)\n",
    "y_pred_classification = np.argmax(y_pred, axis=1)\n",
    "y_pred_1_classification = np.argmax(y_pred_1, axis=1)\n",
    "y_pred_2_classification = np.argmax(y_pred_2, axis=1)\n",
    "y_pred_3_classification = np.argmax(y_pred_3, axis=1)\n",
    "y_pred_4_classification = np.argmax(y_pred_4, axis=1)\n",
    "report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))\n",
    "print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.96704  | 0.974763 | precision |\n",
      "| 1    | 0.873727 | 0.813953 | precision |\n",
      "| 2    | 0.775148 | 0.9625   | precision |\n",
      "| 3    | 0.956696 | 0.886957 | precision |\n",
      "| 4    | 0.983776 | 0.987086 | precision |\n",
      "| Mean | 0.911277 | 0.925052 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.991638 | 0.905273 | recall   |\n",
      "| 1    | 0.964045 | 0.530303 | recall   |\n",
      "| 2    | 0.977612 | 0.669565 | recall   |\n",
      "| 3    | 0.978671 | 0.790698 | recall   |\n",
      "| 4    | 0.995584 | 0.953611 | recall   |\n",
      "| Mean | 0.98151  | 0.76989  | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.979185 | 0.938734 | f1-score |\n",
      "| 1    | 0.916667 | 0.642202 | f1-score |\n",
      "| 2    | 0.864686 | 0.789744 | f1-score |\n",
      "| 3    | 0.967559 | 0.836066 | f1-score |\n",
      "| 4    | 0.989645 | 0.97006  | f1-score |\n",
      "| Mean | 0.943548 | 0.835361 | f1-score |\n",
      "None\n",
      "Number of missclassifications in class 0:  71 out of a total sample of:  16079  - about  0.44 % of the class was missclassified\n",
      "Number of missclassifications in class 1:  264 out of a total sample of:  5691  - about  4.64 % of the class was missclassified\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Use a threshold of 0.999 otherwise set to class 0 - with best params\n",
    "y_pred_classification = [np.argmax(p) if np.max(p) > 0.90 else 0 for p in y_pred]\n",
    "y_pred_1_classification = [np.argmax(p) if np.max(p) > 0.90 else 0 for p in y_pred_1]\n",
    "y_pred_2_classification = [np.argmax(p) if np.max(p) > 0.90 else 0 for p in y_pred_2]\n",
    "y_pred_3_classification = [np.argmax(p) if np.max(p) > 0.90 else 0 for p in y_pred_3]\n",
    "y_pred_4_classification = [np.argmax(p) if np.max(p) > 0.90 else 0 for p in y_pred_4]\n",
    "report = classification_report(y_test, y_pred_classification, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1_classification, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2_classification, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3_classification, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4_classification, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))\n",
    "print('Number of missclassifications in class 0: ', report_4['0']['support'] - round(report_4['0']['recall'] * report_4['0']['support']), 'out of a total sample of: ', report_4['0']['support'], ' - about ', round((1 - report_4['0']['recall']) * 100, 2), '% of the class was missclassified')\n",
    "print('Number of missclassifications in class 1: ', report_4['1']['support'] - round(report_4['1']['recall'] * report_4['1']['support']), 'out of a total sample of: ', report_4['1']['support'], ' - about ', round((1 - report_4['1']['recall']) * 100, 2), '% of the class was missclassified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "import numpy as np\n",
    "def load_glove(file):\n",
    "    \"\"\"Load GloVe embeddings from a text file.\n",
    "    Args:\n",
    "        file (str): path to the glove file.\n",
    "    Returns:\n",
    "        dict: a dictionary mapping words to their vector representations.\n",
    "    \"\"\"\n",
    "    embeddings = {}\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "glove50 = load_glove('../glove.6B/glove.6B.50d.txt')\n",
    "glove100 = load_glove('../glove.6B/glove.6B.100d.txt')\n",
    "glove200 = load_glove('../glove.6B/glove.6B.200d.txt')\n",
    "glove300 = load_glove('../glove.6B/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_embeddings(sentences, embeddings, preprocess=False, weighted_avg=None):\n",
    "    \"\"\"\n",
    "        Convert a list of sentences into a matrix of embeddings. \n",
    "        \n",
    "        Args:\n",
    "            sentences (list): a list of strings, each representing a sentence.\n",
    "            embeddings (dict): a dictionary mapping words to their vector representations.\n",
    "\n",
    "        Returns: \n",
    "            np.array: a 2D array of shape (len(sentences), len(embeddings[word])), where each\n",
    "                      row is the average of the word vectors in the sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    matrix = []\n",
    "    if not preprocess:\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split()\n",
    "            vectors = [embeddings.get(word.lower(), np.zeros(len(embeddings['the']))) for word in words] \n",
    "            if weighted_avg is None:\n",
    "                mean = np.mean(vectors, axis=0)\n",
    "            else:\n",
    "                weights = [weighted_avg.get(word.lower(), 0) for word in words]\n",
    "                weighted_sum = np.sum([v * w for v, w in zip(vectors, weights)], axis=0)\n",
    "                mean = weighted_sum / np.sum(weights)\n",
    "            matrix.append(mean)\n",
    "    else:\n",
    "        for sentence in sentences:\n",
    "            vectors = [embeddings.get(token, np.zeros(len(embeddings['the']))) for token in sentence] \n",
    "            if weighted_avg is None:\n",
    "                mean = np.mean(vectors, axis=0)\n",
    "            else:\n",
    "                weights = [weighted_avg.get(token.lower(), 0) for token in sentence]\n",
    "                weighted_sum = np.sum([v * w for v, w in zip(vectors, weights)], axis=0)\n",
    "                mean = weighted_sum / np.sum(weights)\n",
    "            matrix.append(mean)\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove50 = sentences_to_embeddings(X_train, glove50) \n",
    "X_test_glove50 = sentences_to_embeddings(X_test, glove50)\n",
    "\n",
    "X_1_glove50 = sentences_to_embeddings(X_1, glove50)\n",
    "X_2_glove50 = sentences_to_embeddings(X_2, glove50)\n",
    "X_3_glove50 = sentences_to_embeddings(X_3, glove50)\n",
    "X_glove50 = sentences_to_embeddings(X, glove50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.971258 | 0.904899 | precision |\n",
      "| 1    | 0.986339 | 0.601896 | precision |\n",
      "| 2    | 0.915254 | 0.801527 | precision |\n",
      "| 3    | 0.995847 | 0.911765 | precision |\n",
      "| 4    | 0.979161 | 0.902747 | precision |\n",
      "| Mean | 0.969572 | 0.824567 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.965505 | 0.919922 | recall   |\n",
      "| 1    | 0.811236 | 0.962121 | recall   |\n",
      "| 2    | 0.80597  | 0.913043 | recall   |\n",
      "| 3    | 0.980376 | 0.980237 | recall   |\n",
      "| 4    | 0.964124 | 0.941963 | recall   |\n",
      "| Mean | 0.905442 | 0.943457 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.968373 | 0.912349 | f1-score |\n",
      "| 1    | 0.890259 | 0.740525 | f1-score |\n",
      "| 2    | 0.857143 | 0.853659 | f1-score |\n",
      "| 3    | 0.988051 | 0.944762 | f1-score |\n",
      "| 4    | 0.971584 | 0.921938 | f1-score |\n",
      "| Mean | 0.935082 | 0.874646 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_glove50, y_train)\n",
    "y_pred = svm.predict(X_test_glove50)\n",
    "y_pred_1 = svm.predict(X_1_glove50)\n",
    "y_pred_2 = svm.predict(X_2_glove50)\n",
    "y_pred_3 = svm.predict(X_3_glove50)\n",
    "y_pred_4 = svm.predict(X_glove50)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove100 = sentences_to_embeddings(X_train, glove100) \n",
    "X_test_glove100 = sentences_to_embeddings(X_test, glove100)\n",
    "\n",
    "X_1_glove100 = sentences_to_embeddings(X_1, glove100)\n",
    "X_2_glove100 = sentences_to_embeddings(X_2, glove100)\n",
    "X_3_glove100 = sentences_to_embeddings(X_3, glove100)\n",
    "X_glove100 = sentences_to_embeddings(X, glove100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.977528 | 0.917782 | precision |\n",
      "| 1    | 0.982005 | 0.664894 | precision |\n",
      "| 2    | 1        | 0.771812 | precision |\n",
      "| 3    | 0.9975   | 0.905797 | precision |\n",
      "| 4    | 0.983936 | 0.921289 | precision |\n",
      "| Mean | 0.988194 | 0.836315 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.970035 | 0.9375   | recall   |\n",
      "| 1    | 0.858427 | 0.94697  | recall   |\n",
      "| 2    | 0.746269 | 1        | recall   |\n",
      "| 3    | 0.978741 | 0.988142 | recall   |\n",
      "| 4    | 0.97115  | 0.955153 | recall   |\n",
      "| Mean | 0.904924 | 0.965553 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.973767 | 0.927536 | f1-score |\n",
      "| 1    | 0.916067 | 0.78125  | f1-score |\n",
      "| 2    | 0.854701 | 0.871212 | f1-score |\n",
      "| 3    | 0.988031 | 0.94518  | f1-score |\n",
      "| 4    | 0.977501 | 0.937916 | f1-score |\n",
      "| Mean | 0.942013 | 0.892619 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_glove100, y_train)\n",
    "y_pred = svm.predict(X_test_glove100)\n",
    "y_pred_1 = svm.predict(X_1_glove100)\n",
    "y_pred_2 = svm.predict(X_2_glove100)\n",
    "y_pred_3 = svm.predict(X_3_glove100)\n",
    "y_pred_4 = svm.predict(X_glove100)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove200 = sentences_to_embeddings(X_train, glove200) \n",
    "X_test_glove200 = sentences_to_embeddings(X_test, glove200)\n",
    "\n",
    "X_1_glove200 = sentences_to_embeddings(X_1, glove200)\n",
    "X_2_glove200 = sentences_to_embeddings(X_2, glove200)\n",
    "X_3_glove200 = sentences_to_embeddings(X_3, glove200)\n",
    "X_glove200 = sentences_to_embeddings(X, glove200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.98208  | 0.928435 | precision |\n",
      "| 1    | 0.984772 | 0.688525 | precision |\n",
      "| 2    | 0.924528 | 0.748252 | precision |\n",
      "| 3    | 0.9975   | 0.905797 | precision |\n",
      "| 4    | 0.985766 | 0.926838 | precision |\n",
      "| Mean | 0.974929 | 0.839569 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.973868 | 0.950195 | recall   |\n",
      "| 1    | 0.87191  | 0.954545 | recall   |\n",
      "| 2    | 0.731343 | 0.930435 | recall   |\n",
      "| 3    | 0.978741 | 0.988142 | recall   |\n",
      "| 4    | 0.973202 | 0.960253 | recall   |\n",
      "| Mean | 0.905813 | 0.956714 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.977957 | 0.939189 | f1-score |\n",
      "| 1    | 0.924911 | 0.8      | f1-score |\n",
      "| 2    | 0.816667 | 0.829457 | f1-score |\n",
      "| 3    | 0.988031 | 0.94518  | f1-score |\n",
      "| 4    | 0.979444 | 0.94325  | f1-score |\n",
      "| Mean | 0.937402 | 0.891415 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_glove200, y_train)\n",
    "y_pred = svm.predict(X_test_glove200)\n",
    "y_pred_1 = svm.predict(X_1_glove200)\n",
    "y_pred_2 = svm.predict(X_2_glove200)\n",
    "y_pred_3 = svm.predict(X_3_glove200)\n",
    "y_pred_4 = svm.predict(X_glove200)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove300 = sentences_to_embeddings(X_train, glove300) \n",
    "X_test_glove300 = sentences_to_embeddings(X_test, glove300)\n",
    "\n",
    "X_1_glove300 = sentences_to_embeddings(X_1, glove300)\n",
    "X_2_glove300 = sentences_to_embeddings(X_2, glove300)\n",
    "X_3_glove300 = sentences_to_embeddings(X_3, glove300)\n",
    "X_glove300 = sentences_to_embeddings(X, glove300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.981754 | 0.931034 | precision |\n",
      "| 1    | 0.989691 | 0.677249 | precision |\n",
      "| 2    | 0.888889 | 0.730496 | precision |\n",
      "| 3    | 0.99584  | 0.905109 | precision |\n",
      "| 4    | 0.986601 | 0.93205  | precision |\n",
      "| Mean | 0.968555 | 0.835188 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.974913 | 0.949219 | recall   |\n",
      "| 1    | 0.862921 | 0.969697 | recall   |\n",
      "| 2    | 0.716418 | 0.895652 | recall   |\n",
      "| 3    | 0.978741 | 0.980237 | recall   |\n",
      "| 4    | 0.975191 | 0.96254  | recall   |\n",
      "| Mean | 0.901637 | 0.951469 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.978322 | 0.940039 | f1-score |\n",
      "| 1    | 0.921969 | 0.797508 | f1-score |\n",
      "| 2    | 0.793388 | 0.804688 | f1-score |\n",
      "| 3    | 0.987216 | 0.941176 | f1-score |\n",
      "| 4    | 0.980863 | 0.94705  | f1-score |\n",
      "| Mean | 0.932352 | 0.886092 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_glove300, y_train)\n",
    "y_pred = svm.predict(X_test_glove300)\n",
    "y_pred_1 = svm.predict(X_1_glove300)\n",
    "y_pred_2 = svm.predict(X_2_glove300)\n",
    "y_pred_3 = svm.predict(X_3_glove300)\n",
    "y_pred_4 = svm.predict(X_glove300)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.986301 | 0.940783 | precision |\n",
      "| 1    | 0.992386 | 0.704918 | precision |\n",
      "| 2    | 0.970588 | 0.761905 | precision |\n",
      "| 3    | 0.998337 | 0.934307 | precision |\n",
      "| 4    | 0.990933 | 0.942086 | precision |\n",
      "| Mean | 0.987709 | 0.8568   | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.978397 | 0.961914 | recall   |\n",
      "| 1    | 0.878652 | 0.977273 | recall   |\n",
      "| 2    | 0.738806 | 0.973913 | recall   |\n",
      "| 3    | 0.985234 | 0.992248 | recall   |\n",
      "| 4    | 0.978792 | 0.974697 | recall   |\n",
      "| Mean | 0.911976 | 0.976009 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.982333 | 0.951231 | f1-score |\n",
      "| 1    | 0.932062 | 0.819048 | f1-score |\n",
      "| 2    | 0.838983 | 0.854962 | f1-score |\n",
      "| 3    | 0.991742 | 0.962406 | f1-score |\n",
      "| 4    | 0.984825 | 0.958114 | f1-score |\n",
      "| Mean | 0.945989 | 0.909152 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Preprocess with mode = 1 & lower = True\n",
    "X_train_glove300 = sentences_to_embeddings(preprocess_sentences(X_train, mode=1, labels=None, lower=True), glove300, preprocess=True) \n",
    "X_test_glove300 = sentences_to_embeddings(preprocess_sentences(X_test, mode=1, labels=None, lower=True), glove300, preprocess=True)\n",
    "\n",
    "X_1_glove300 = sentences_to_embeddings(preprocess_sentences(X_1, mode=1, labels=None, lower=True), glove300, preprocess=True)\n",
    "X_2_glove300 = sentences_to_embeddings(preprocess_sentences(X_2, mode=1, labels=None, lower=True), glove300, preprocess=True)\n",
    "X_3_glove300 = sentences_to_embeddings(preprocess_sentences(X_3, mode=1, labels=None, lower=True), glove300, preprocess=True)\n",
    "X_glove300 = sentences_to_embeddings(preprocess_sentences(X, mode=1, labels=None, lower=True), glove300, preprocess=True)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_glove300, y_train)\n",
    "y_pred = svm.predict(X_test_glove300)\n",
    "y_pred_1 = svm.predict(X_1_glove300)\n",
    "y_pred_2 = svm.predict(X_2_glove300)\n",
    "y_pred_3 = svm.predict(X_3_glove300)\n",
    "y_pred_4 = svm.predict(X_glove300)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.987013 | 0.944498 | precision |\n",
      "| 1    | 0.994872 | 0.695187 | precision |\n",
      "| 2    | 0.961905 | 0.770833 | precision |\n",
      "| 3    | 0.995854 | 0.933579 | precision |\n",
      "| 4    | 0.991318 | 0.945191 | precision |\n",
      "| Mean | 0.986192 | 0.857858 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.979791 | 0.963867 | recall   |\n",
      "| 1    | 0.87191  | 0.984848 | recall   |\n",
      "| 2    | 0.753731 | 0.965217 | recall   |\n",
      "| 3    | 0.985234 | 0.98062  | recall   |\n",
      "| 4    | 0.979974 | 0.975751 | recall   |\n",
      "| Mean | 0.914128 | 0.974061 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.983389 | 0.954084 | f1-score |\n",
      "| 1    | 0.929341 | 0.815047 | f1-score |\n",
      "| 2    | 0.845188 | 0.857143 | f1-score |\n",
      "| 3    | 0.990515 | 0.956522 | f1-score |\n",
      "| 4    | 0.985613 | 0.960228 | f1-score |\n",
      "| Mean | 0.946809 | 0.908605 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Preprocess with mode = 1 & lower = True, remove stopwords = True\n",
    "X_train_glove300 = sentences_to_embeddings(preprocess_sentences(X_train, mode=1, labels=None, lower=True, remove_stopwords=True), glove300, preprocess=True) \n",
    "X_test_glove300 = sentences_to_embeddings(preprocess_sentences(X_test, mode=1, labels=None, lower=True, remove_stopwords=True), glove300, preprocess=True)\n",
    "\n",
    "X_1_glove300 = sentences_to_embeddings(preprocess_sentences(X_1, mode=1, labels=None, lower=True, remove_stopwords=True), glove300, preprocess=True)\n",
    "X_2_glove300 = sentences_to_embeddings(preprocess_sentences(X_2, mode=1, labels=None, lower=True, remove_stopwords=True), glove300, preprocess=True)\n",
    "X_3_glove300 = sentences_to_embeddings(preprocess_sentences(X_3, mode=1, labels=None, lower=True, remove_stopwords=True), glove300, preprocess=True)\n",
    "X_glove300 = sentences_to_embeddings(preprocess_sentences(X, mode=1, labels=None, lower=True, remove_stopwords=True), glove300, preprocess=True)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_glove300, y_train)\n",
    "y_pred = svm.predict(X_test_glove300)\n",
    "y_pred_1 = svm.predict(X_1_glove300)\n",
    "y_pred_2 = svm.predict(X_2_glove300)\n",
    "y_pred_3 = svm.predict(X_3_glove300)\n",
    "y_pred_4 = svm.predict(X_glove300)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.987328 | 0.938272 | precision |\n",
      "| 1    | 0.989848 | 0.699454 | precision |\n",
      "| 2    | 0.989899 | 0.76     | precision |\n",
      "| 3    | 0.999168 | 0.934545 | precision |\n",
      "| 4    | 0.991233 | 0.938631 | precision |\n",
      "| Mean | 0.991495 | 0.85418  | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.977352 | 0.964844 | recall   |\n",
      "| 1    | 0.876404 | 0.969697 | recall   |\n",
      "| 2    | 0.731343 | 0.991304 | recall   |\n",
      "| 3    | 0.985234 | 0.996124 | recall   |\n",
      "| 4    | 0.977424 | 0.975575 | recall   |\n",
      "| Mean | 0.909551 | 0.979509 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.982315 | 0.951372 | f1-score |\n",
      "| 1    | 0.929678 | 0.812698 | f1-score |\n",
      "| 2    | 0.841202 | 0.860377 | f1-score |\n",
      "| 3    | 0.992152 | 0.964353 | f1-score |\n",
      "| 4    | 0.98428  | 0.956747 | f1-score |\n",
      "| Mean | 0.945925 | 0.909109 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Preprocess with mode = 1 & lower = True, remove stopwords = False, remove punctuation = True\n",
    "X_train_glove300 = sentences_to_embeddings(preprocess_sentences(X_train, mode=1, labels=None, lower=True, remove_stopwords=False, remove_punctuation=True), glove300, preprocess=True) \n",
    "X_test_glove300 = sentences_to_embeddings(preprocess_sentences(X_test, mode=1, labels=None, lower=True, remove_stopwords=False, remove_punctuation=True), glove300, preprocess=True)\n",
    "\n",
    "X_1_glove300 = sentences_to_embeddings(preprocess_sentences(X_1, mode=1, labels=None, lower=True, remove_stopwords=False, remove_punctuation=True), glove300, preprocess=True)\n",
    "X_2_glove300 = sentences_to_embeddings(preprocess_sentences(X_2, mode=1, labels=None, lower=True, remove_stopwords=False, remove_punctuation=True), glove300, preprocess=True)\n",
    "X_3_glove300 = sentences_to_embeddings(preprocess_sentences(X_3, mode=1, labels=None, lower=True, remove_stopwords=False, remove_punctuation=True), glove300, preprocess=True)\n",
    "X_glove300 = sentences_to_embeddings(preprocess_sentences(X, mode=1, labels=None, lower=True, remove_stopwords=False, remove_punctuation=True), glove300, preprocess=True)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_glove300, y_train)\n",
    "y_pred = svm.predict(X_test_glove300)\n",
    "y_pred_1 = svm.predict(X_1_glove300)\n",
    "y_pred_2 = svm.predict(X_2_glove300)\n",
    "y_pred_3 = svm.predict(X_3_glove300)\n",
    "y_pred_4 = svm.predict(X_glove300)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.988032 | 0.940171 | precision |\n",
      "| 1    | 0.997429 | 0.696809 | precision |\n",
      "| 2    | 0.98     | 0.758389 | precision |\n",
      "| 3    | 0.997508 | 0.934066 | precision |\n",
      "| 4    | 0.991685 | 0.943003 | precision |\n",
      "| Mean | 0.990931 | 0.854487 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.978049 | 0.966797 | recall   |\n",
      "| 1    | 0.87191  | 0.992424 | recall   |\n",
      "| 2    | 0.731343 | 0.982609 | recall   |\n",
      "| 3    | 0.985234 | 0.988372 | recall   |\n",
      "| 4    | 0.979103 | 0.976805 | recall   |\n",
      "| Mean | 0.909128 | 0.981401 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.983015 | 0.953298 | f1-score |\n",
      "| 1    | 0.930456 | 0.81875  | f1-score |\n",
      "| 2    | 0.837607 | 0.856061 | f1-score |\n",
      "| 3    | 0.991333 | 0.960452 | f1-score |\n",
      "| 4    | 0.985354 | 0.959606 | f1-score |\n",
      "| Mean | 0.945553 | 0.909633 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Preprocess with mode = 1 & lower = True, remove stopwords = True, remove punctuation = True\n",
    "X_train_glove300 = sentences_to_embeddings(preprocess_sentences(X_train, mode=1, labels=None, lower=True, remove_stopwords=True, remove_punctuation=True), glove300, preprocess=True) \n",
    "X_test_glove300 = sentences_to_embeddings(preprocess_sentences(X_test, mode=1, labels=None, lower=True, remove_stopwords=True, remove_punctuation=True), glove300, preprocess=True)\n",
    "\n",
    "X_1_glove300 = sentences_to_embeddings(preprocess_sentences(X_1, mode=1, labels=None, lower=True, remove_stopwords=True, remove_punctuation=True), glove300, preprocess=True)\n",
    "X_2_glove300 = sentences_to_embeddings(preprocess_sentences(X_2, mode=1, labels=None, lower=True, remove_stopwords=True, remove_punctuation=True), glove300, preprocess=True)\n",
    "X_3_glove300 = sentences_to_embeddings(preprocess_sentences(X_3, mode=1, labels=None, lower=True, remove_stopwords=True, remove_punctuation=True), glove300, preprocess=True)\n",
    "X_glove300 = sentences_to_embeddings(preprocess_sentences(X, mode=1, labels=None, lower=True, remove_stopwords=True, remove_punctuation=True), glove300, preprocess=True)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_glove300, y_train)\n",
    "y_pred = svm.predict(X_test_glove300)\n",
    "y_pred_1 = svm.predict(X_1_glove300)\n",
    "y_pred_2 = svm.predict(X_2_glove300)\n",
    "y_pred_3 = svm.predict(X_3_glove300)\n",
    "y_pred_4 = svm.predict(X_glove300)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.987672 | 0.937441 | precision |\n",
      "| 1    | 0.997396 | 0.678756 | precision |\n",
      "| 2    | 0.98     | 0.758389 | precision |\n",
      "| 3    | 0.997508 | 0.934066 | precision |\n",
      "| 4    | 0.992243 | 0.941654 | precision |\n",
      "| Mean | 0.990964 | 0.850061 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.977003 | 0.96582  | recall   |\n",
      "| 1    | 0.860674 | 0.992424 | recall   |\n",
      "| 2    | 0.731343 | 0.982609 | recall   |\n",
      "| 3    | 0.985234 | 0.988372 | recall   |\n",
      "| 4    | 0.978543 | 0.978387 | recall   |\n",
      "| Mean | 0.90656  | 0.981522 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.982309 | 0.951419 | f1-score |\n",
      "| 1    | 0.924005 | 0.806154 | f1-score |\n",
      "| 2    | 0.837607 | 0.856061 | f1-score |\n",
      "| 3    | 0.991333 | 0.960452 | f1-score |\n",
      "| 4    | 0.985346 | 0.959669 | f1-score |\n",
      "| Mean | 0.94412  | 0.906751 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Preprocess with mode = 1 & lower = True, remove stopwords = True, remove punctuation = True\n",
    "X_train_glove300 = sentences_to_embeddings(preprocess_sentences(X_train, mode=1, labels=None, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True), glove300, preprocess=True) \n",
    "X_test_glove300 = sentences_to_embeddings(preprocess_sentences(X_test, mode=1, labels=None, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True), glove300, preprocess=True)\n",
    "\n",
    "X_1_glove300 = sentences_to_embeddings(preprocess_sentences(X_1, mode=1, labels=None, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True), glove300, preprocess=True)\n",
    "X_2_glove300 = sentences_to_embeddings(preprocess_sentences(X_2, mode=1, labels=None, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True), glove300, preprocess=True)\n",
    "X_3_glove300 = sentences_to_embeddings(preprocess_sentences(X_3, mode=1, labels=None, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True), glove300, preprocess=True)\n",
    "X_glove300 = sentences_to_embeddings(preprocess_sentences(X, mode=1, labels=None, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True), glove300, preprocess=True)\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_glove300, y_train)\n",
    "y_pred = svm.predict(X_test_glove300)\n",
    "y_pred_1 = svm.predict(X_1_glove300)\n",
    "y_pred_2 = svm.predict(X_2_glove300)\n",
    "y_pred_3 = svm.predict(X_3_glove300)\n",
    "y_pred_4 = svm.predict(X_glove300)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastText, load_facebook_model\n",
    "\n",
    "model = FastText(vector_size=500, window=5, min_count=10, workers=6)\n",
    "\n",
    "model.build_vocab(X_train)\n",
    "\n",
    "model.train(X_train, total_examples=len(X_train), epochs=10)\n",
    "\n",
    "X_train_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_train]\n",
    "X_test_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_test]\n",
    "\n",
    "X_1_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_1]\n",
    "X_2_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_2]\n",
    "X_3_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_3]\n",
    "X_ft = [model.wv.get_sentence_vector(sentence) for sentence in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.981645 | 0.916117 | precision |\n",
      "| 1    | 0.984576 | 0.670213 | precision |\n",
      "| 2    | 0.972477 | 0.8      | precision |\n",
      "| 3    | 0.979339 | 0.872659 | precision |\n",
      "| 4    | 0.985067 | 0.914348 | precision |\n",
      "| Mean | 0.980621 | 0.834667 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.96899  | 0.949219 | recall   |\n",
      "| 1    | 0.860674 | 0.954545 | recall   |\n",
      "| 2    | 0.791045 | 0.973913 | recall   |\n",
      "| 3    | 0.972108 | 0.903101 | recall   |\n",
      "| 4    | 0.968219 | 0.958531 | recall   |\n",
      "| Mean | 0.912207 | 0.947862 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.975276 | 0.932374 | f1-score |\n",
      "| 1    | 0.918465 | 0.7875   | f1-score |\n",
      "| 2    | 0.872428 | 0.878431 | f1-score |\n",
      "| 3    | 0.97571  | 0.887619 | f1-score |\n",
      "| 4    | 0.976571 | 0.935918 | f1-score |\n",
      "| Mean | 0.94369  | 0.884369 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_ft, y_train)\n",
    "y_pred = svm.predict(X_test_ft)\n",
    "y_pred_1 = svm.predict(X_1_ft)\n",
    "y_pred_2 = svm.predict(X_2_ft)\n",
    "y_pred_3 = svm.predict(X_3_ft)\n",
    "y_pred_4 = svm.predict(X_ft)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.980641 | 0.920228 | precision |\n",
      "| 1    | 0.971939 | 0.654054 | precision |\n",
      "| 2    | 0.925926 | 0.758865 | precision |\n",
      "| 3    | 0.961912 | 0.868313 | precision |\n",
      "| 4    | 0.981727 | 0.915424 | precision |\n",
      "| Mean | 0.964429 | 0.823377 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.970732 | 0.946289 | recall   |\n",
      "| 1    | 0.85618  | 0.916667 | recall   |\n",
      "| 2    | 0.746269 | 0.930435 | recall   |\n",
      "| 3    | 0.973749 | 0.817829 | recall   |\n",
      "| 4    | 0.968966 | 0.949042 | recall   |\n",
      "| Mean | 0.903179 | 0.912052 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.975661 | 0.933077 | f1-score |\n",
      "| 1    | 0.910394 | 0.763407 | f1-score |\n",
      "| 2    | 0.826446 | 0.835938 | f1-score |\n",
      "| 3    | 0.967795 | 0.842315 | f1-score |\n",
      "| 4    | 0.975304 | 0.93193  | f1-score |\n",
      "| Mean | 0.93112  | 0.861333 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Preprocess mode = 1, lower = true & remove_stopwords = true\n",
    "model = FastText(vector_size=500, window=5, min_count=10, workers=6)\n",
    "\n",
    "X_train_temp = preprocess_sentences(X_train, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=False)\n",
    "X_train_temp = [' '.join(token for token in sentence) for sentence in X_train_temp]\n",
    "X_test_temp = preprocess_sentences(X_test, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=False)\n",
    "X_test_temp = [' '.join(token for token in sentence) for sentence in X_test_temp]\n",
    "X_1_temp = preprocess_sentences(X_1, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=False)\n",
    "X_1_temp = [' '.join(token for token in sentence) for sentence in X_1_temp]\n",
    "X_2_temp = preprocess_sentences(X_2, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=False)\n",
    "X_2_temp = [' '.join(token for token in sentence) for sentence in X_2_temp]\n",
    "X_3_temp = preprocess_sentences(X_3, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=False)\n",
    "X_3_temp = [' '.join(token for token in sentence) for sentence in X_3_temp]\n",
    "X_temp = preprocess_sentences(X, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=False, lemmatize=False)\n",
    "X_temp = [' '.join(token for token in sentence) for sentence in X_temp]\n",
    "\n",
    "model.build_vocab(X_train_temp)\n",
    "\n",
    "model.train(X_train_temp, total_examples=len(X_train_temp), epochs=10)\n",
    "\n",
    "X_train_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_train_temp]\n",
    "X_test_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_test_temp]\n",
    "\n",
    "X_1_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_1_temp]\n",
    "X_2_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_2_temp]\n",
    "X_3_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_3_temp]\n",
    "X_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_temp]\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_ft, y_train)\n",
    "y_pred = svm.predict(X_test_ft)\n",
    "y_pred_1 = svm.predict(X_1_ft)\n",
    "y_pred_2 = svm.predict(X_2_ft)\n",
    "y_pred_3 = svm.predict(X_3_ft)\n",
    "y_pred_4 = svm.predict(X_ft)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.97567  | 0.902647 | precision |\n",
      "| 1    | 0.972678 | 0.578199 | precision |\n",
      "| 2    | 0.896226 | 0.727273 | precision |\n",
      "| 3    | 0.961601 | 0.833992 | precision |\n",
      "| 4    | 0.977993 | 0.896928 | precision |\n",
      "| Mean | 0.956834 | 0.787808 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.964111 | 0.932617 | recall   |\n",
      "| 1    | 0.8      | 0.924242 | recall   |\n",
      "| 2    | 0.708955 | 0.904348 | recall   |\n",
      "| 3    | 0.965546 | 0.817829 | recall   |\n",
      "| 4    | 0.961814 | 0.938851 | recall   |\n",
      "| Mean | 0.880085 | 0.903578 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.969856 | 0.917387 | f1-score |\n",
      "| 1    | 0.877928 | 0.71137  | f1-score |\n",
      "| 2    | 0.791667 | 0.806202 | f1-score |\n",
      "| 3    | 0.963569 | 0.825832 | f1-score |\n",
      "| 4    | 0.969836 | 0.917411 | f1-score |\n",
      "| Mean | 0.914571 | 0.83564  | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Preprocess mode = 1, lower = true & remove_stopwords = true, remove_punctuation = True\n",
    "model = FastText(vector_size=500, window=5, min_count=10, workers=6)\n",
    "\n",
    "X_train_temp = preprocess_sentences(X_train, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=False)\n",
    "X_train_temp = [' '.join(token for token in sentence) for sentence in X_train_temp]\n",
    "X_test_temp = preprocess_sentences(X_test, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=False)\n",
    "X_test_temp = [' '.join(token for token in sentence) for sentence in X_test_temp]\n",
    "X_1_temp = preprocess_sentences(X_1, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=False)\n",
    "X_1_temp = [' '.join(token for token in sentence) for sentence in X_1_temp]\n",
    "X_2_temp = preprocess_sentences(X_2, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=False)\n",
    "X_2_temp = [' '.join(token for token in sentence) for sentence in X_2_temp]\n",
    "X_3_temp = preprocess_sentences(X_3, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=False)\n",
    "X_3_temp = [' '.join(token for token in sentence) for sentence in X_3_temp]\n",
    "X_temp = preprocess_sentences(X, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=False)\n",
    "X_temp = [' '.join(token for token in sentence) for sentence in X_temp]\n",
    "\n",
    "model.build_vocab(X_train_temp)\n",
    "\n",
    "model.train(X_train_temp, total_examples=len(X_train_temp), epochs=10)\n",
    "\n",
    "X_train_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_train_temp]\n",
    "X_test_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_test_temp]\n",
    "\n",
    "X_1_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_1_temp]\n",
    "X_2_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_2_temp]\n",
    "X_3_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_3_temp]\n",
    "X_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_temp]\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_ft, y_train)\n",
    "y_pred = svm.predict(X_test_ft)\n",
    "y_pred_1 = svm.predict(X_1_ft)\n",
    "y_pred_2 = svm.predict(X_2_ft)\n",
    "y_pred_3 = svm.predict(X_3_ft)\n",
    "y_pred_4 = svm.predict(X_ft)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.977385 | 0.902256 | precision |\n",
      "| 1    | 0.970027 | 0.57619  | precision |\n",
      "| 2    | 0.899083 | 0.742857 | precision |\n",
      "| 3    | 0.961601 | 0.833992 | precision |\n",
      "| 4    | 0.979412 | 0.896725 | precision |\n",
      "| Mean | 0.957502 | 0.790404 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.963763 | 0.9375   | recall   |\n",
      "| 1    | 0.8      | 0.916667 | recall   |\n",
      "| 2    | 0.731343 | 0.904348 | recall   |\n",
      "| 3    | 0.965546 | 0.817829 | recall   |\n",
      "| 4    | 0.961565 | 0.942892 | recall   |\n",
      "| Mean | 0.884443 | 0.903847 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.970526 | 0.91954  | f1-score |\n",
      "| 1    | 0.876847 | 0.707602 | f1-score |\n",
      "| 2    | 0.806584 | 0.815686 | f1-score |\n",
      "| 3    | 0.963569 | 0.825832 | f1-score |\n",
      "| 4    | 0.970406 | 0.919229 | f1-score |\n",
      "| Mean | 0.917587 | 0.837578 | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Preprocess mode = 1, lower = true & remove_stopwords = true, remove_punctuation = True, lemmatize = True\n",
    "model = FastText(vector_size=500, window=5, min_count=10, workers=6)\n",
    "\n",
    "X_train_temp = preprocess_sentences(X_train, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True)\n",
    "X_train_temp = [' '.join(token for token in sentence) for sentence in X_train_temp]\n",
    "X_test_temp = preprocess_sentences(X_test, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True)\n",
    "X_test_temp = [' '.join(token for token in sentence) for sentence in X_test_temp]\n",
    "X_1_temp = preprocess_sentences(X_1, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True)\n",
    "X_1_temp = [' '.join(token for token in sentence) for sentence in X_1_temp]\n",
    "X_2_temp = preprocess_sentences(X_2, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True)\n",
    "X_2_temp = [' '.join(token for token in sentence) for sentence in X_2_temp]\n",
    "X_3_temp = preprocess_sentences(X_3, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True)\n",
    "X_3_temp = [' '.join(token for token in sentence) for sentence in X_3_temp]\n",
    "X_temp = preprocess_sentences(X, labels=None, mode=1, lower=True, remove_stopwords=True, remove_punctuation=True, lemmatize=True)\n",
    "X_temp = [' '.join(token for token in sentence) for sentence in X_temp]\n",
    "\n",
    "model.build_vocab(X_train_temp)\n",
    "\n",
    "model.train(X_train_temp, total_examples=len(X_train_temp), epochs=10)\n",
    "\n",
    "X_train_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_train_temp]\n",
    "X_test_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_test_temp]\n",
    "\n",
    "X_1_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_1_temp]\n",
    "X_2_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_2_temp]\n",
    "X_3_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_3_temp]\n",
    "X_ft = [model.wv.get_sentence_vector(sentence) for sentence in X_temp]\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_ft, y_train)\n",
    "y_pred = svm.predict(X_test_ft)\n",
    "y_pred_1 = svm.predict(X_1_ft)\n",
    "y_pred_2 = svm.predict(X_2_ft)\n",
    "y_pred_3 = svm.predict(X_3_ft)\n",
    "y_pred_4 = svm.predict(X_ft)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15573/15573 [00:00<00:00, 90872.90it/s]\n",
      "100%|██████████| 3894/3894 [00:00<00:00, 101318.35it/s]\n",
      "100%|██████████| 577/577 [00:00<00:00, 102586.30it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 52109.65it/s]\n",
      "100%|██████████| 1476/1476 [00:00<00:00, 88529.69it/s]\n",
      "100%|██████████| 21769/21769 [00:00<00:00, 105235.51it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X_train)]\n",
    "X_test_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X_test)]\n",
    "\n",
    "X_1_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X_1)]\n",
    "X_2_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X_2)]\n",
    "X_3_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X_3)]\n",
    "X_temp = [\"\".join(word.lower() for word in sentence) for sentence in tqdm(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15573/15573 [00:13<00:00, 1159.99it/s]\n",
      "100%|██████████| 3894/3894 [00:03<00:00, 1202.50it/s]\n",
      "100%|██████████| 577/577 [00:00<00:00, 1194.95it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 826.28it/s] \n",
      "100%|██████████| 1476/1476 [00:00<00:00, 1583.30it/s]\n",
      "100%|██████████| 21769/21769 [00:18<00:00, 1204.99it/s]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.fasttext import FastText\n",
    "\n",
    "model = FastText(vector_size=500, window=5, min_count=10, workers=6)\n",
    "\n",
    "model.build_vocab(X_train_temp)\n",
    "\n",
    "model.train(X_train_temp, total_examples=len(X_train_temp), epochs=10)\n",
    "\n",
    "X_train_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_train_temp)]\n",
    "X_test_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_test_temp)]\n",
    "\n",
    "X_1_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_1_temp)]\n",
    "X_2_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_2_temp)]\n",
    "X_3_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_3_temp)]\n",
    "X_ft = [model.wv.get_sentence_vector(sentence) for sentence in tqdm(X_temp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.979809 | 0.902894 | precision |\n",
      "| 1    | 0.984    | 0.623762 | precision |\n",
      "| 2    | 0.962264 | 0.776224 | precision |\n",
      "| 3    | 0.969547 | 0.827586 | precision |\n",
      "| 4    | 0.981653 | 0.896959 | precision |\n",
      "| Mean | 0.975455 | 0.805485 | precision |\n",
      "## Recall\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.963763 | 0.944336 | recall   |\n",
      "| 1    | 0.829213 | 0.954545 | recall   |\n",
      "| 2    | 0.761194 | 0.965217 | recall   |\n",
      "| 3    | 0.963205 | 0.853755 | recall   |\n",
      "| 4    | 0.96145  | 0.949173 | recall   |\n",
      "| Mean | 0.895765 | 0.933405 | recall   |\n",
      "## F1-score\n",
      "|      |        0 |        1 | Metric   |\n",
      "|:-----|---------:|---------:|:---------|\n",
      "| 0    | 0.97172  | 0.92315  | f1-score |\n",
      "| 1    | 0.9      | 0.754491 | f1-score |\n",
      "| 2    | 0.85     | 0.860465 | f1-score |\n",
      "| 3    | 0.966366 | 0.840467 | f1-score |\n",
      "| 4    | 0.971447 | 0.922328 | f1-score |\n",
      "| Mean | 0.931906 | 0.86018  | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_ft, y_train)\n",
    "y_pred = svm.predict(X_test_ft)\n",
    "y_pred_1 = svm.predict(X_1_ft)\n",
    "y_pred_2 = svm.predict(X_2_ft)\n",
    "y_pred_3 = svm.predict(X_3_ft)\n",
    "y_pred_4 = svm.predict(X_ft)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply grid search to FastText parameters to get the best embedding parameters.\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.sklearn_api.ftmodel import FastText\n",
    "\n",
    "model = FastText(vector_size=300, window=5, min_count=10, workers=4)\n",
    "pipeline = Pipeline([\n",
    "    ('ft', model),\n",
    "    ('lr', LogisticRegression())\n",
    "])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'ft__vector_size': [50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "    'ft__window': [1, 3, 5, 7, 9, 11, 13, 15],\n",
    "    'ft__min_count': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, scoring='accuracy', cv=5, n_jobs=-1, verbose=2)\n",
    "\n",
    "grid_search.fit(X_train,y_train)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.cv_results_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT (Sentence Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentence_transformers as st\n",
    "\n",
    "class BertSentenceEmbedder():\n",
    "    '''\n",
    "    An interface for converting given text into sentence BERT embeddings.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Load the pre-trained model and tokenizer\n",
    "        '''\n",
    "        self.model = st.SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    def embed(self, sentences):\n",
    "        '''\n",
    "        Convert the given sentences into BERT embeddings.\n",
    "        :param sentences: A list of sentences to convert into BERT embeddings.\n",
    "        :return: A list of BERT embeddings for the given sentences.\n",
    "        '''\n",
    "        return self.model.encode(sentences, convert_to_tensor=True)\n",
    "\n",
    "embedder = BertSentenceEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15573/15573 [02:01<00:00, 128.12it/s]\n",
      "100%|██████████| 3894/3894 [00:29<00:00, 130.88it/s]\n",
      "100%|██████████| 577/577 [00:04<00:00, 133.46it/s]\n",
      "100%|██████████| 249/249 [00:01<00:00, 128.29it/s]\n",
      "100%|██████████| 1476/1476 [00:11<00:00, 132.90it/s]\n",
      "100%|██████████| 21769/21769 [02:46<00:00, 131.10it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_st = [embedder.embed(sentence) for sentence in tqdm(X_train)]\n",
    "X_test_st = [embedder.embed(sentence) for sentence in tqdm(X_test)]\n",
    "\n",
    "X_1_st = [embedder.embed(sentence) for sentence in tqdm(X_1)]\n",
    "X_2_st = [embedder.embed(sentence) for sentence in tqdm(X_2)]\n",
    "X_3_st = [embedder.embed(sentence) for sentence in tqdm(X_3)]\n",
    "X_st = [embedder.embed(sentence) for sentence in tqdm(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go back to cpu\n",
    "X_train_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_train_st])\n",
    "X_train_st = X_train_st.reshape(-1, 1)\n",
    "X_test_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_test_st])\n",
    "X_test_st = X_test_st.reshape(-1, 1)\n",
    "X_1_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_1_st])\n",
    "X_1_st = X_1_st.reshape(-1, 1)\n",
    "X_2_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_2_st])\n",
    "X_2_st = X_2_st.reshape(-1, 1)\n",
    "X_3_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_3_st])\n",
    "X_3_st = X_3_st.reshape(-1, 1)\n",
    "X_st = np.array([np.mean(np.array(embedding.cpu()), axis=0) for embedding in X_st])\n",
    "X_st = X_st.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.758685 | 0.926829 | precision |\n",
      "| 1    | 0.771777 | 0.333333 | precision |\n",
      "| 2    | 0.536585 | 0.333333 | precision |\n",
      "| 3    | 0.83871  | 0.947368 | precision |\n",
      "| 4    | 0.758919 | 0.930709 | precision |\n",
      "| Mean | 0.73     | 0.69     | precision |\n",
      "## Recall\n",
      "|      |        0 |          1 | Metric   |\n",
      "|:-----|---------:|-----------:|:---------|\n",
      "| 0    | 0.996864 | 0.111328   | recall   |\n",
      "| 1    | 0.995506 | 0.00757576 | recall   |\n",
      "| 2    | 0.985075 | 0.00869565 | recall   |\n",
      "| 3    | 0.999182 | 0.0711462  | recall   |\n",
      "| 4    | 0.997264 | 0.10394    | recall   |\n",
      "| Mean | 0.99     | 0.06       | recall   |\n",
      "## F1-score\n",
      "|      |        0 |         1 | Metric   |\n",
      "|:-----|---------:|----------:|:---------|\n",
      "| 0    | 0.861617 | 0.198779  | f1-score |\n",
      "| 1    | 0.86948  | 0.0148148 | f1-score |\n",
      "| 2    | 0.694737 | 0.0169492 | f1-score |\n",
      "| 3    | 0.91194  | 0.132353  | f1-score |\n",
      "| 4    | 0.861918 | 0.186996  | f1-score |\n",
      "| Mean | 0.84     | 0.11      | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_st, y_train)\n",
    "y_pred = svm.predict(X_test_st)\n",
    "y_pred_1 = svm.predict(X_1_st)\n",
    "y_pred_2 = svm.predict(X_2_st)\n",
    "y_pred_3 = svm.predict(X_3_st)\n",
    "y_pred_4 = svm.predict(X_st)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4669100, 15551690)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "model = Word2Vec(vector_size=500, window=5, min_count=1, workers=6)\n",
    "model.build_vocab(X_train)\n",
    "model.train(X_train, total_examples=len(X_train), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word2vec_sentence_embeddings(sentences, model):\n",
    "    sentence_embeddings = []\n",
    "    for sentence in tqdm(sentences):\n",
    "        word_vectors = []\n",
    "        for word in sentence.split():\n",
    "            try:\n",
    "                word_vectors.append(model.wv.get_vector(word.lower()))\n",
    "            except KeyError:\n",
    "                word_vectors.append(np.zeros(500))\n",
    "        if word_vectors:\n",
    "            sentence_embeddings.append(np.mean(np.mean(word_vectors, axis=0), axis=0))\n",
    "    sentence_embeddings = np.array(sentence_embeddings)\n",
    "    sentence_embeddings = sentence_embeddings.reshape(-1, 1)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15573/15573 [00:01<00:00, 15093.93it/s]\n",
      "100%|██████████| 3894/3894 [00:00<00:00, 14176.28it/s]\n",
      "100%|██████████| 577/577 [00:00<00:00, 13687.34it/s]\n",
      "100%|██████████| 249/249 [00:00<00:00, 12501.28it/s]\n",
      "100%|██████████| 1476/1476 [00:00<00:00, 16481.57it/s]\n",
      "100%|██████████| 21769/21769 [00:01<00:00, 15875.40it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_word2vec = get_word2vec_sentence_embeddings(X_train, model) \n",
    "X_test_word2vec = get_word2vec_sentence_embeddings(X_test, model)\n",
    "\n",
    "X_1_word2vec = get_word2vec_sentence_embeddings(X_1, model)\n",
    "X_2_word2vec = get_word2vec_sentence_embeddings(X_2, model)\n",
    "X_3_word2vec = get_word2vec_sentence_embeddings(X_3, model)\n",
    "X_word2vec = get_word2vec_sentence_embeddings(X, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Precision\n",
      "|      |        0 |        1 | Metric    |\n",
      "|:-----|---------:|---------:|:----------|\n",
      "| 0    | 0.765232 | 0.781095 | precision |\n",
      "| 1    | 0.766478 | 0.173913 | precision |\n",
      "| 2    | 0.59276  | 0.892857 | precision |\n",
      "| 3    | 0.854093 | 0.676056 | precision |\n",
      "| 4    | 0.768065 | 0.767227 | precision |\n",
      "| Mean | 0.749325 | 0.65823  | precision |\n",
      "## Recall\n",
      "|      |        0 |         1 | Metric   |\n",
      "|:-----|---------:|----------:|:---------|\n",
      "| 0    | 0.984669 | 0.15332   | recall   |\n",
      "| 1    | 0.914607 | 0.0606061 | recall   |\n",
      "| 2    | 0.977612 | 0.217391  | recall   |\n",
      "| 3    | 0.981194 | 0.189723  | recall   |\n",
      "| 4    | 0.982777 | 0.16057   | recall   |\n",
      "| Mean | 0.968172 | 0.156322  | recall   |\n",
      "## F1-score\n",
      "|      |        0 |         1 | Metric   |\n",
      "|:-----|---------:|----------:|:---------|\n",
      "| 0    | 0.861192 | 0.256327  | f1-score |\n",
      "| 1    | 0.834016 | 0.0898876 | f1-score |\n",
      "| 2    | 0.738028 | 0.34965   | f1-score |\n",
      "| 3    | 0.913242 | 0.296296  | f1-score |\n",
      "| 4    | 0.862255 | 0.265561  | f1-score |\n",
      "| Mean | 0.841747 | 0.251544  | f1-score |\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC()\n",
    "svm.fit(X_train_word2vec, y_train)\n",
    "y_pred = svm.predict(X_test_word2vec)\n",
    "y_pred_1 = svm.predict(X_1_word2vec)\n",
    "y_pred_2 = svm.predict(X_2_word2vec)\n",
    "y_pred_3 = svm.predict(X_3_word2vec)\n",
    "y_pred_4 = svm.predict(X_word2vec)\n",
    "report = classification_report(y_test, y_pred, output_dict=True)\n",
    "report_1 = classification_report(y_1, y_pred_1, output_dict=True)\n",
    "report_2 = classification_report(y_2, y_pred_2, output_dict=True)\n",
    "report_3 = classification_report(y_3, y_pred_3, output_dict=True)\n",
    "report_4 = classification_report(y, y_pred_4, output_dict=True)\n",
    "print(aggregate_reports([report, report_1, report_2, report_3, report_4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
